2020-06-10T16:21:48 INFO: Loading datasets
2020-06-10T16:21:54 INFO: CUDA Device 0 is: Tesla T4
2020-06-10T16:21:59 INFO: Torch version is: 1.5.0+cu101
2020-06-10T16:21:59 INFO: Loading checkpoint
2020-06-10T16:22:00 INFO: Copying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight
2020-06-10T16:22:00 INFO: Copying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight
2020-06-10T16:22:00 INFO: Copying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight
2020-06-10T16:22:00 INFO: Copying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.v_embeddings.image_embeddings.weight from model.bert.v_embeddings.image_embeddings.weight
2020-06-10T16:22:00 INFO: Copying model.bert.v_embeddings.image_embeddings.bias from model.bert.v_embeddings.image_embeddings.bias
2020-06-10T16:22:00 INFO: Copying model.bert.v_embeddings.image_location_embeddings.weight from model.bert.v_embeddings.image_location_embeddings.weight
2020-06-10T16:22:00 INFO: Copying model.bert.v_embeddings.image_location_embeddings.bias from model.bert.v_embeddings.image_location_embeddings.bias
2020-06-10T16:22:00 INFO: Copying model.bert.v_embeddings.LayerNorm.weight from model.bert.v_embeddings.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.v_embeddings.LayerNorm.bias from model.bert.v_embeddings.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.attention.self.query.weight from model.bert.encoder.v_layer.0.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.attention.self.query.bias from model.bert.encoder.v_layer.0.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.attention.self.key.weight from model.bert.encoder.v_layer.0.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.attention.self.key.bias from model.bert.encoder.v_layer.0.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.attention.self.value.weight from model.bert.encoder.v_layer.0.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.attention.self.value.bias from model.bert.encoder.v_layer.0.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.attention.output.dense.weight from model.bert.encoder.v_layer.0.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.attention.output.dense.bias from model.bert.encoder.v_layer.0.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.intermediate.dense.weight from model.bert.encoder.v_layer.0.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.intermediate.dense.bias from model.bert.encoder.v_layer.0.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.output.dense.weight from model.bert.encoder.v_layer.0.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.output.dense.bias from model.bert.encoder.v_layer.0.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.output.LayerNorm.weight from model.bert.encoder.v_layer.0.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.0.output.LayerNorm.bias from model.bert.encoder.v_layer.0.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.attention.self.query.weight from model.bert.encoder.v_layer.1.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.attention.self.query.bias from model.bert.encoder.v_layer.1.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.attention.self.key.weight from model.bert.encoder.v_layer.1.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.attention.self.key.bias from model.bert.encoder.v_layer.1.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.attention.self.value.weight from model.bert.encoder.v_layer.1.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.attention.self.value.bias from model.bert.encoder.v_layer.1.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.attention.output.dense.weight from model.bert.encoder.v_layer.1.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.attention.output.dense.bias from model.bert.encoder.v_layer.1.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.intermediate.dense.weight from model.bert.encoder.v_layer.1.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.intermediate.dense.bias from model.bert.encoder.v_layer.1.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.output.dense.weight from model.bert.encoder.v_layer.1.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.output.dense.bias from model.bert.encoder.v_layer.1.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.output.LayerNorm.weight from model.bert.encoder.v_layer.1.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.1.output.LayerNorm.bias from model.bert.encoder.v_layer.1.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.attention.self.query.weight from model.bert.encoder.v_layer.2.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.attention.self.query.bias from model.bert.encoder.v_layer.2.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.attention.self.key.weight from model.bert.encoder.v_layer.2.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.attention.self.key.bias from model.bert.encoder.v_layer.2.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.attention.self.value.weight from model.bert.encoder.v_layer.2.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.attention.self.value.bias from model.bert.encoder.v_layer.2.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.attention.output.dense.weight from model.bert.encoder.v_layer.2.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.attention.output.dense.bias from model.bert.encoder.v_layer.2.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.intermediate.dense.weight from model.bert.encoder.v_layer.2.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.intermediate.dense.bias from model.bert.encoder.v_layer.2.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.output.dense.weight from model.bert.encoder.v_layer.2.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.output.dense.bias from model.bert.encoder.v_layer.2.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.output.LayerNorm.weight from model.bert.encoder.v_layer.2.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.2.output.LayerNorm.bias from model.bert.encoder.v_layer.2.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.attention.self.query.weight from model.bert.encoder.v_layer.3.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.attention.self.query.bias from model.bert.encoder.v_layer.3.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.attention.self.key.weight from model.bert.encoder.v_layer.3.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.attention.self.key.bias from model.bert.encoder.v_layer.3.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.attention.self.value.weight from model.bert.encoder.v_layer.3.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.attention.self.value.bias from model.bert.encoder.v_layer.3.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.attention.output.dense.weight from model.bert.encoder.v_layer.3.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.attention.output.dense.bias from model.bert.encoder.v_layer.3.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.intermediate.dense.weight from model.bert.encoder.v_layer.3.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.intermediate.dense.bias from model.bert.encoder.v_layer.3.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.output.dense.weight from model.bert.encoder.v_layer.3.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.output.dense.bias from model.bert.encoder.v_layer.3.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.output.LayerNorm.weight from model.bert.encoder.v_layer.3.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.3.output.LayerNorm.bias from model.bert.encoder.v_layer.3.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.attention.self.query.weight from model.bert.encoder.v_layer.4.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.attention.self.query.bias from model.bert.encoder.v_layer.4.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.attention.self.key.weight from model.bert.encoder.v_layer.4.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.attention.self.key.bias from model.bert.encoder.v_layer.4.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.attention.self.value.weight from model.bert.encoder.v_layer.4.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.attention.self.value.bias from model.bert.encoder.v_layer.4.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.attention.output.dense.weight from model.bert.encoder.v_layer.4.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.attention.output.dense.bias from model.bert.encoder.v_layer.4.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.intermediate.dense.weight from model.bert.encoder.v_layer.4.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.intermediate.dense.bias from model.bert.encoder.v_layer.4.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.output.dense.weight from model.bert.encoder.v_layer.4.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.output.dense.bias from model.bert.encoder.v_layer.4.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.output.LayerNorm.weight from model.bert.encoder.v_layer.4.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.4.output.LayerNorm.bias from model.bert.encoder.v_layer.4.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.attention.self.query.weight from model.bert.encoder.v_layer.5.attention.self.query.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.attention.self.query.bias from model.bert.encoder.v_layer.5.attention.self.query.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.attention.self.key.weight from model.bert.encoder.v_layer.5.attention.self.key.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.attention.self.key.bias from model.bert.encoder.v_layer.5.attention.self.key.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.attention.self.value.weight from model.bert.encoder.v_layer.5.attention.self.value.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.attention.self.value.bias from model.bert.encoder.v_layer.5.attention.self.value.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.attention.output.dense.weight from model.bert.encoder.v_layer.5.attention.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.attention.output.dense.bias from model.bert.encoder.v_layer.5.attention.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.intermediate.dense.weight from model.bert.encoder.v_layer.5.intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.intermediate.dense.bias from model.bert.encoder.v_layer.5.intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.output.dense.weight from model.bert.encoder.v_layer.5.output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.output.dense.bias from model.bert.encoder.v_layer.5.output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.output.LayerNorm.weight from model.bert.encoder.v_layer.5.output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.v_layer.5.output.LayerNorm.bias from model.bert.encoder.v_layer.5.output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biattention.query1.weight from model.bert.encoder.c_layer.0.biattention.query1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biattention.query1.bias from model.bert.encoder.c_layer.0.biattention.query1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biattention.key1.weight from model.bert.encoder.c_layer.0.biattention.key1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biattention.key1.bias from model.bert.encoder.c_layer.0.biattention.key1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biattention.value1.weight from model.bert.encoder.c_layer.0.biattention.value1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biattention.value1.bias from model.bert.encoder.c_layer.0.biattention.value1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biattention.query2.weight from model.bert.encoder.c_layer.0.biattention.query2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biattention.query2.bias from model.bert.encoder.c_layer.0.biattention.query2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biattention.key2.weight from model.bert.encoder.c_layer.0.biattention.key2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biattention.key2.bias from model.bert.encoder.c_layer.0.biattention.key2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biattention.value2.weight from model.bert.encoder.c_layer.0.biattention.value2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biattention.value2.bias from model.bert.encoder.c_layer.0.biattention.value2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense1.weight from model.bert.encoder.c_layer.0.biOutput.dense1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense1.bias from model.bert.encoder.c_layer.0.biOutput.dense1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense1.weight from model.bert.encoder.c_layer.0.biOutput.q_dense1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense1.bias from model.bert.encoder.c_layer.0.biOutput.q_dense1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense2.weight from model.bert.encoder.c_layer.0.biOutput.dense2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense2.bias from model.bert.encoder.c_layer.0.biOutput.dense2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense2.weight from model.bert.encoder.c_layer.0.biOutput.q_dense2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense2.bias from model.bert.encoder.c_layer.0.biOutput.q_dense2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.v_intermediate.dense.weight from model.bert.encoder.c_layer.0.v_intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.v_intermediate.dense.bias from model.bert.encoder.c_layer.0.v_intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.v_output.dense.weight from model.bert.encoder.c_layer.0.v_output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.v_output.dense.bias from model.bert.encoder.c_layer.0.v_output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.v_output.LayerNorm.weight from model.bert.encoder.c_layer.0.v_output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.v_output.LayerNorm.bias from model.bert.encoder.c_layer.0.v_output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.t_intermediate.dense.weight from model.bert.encoder.c_layer.0.t_intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.t_intermediate.dense.bias from model.bert.encoder.c_layer.0.t_intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.t_output.dense.weight from model.bert.encoder.c_layer.0.t_output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.t_output.dense.bias from model.bert.encoder.c_layer.0.t_output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.t_output.LayerNorm.weight from model.bert.encoder.c_layer.0.t_output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.0.t_output.LayerNorm.bias from model.bert.encoder.c_layer.0.t_output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biattention.query1.weight from model.bert.encoder.c_layer.1.biattention.query1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biattention.query1.bias from model.bert.encoder.c_layer.1.biattention.query1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biattention.key1.weight from model.bert.encoder.c_layer.1.biattention.key1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biattention.key1.bias from model.bert.encoder.c_layer.1.biattention.key1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biattention.value1.weight from model.bert.encoder.c_layer.1.biattention.value1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biattention.value1.bias from model.bert.encoder.c_layer.1.biattention.value1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biattention.query2.weight from model.bert.encoder.c_layer.1.biattention.query2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biattention.query2.bias from model.bert.encoder.c_layer.1.biattention.query2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biattention.key2.weight from model.bert.encoder.c_layer.1.biattention.key2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biattention.key2.bias from model.bert.encoder.c_layer.1.biattention.key2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biattention.value2.weight from model.bert.encoder.c_layer.1.biattention.value2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biattention.value2.bias from model.bert.encoder.c_layer.1.biattention.value2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense1.weight from model.bert.encoder.c_layer.1.biOutput.dense1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense1.bias from model.bert.encoder.c_layer.1.biOutput.dense1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense1.weight from model.bert.encoder.c_layer.1.biOutput.q_dense1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense1.bias from model.bert.encoder.c_layer.1.biOutput.q_dense1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense2.weight from model.bert.encoder.c_layer.1.biOutput.dense2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense2.bias from model.bert.encoder.c_layer.1.biOutput.dense2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense2.weight from model.bert.encoder.c_layer.1.biOutput.q_dense2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense2.bias from model.bert.encoder.c_layer.1.biOutput.q_dense2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.v_intermediate.dense.weight from model.bert.encoder.c_layer.1.v_intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.v_intermediate.dense.bias from model.bert.encoder.c_layer.1.v_intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.v_output.dense.weight from model.bert.encoder.c_layer.1.v_output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.v_output.dense.bias from model.bert.encoder.c_layer.1.v_output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.v_output.LayerNorm.weight from model.bert.encoder.c_layer.1.v_output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.v_output.LayerNorm.bias from model.bert.encoder.c_layer.1.v_output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.t_intermediate.dense.weight from model.bert.encoder.c_layer.1.t_intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.t_intermediate.dense.bias from model.bert.encoder.c_layer.1.t_intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.t_output.dense.weight from model.bert.encoder.c_layer.1.t_output.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.t_output.dense.bias from model.bert.encoder.c_layer.1.t_output.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.t_output.LayerNorm.weight from model.bert.encoder.c_layer.1.t_output.LayerNorm.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.1.t_output.LayerNorm.bias from model.bert.encoder.c_layer.1.t_output.LayerNorm.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biattention.query1.weight from model.bert.encoder.c_layer.2.biattention.query1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biattention.query1.bias from model.bert.encoder.c_layer.2.biattention.query1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biattention.key1.weight from model.bert.encoder.c_layer.2.biattention.key1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biattention.key1.bias from model.bert.encoder.c_layer.2.biattention.key1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biattention.value1.weight from model.bert.encoder.c_layer.2.biattention.value1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biattention.value1.bias from model.bert.encoder.c_layer.2.biattention.value1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biattention.query2.weight from model.bert.encoder.c_layer.2.biattention.query2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biattention.query2.bias from model.bert.encoder.c_layer.2.biattention.query2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biattention.key2.weight from model.bert.encoder.c_layer.2.biattention.key2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biattention.key2.bias from model.bert.encoder.c_layer.2.biattention.key2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biattention.value2.weight from model.bert.encoder.c_layer.2.biattention.value2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biattention.value2.bias from model.bert.encoder.c_layer.2.biattention.value2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense1.weight from model.bert.encoder.c_layer.2.biOutput.dense1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense1.bias from model.bert.encoder.c_layer.2.biOutput.dense1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense1.weight from model.bert.encoder.c_layer.2.biOutput.q_dense1.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense1.bias from model.bert.encoder.c_layer.2.biOutput.q_dense1.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense2.weight from model.bert.encoder.c_layer.2.biOutput.dense2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense2.bias from model.bert.encoder.c_layer.2.biOutput.dense2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense2.weight from model.bert.encoder.c_layer.2.biOutput.q_dense2.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense2.bias from model.bert.encoder.c_layer.2.biOutput.q_dense2.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.v_intermediate.dense.weight from model.bert.encoder.c_layer.2.v_intermediate.dense.weight
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.v_intermediate.dense.bias from model.bert.encoder.c_layer.2.v_intermediate.dense.bias
2020-06-10T16:22:00 INFO: Copying model.bert.encoder.c_layer.2.v_output.dense.weight from model.bert.encoder.c_layer.2.v_output.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.2.v_output.dense.bias from model.bert.encoder.c_layer.2.v_output.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.2.v_output.LayerNorm.weight from model.bert.encoder.c_layer.2.v_output.LayerNorm.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.2.v_output.LayerNorm.bias from model.bert.encoder.c_layer.2.v_output.LayerNorm.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.2.t_intermediate.dense.weight from model.bert.encoder.c_layer.2.t_intermediate.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.2.t_intermediate.dense.bias from model.bert.encoder.c_layer.2.t_intermediate.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.2.t_output.dense.weight from model.bert.encoder.c_layer.2.t_output.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.2.t_output.dense.bias from model.bert.encoder.c_layer.2.t_output.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.2.t_output.LayerNorm.weight from model.bert.encoder.c_layer.2.t_output.LayerNorm.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.2.t_output.LayerNorm.bias from model.bert.encoder.c_layer.2.t_output.LayerNorm.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biattention.query1.weight from model.bert.encoder.c_layer.3.biattention.query1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biattention.query1.bias from model.bert.encoder.c_layer.3.biattention.query1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biattention.key1.weight from model.bert.encoder.c_layer.3.biattention.key1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biattention.key1.bias from model.bert.encoder.c_layer.3.biattention.key1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biattention.value1.weight from model.bert.encoder.c_layer.3.biattention.value1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biattention.value1.bias from model.bert.encoder.c_layer.3.biattention.value1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biattention.query2.weight from model.bert.encoder.c_layer.3.biattention.query2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biattention.query2.bias from model.bert.encoder.c_layer.3.biattention.query2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biattention.key2.weight from model.bert.encoder.c_layer.3.biattention.key2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biattention.key2.bias from model.bert.encoder.c_layer.3.biattention.key2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biattention.value2.weight from model.bert.encoder.c_layer.3.biattention.value2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biattention.value2.bias from model.bert.encoder.c_layer.3.biattention.value2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense1.weight from model.bert.encoder.c_layer.3.biOutput.dense1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense1.bias from model.bert.encoder.c_layer.3.biOutput.dense1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense1.weight from model.bert.encoder.c_layer.3.biOutput.q_dense1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense1.bias from model.bert.encoder.c_layer.3.biOutput.q_dense1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense2.weight from model.bert.encoder.c_layer.3.biOutput.dense2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense2.bias from model.bert.encoder.c_layer.3.biOutput.dense2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense2.weight from model.bert.encoder.c_layer.3.biOutput.q_dense2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense2.bias from model.bert.encoder.c_layer.3.biOutput.q_dense2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.v_intermediate.dense.weight from model.bert.encoder.c_layer.3.v_intermediate.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.v_intermediate.dense.bias from model.bert.encoder.c_layer.3.v_intermediate.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.v_output.dense.weight from model.bert.encoder.c_layer.3.v_output.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.v_output.dense.bias from model.bert.encoder.c_layer.3.v_output.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.v_output.LayerNorm.weight from model.bert.encoder.c_layer.3.v_output.LayerNorm.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.v_output.LayerNorm.bias from model.bert.encoder.c_layer.3.v_output.LayerNorm.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.t_intermediate.dense.weight from model.bert.encoder.c_layer.3.t_intermediate.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.t_intermediate.dense.bias from model.bert.encoder.c_layer.3.t_intermediate.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.t_output.dense.weight from model.bert.encoder.c_layer.3.t_output.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.t_output.dense.bias from model.bert.encoder.c_layer.3.t_output.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.t_output.LayerNorm.weight from model.bert.encoder.c_layer.3.t_output.LayerNorm.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.3.t_output.LayerNorm.bias from model.bert.encoder.c_layer.3.t_output.LayerNorm.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biattention.query1.weight from model.bert.encoder.c_layer.4.biattention.query1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biattention.query1.bias from model.bert.encoder.c_layer.4.biattention.query1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biattention.key1.weight from model.bert.encoder.c_layer.4.biattention.key1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biattention.key1.bias from model.bert.encoder.c_layer.4.biattention.key1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biattention.value1.weight from model.bert.encoder.c_layer.4.biattention.value1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biattention.value1.bias from model.bert.encoder.c_layer.4.biattention.value1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biattention.query2.weight from model.bert.encoder.c_layer.4.biattention.query2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biattention.query2.bias from model.bert.encoder.c_layer.4.biattention.query2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biattention.key2.weight from model.bert.encoder.c_layer.4.biattention.key2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biattention.key2.bias from model.bert.encoder.c_layer.4.biattention.key2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biattention.value2.weight from model.bert.encoder.c_layer.4.biattention.value2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biattention.value2.bias from model.bert.encoder.c_layer.4.biattention.value2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense1.weight from model.bert.encoder.c_layer.4.biOutput.dense1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense1.bias from model.bert.encoder.c_layer.4.biOutput.dense1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense1.weight from model.bert.encoder.c_layer.4.biOutput.q_dense1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense1.bias from model.bert.encoder.c_layer.4.biOutput.q_dense1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense2.weight from model.bert.encoder.c_layer.4.biOutput.dense2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense2.bias from model.bert.encoder.c_layer.4.biOutput.dense2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense2.weight from model.bert.encoder.c_layer.4.biOutput.q_dense2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense2.bias from model.bert.encoder.c_layer.4.biOutput.q_dense2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.v_intermediate.dense.weight from model.bert.encoder.c_layer.4.v_intermediate.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.v_intermediate.dense.bias from model.bert.encoder.c_layer.4.v_intermediate.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.v_output.dense.weight from model.bert.encoder.c_layer.4.v_output.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.v_output.dense.bias from model.bert.encoder.c_layer.4.v_output.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.v_output.LayerNorm.weight from model.bert.encoder.c_layer.4.v_output.LayerNorm.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.v_output.LayerNorm.bias from model.bert.encoder.c_layer.4.v_output.LayerNorm.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.t_intermediate.dense.weight from model.bert.encoder.c_layer.4.t_intermediate.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.t_intermediate.dense.bias from model.bert.encoder.c_layer.4.t_intermediate.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.t_output.dense.weight from model.bert.encoder.c_layer.4.t_output.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.t_output.dense.bias from model.bert.encoder.c_layer.4.t_output.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.t_output.LayerNorm.weight from model.bert.encoder.c_layer.4.t_output.LayerNorm.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.4.t_output.LayerNorm.bias from model.bert.encoder.c_layer.4.t_output.LayerNorm.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biattention.query1.weight from model.bert.encoder.c_layer.5.biattention.query1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biattention.query1.bias from model.bert.encoder.c_layer.5.biattention.query1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biattention.key1.weight from model.bert.encoder.c_layer.5.biattention.key1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biattention.key1.bias from model.bert.encoder.c_layer.5.biattention.key1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biattention.value1.weight from model.bert.encoder.c_layer.5.biattention.value1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biattention.value1.bias from model.bert.encoder.c_layer.5.biattention.value1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biattention.query2.weight from model.bert.encoder.c_layer.5.biattention.query2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biattention.query2.bias from model.bert.encoder.c_layer.5.biattention.query2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biattention.key2.weight from model.bert.encoder.c_layer.5.biattention.key2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biattention.key2.bias from model.bert.encoder.c_layer.5.biattention.key2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biattention.value2.weight from model.bert.encoder.c_layer.5.biattention.value2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biattention.value2.bias from model.bert.encoder.c_layer.5.biattention.value2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense1.weight from model.bert.encoder.c_layer.5.biOutput.dense1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense1.bias from model.bert.encoder.c_layer.5.biOutput.dense1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense1.weight from model.bert.encoder.c_layer.5.biOutput.q_dense1.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense1.bias from model.bert.encoder.c_layer.5.biOutput.q_dense1.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense2.weight from model.bert.encoder.c_layer.5.biOutput.dense2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense2.bias from model.bert.encoder.c_layer.5.biOutput.dense2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense2.weight from model.bert.encoder.c_layer.5.biOutput.q_dense2.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense2.bias from model.bert.encoder.c_layer.5.biOutput.q_dense2.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.v_intermediate.dense.weight from model.bert.encoder.c_layer.5.v_intermediate.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.v_intermediate.dense.bias from model.bert.encoder.c_layer.5.v_intermediate.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.v_output.dense.weight from model.bert.encoder.c_layer.5.v_output.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.v_output.dense.bias from model.bert.encoder.c_layer.5.v_output.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.v_output.LayerNorm.weight from model.bert.encoder.c_layer.5.v_output.LayerNorm.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.v_output.LayerNorm.bias from model.bert.encoder.c_layer.5.v_output.LayerNorm.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.t_intermediate.dense.weight from model.bert.encoder.c_layer.5.t_intermediate.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.t_intermediate.dense.bias from model.bert.encoder.c_layer.5.t_intermediate.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.t_output.dense.weight from model.bert.encoder.c_layer.5.t_output.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.t_output.dense.bias from model.bert.encoder.c_layer.5.t_output.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.t_output.LayerNorm.weight from model.bert.encoder.c_layer.5.t_output.LayerNorm.weight
2020-06-10T16:22:01 INFO: Copying model.bert.encoder.c_layer.5.t_output.LayerNorm.bias from model.bert.encoder.c_layer.5.t_output.LayerNorm.bias
2020-06-10T16:22:01 INFO: Copying model.bert.t_pooler.dense.weight from model.bert.t_pooler.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.t_pooler.dense.bias from model.bert.t_pooler.dense.bias
2020-06-10T16:22:01 INFO: Copying model.bert.v_pooler.dense.weight from model.bert.v_pooler.dense.weight
2020-06-10T16:22:01 INFO: Copying model.bert.v_pooler.dense.bias from model.bert.v_pooler.dense.bias
2020-06-10T16:22:01 INFO: Pretrained model loaded
2020-06-10T16:22:01 INFO: ===== Model =====
2020-06-10T16:22:01 INFO: ViLBERT(
  (model): ViLBERTForClassification(
    (bert): ViLBERTBase(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (v_embeddings): BertImageFeatureEmbeddings(
        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)
        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)
        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (v_layer): ModuleList(
          (0): BertImageLayer(
            (attention): BertImageAttention(
              (self): BertImageSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertImageSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertImageIntermediate(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (output): BertImageOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertImageLayer(
            (attention): BertImageAttention(
              (self): BertImageSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertImageSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertImageIntermediate(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (output): BertImageOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertImageLayer(
            (attention): BertImageAttention(
              (self): BertImageSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertImageSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertImageIntermediate(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (output): BertImageOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertImageLayer(
            (attention): BertImageAttention(
              (self): BertImageSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertImageSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertImageIntermediate(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (output): BertImageOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertImageLayer(
            (attention): BertImageAttention(
              (self): BertImageSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertImageSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertImageIntermediate(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (output): BertImageOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertImageLayer(
            (attention): BertImageAttention(
              (self): BertImageSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertImageSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertImageIntermediate(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (output): BertImageOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (c_layer): ModuleList(
          (0): BertConnectionLayer(
            (biattention): BertBiAttention(
              (query1): Linear(in_features=1024, out_features=1024, bias=True)
              (key1): Linear(in_features=1024, out_features=1024, bias=True)
              (value1): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (query2): Linear(in_features=768, out_features=1024, bias=True)
              (key2): Linear(in_features=768, out_features=1024, bias=True)
              (value2): Linear(in_features=768, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
            (biOutput): BertBiOutput(
              (dense1): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)
              (q_dropout1): Dropout(p=0.1, inplace=False)
              (dense2): Linear(in_features=1024, out_features=768, bias=True)
              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)
              (q_dropout2): Dropout(p=0.1, inplace=False)
            )
            (v_intermediate): BertImageIntermediate(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (v_output): BertImageOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (t_intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (t_output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertConnectionLayer(
            (biattention): BertBiAttention(
              (query1): Linear(in_features=1024, out_features=1024, bias=True)
              (key1): Linear(in_features=1024, out_features=1024, bias=True)
              (value1): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (query2): Linear(in_features=768, out_features=1024, bias=True)
              (key2): Linear(in_features=768, out_features=1024, bias=True)
              (value2): Linear(in_features=768, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
            (biOutput): BertBiOutput(
              (dense1): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)
              (q_dropout1): Dropout(p=0.1, inplace=False)
              (dense2): Linear(in_features=1024, out_features=768, bias=True)
              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)
              (q_dropout2): Dropout(p=0.1, inplace=False)
            )
            (v_intermediate): BertImageIntermediate(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (v_output): BertImageOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (t_intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (t_output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertConnectionLayer(
            (biattention): BertBiAttention(
              (query1): Linear(in_features=1024, out_features=1024, bias=True)
              (key1): Linear(in_features=1024, out_features=1024, bias=True)
              (value1): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (query2): Linear(in_features=768, out_features=1024, bias=True)
              (key2): Linear(in_features=768, out_features=1024, bias=True)
              (value2): Linear(in_features=768, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
            (biOutput): BertBiOutput(
              (dense1): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)
              (q_dropout1): Dropout(p=0.1, inplace=False)
              (dense2): Linear(in_features=1024, out_features=768, bias=True)
              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)
              (q_dropout2): Dropout(p=0.1, inplace=False)
            )
            (v_intermediate): BertImageIntermediate(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (v_output): BertImageOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (t_intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (t_output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertConnectionLayer(
            (biattention): BertBiAttention(
              (query1): Linear(in_features=1024, out_features=1024, bias=True)
              (key1): Linear(in_features=1024, out_features=1024, bias=True)
              (value1): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (query2): Linear(in_features=768, out_features=1024, bias=True)
              (key2): Linear(in_features=768, out_features=1024, bias=True)
              (value2): Linear(in_features=768, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
            (biOutput): BertBiOutput(
              (dense1): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)
              (q_dropout1): Dropout(p=0.1, inplace=False)
              (dense2): Linear(in_features=1024, out_features=768, bias=True)
              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)
              (q_dropout2): Dropout(p=0.1, inplace=False)
            )
            (v_intermediate): BertImageIntermediate(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (v_output): BertImageOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (t_intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (t_output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertConnectionLayer(
            (biattention): BertBiAttention(
              (query1): Linear(in_features=1024, out_features=1024, bias=True)
              (key1): Linear(in_features=1024, out_features=1024, bias=True)
              (value1): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (query2): Linear(in_features=768, out_features=1024, bias=True)
              (key2): Linear(in_features=768, out_features=1024, bias=True)
              (value2): Linear(in_features=768, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
            (biOutput): BertBiOutput(
              (dense1): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)
              (q_dropout1): Dropout(p=0.1, inplace=False)
              (dense2): Linear(in_features=1024, out_features=768, bias=True)
              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)
              (q_dropout2): Dropout(p=0.1, inplace=False)
            )
            (v_intermediate): BertImageIntermediate(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (v_output): BertImageOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (t_intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (t_output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertConnectionLayer(
            (biattention): BertBiAttention(
              (query1): Linear(in_features=1024, out_features=1024, bias=True)
              (key1): Linear(in_features=1024, out_features=1024, bias=True)
              (value1): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (query2): Linear(in_features=768, out_features=1024, bias=True)
              (key2): Linear(in_features=768, out_features=1024, bias=True)
              (value2): Linear(in_features=768, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
            (biOutput): BertBiOutput(
              (dense1): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)
              (q_dropout1): Dropout(p=0.1, inplace=False)
              (dense2): Linear(in_features=1024, out_features=768, bias=True)
              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)
              (q_dropout2): Dropout(p=0.1, inplace=False)
            )
            (v_intermediate): BertImageIntermediate(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (v_output): BertImageOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (t_intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (t_output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (t_pooler): BertTextPooler(
        (dense): Linear(in_features=768, out_features=1024, bias=True)
        (activation): ReLU()
      )
      (v_pooler): BertImagePooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): ReLU()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Sequential(
      (0): BertPredictionHeadTransform(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      )
      (1): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
  (losses): Losses()
)
2020-06-10T16:22:01 INFO: Total Parameters: 247780354. Trained Parameters: 247780354
2020-06-10T16:22:01 INFO: Starting training...
2020-06-10T16:22:05 WARNING: /pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha)

2020-06-10T16:22:05 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T16:24:02 INFO: progress: 100/22000, train/total_loss: 0.7098, train/total_loss/avg: 0.7098, train/hateful_memes/cross_entropy: 0.7098, train/hateful_memes/cross_entropy/avg: 0.7098, max mem: 8311.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 0.83, time: 02m 916ms, time_since_start: 02m 13s 665ms, eta: 07h 21m 47s 269ms
2020-06-10T16:25:59 INFO: progress: 200/22000, train/total_loss: 0.7098, train/total_loss/avg: 0.7522, train/hateful_memes/cross_entropy: 0.7098, train/hateful_memes/cross_entropy/avg: 0.7522, max mem: 8311.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 894ms, time_since_start: 04m 10s 559ms, eta: 07h 05m 08s 506ms
2020-06-10T16:27:55 INFO: progress: 300/22000, train/total_loss: 0.7945, train/total_loss/avg: 0.7695, train/hateful_memes/cross_entropy: 0.7945, train/hateful_memes/cross_entropy/avg: 0.7695, max mem: 8311.0, experiment: run, epoch: 1, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 485ms, time_since_start: 06m 07s 045ms, eta: 07h 01m 42s 689ms
2020-06-10T16:29:51 INFO: progress: 400/22000, train/total_loss: 0.7098, train/total_loss/avg: 0.7372, train/hateful_memes/cross_entropy: 0.7098, train/hateful_memes/cross_entropy/avg: 0.7372, max mem: 8311.0, experiment: run, epoch: 1, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 228ms, time_since_start: 08m 03s 273ms, eta: 06h 58m 50s 360ms
2020-06-10T16:31:49 INFO: progress: 500/22000, train/total_loss: 0.7098, train/total_loss/avg: 0.7226, train/hateful_memes/cross_entropy: 0.7098, train/hateful_memes/cross_entropy/avg: 0.7226, max mem: 8311.0, experiment: run, epoch: 1, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0., ups: 0.85, time: 01m 57s 177ms, time_since_start: 10m 451ms, eta: 07h 18s 322ms
2020-06-10T16:33:44 INFO: progress: 600/22000, train/total_loss: 0.6644, train/total_loss/avg: 0.6916, train/hateful_memes/cross_entropy: 0.6644, train/hateful_memes/cross_entropy/avg: 0.6916, max mem: 8311.0, experiment: run, epoch: 2, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 298ms, time_since_start: 11m 55s 749ms, eta: 06h 51m 38s 612ms
2020-06-10T16:35:40 INFO: progress: 700/22000, train/total_loss: 0.6644, train/total_loss/avg: 0.6713, train/hateful_memes/cross_entropy: 0.6644, train/hateful_memes/cross_entropy/avg: 0.6713, max mem: 8311.0, experiment: run, epoch: 2, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 523ms, time_since_start: 13m 52s 273ms, eta: 06h 54m 04s 341ms
2020-06-10T16:37:34 INFO: progress: 800/22000, train/total_loss: 0.6644, train/total_loss/avg: 0.6748, train/hateful_memes/cross_entropy: 0.6644, train/hateful_memes/cross_entropy/avg: 0.6748, max mem: 8311.0, experiment: run, epoch: 2, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 077ms, time_since_start: 15m 46s 350ms, eta: 06h 43m 28s 518ms
2020-06-10T16:39:31 INFO: progress: 900/22000, train/total_loss: 0.6825, train/total_loss/avg: 0.6757, train/hateful_memes/cross_entropy: 0.6825, train/hateful_memes/cross_entropy/avg: 0.6757, max mem: 8311.0, experiment: run, epoch: 2, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 137ms, time_since_start: 17m 42s 487ms, eta: 06h 48m 49s 462ms
2020-06-10T16:41:27 INFO: progress: 1000/22000, train/total_loss: 0.6644, train/total_loss/avg: 0.6721, train/hateful_memes/cross_entropy: 0.6644, train/hateful_memes/cross_entropy/avg: 0.6721, max mem: 8311.0, experiment: run, epoch: 2, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 669ms, time_since_start: 19m 39s 157ms, eta: 06h 48m 45s 019ms
2020-06-10T16:41:27 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T16:41:43 INFO: Evaluation time. Running on full validation set...
2020-06-10T16:43:42 INFO: progress: 1000/22000, val/total_loss: 0.8057, val/hateful_memes/cross_entropy: 0.8057, val/hateful_memes/accuracy: 0.5600, val/hateful_memes/binary_f1: 0.3750, val/hateful_memes/roc_auc: 0.6532, num_updates: 1000, epoch: 2, iterations: 1000, max_updates: 22000, val_time: 59s 156ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.653232
2020-06-10T16:43:44 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T16:45:37 INFO: progress: 1100/22000, train/total_loss: 0.6644, train/total_loss/avg: 0.6707, train/hateful_memes/cross_entropy: 0.6644, train/hateful_memes/cross_entropy/avg: 0.6707, max mem: 8311.0, experiment: run, epoch: 3, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 434ms, time_since_start: 23m 48s 857ms, eta: 06h 39m 664ms
2020-06-10T16:47:30 INFO: progress: 1200/22000, train/total_loss: 0.6567, train/total_loss/avg: 0.6523, train/hateful_memes/cross_entropy: 0.6567, train/hateful_memes/cross_entropy/avg: 0.6523, max mem: 8311.0, experiment: run, epoch: 3, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 53s 088ms, time_since_start: 25m 41s 946ms, eta: 06h 32m 25s 916ms
2020-06-10T16:49:24 INFO: progress: 1300/22000, train/total_loss: 0.6567, train/total_loss/avg: 0.6425, train/hateful_memes/cross_entropy: 0.6567, train/hateful_memes/cross_entropy/avg: 0.6425, max mem: 8311.0, experiment: run, epoch: 3, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 423ms, time_since_start: 27m 36s 369ms, eta: 06h 35m 09s 339ms
2020-06-10T16:51:17 INFO: progress: 1400/22000, train/total_loss: 0.6402, train/total_loss/avg: 0.6330, train/hateful_memes/cross_entropy: 0.6402, train/hateful_memes/cross_entropy/avg: 0.6330, max mem: 8311.0, experiment: run, epoch: 3, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00001, ups: 0.89, time: 01m 52s 397ms, time_since_start: 29m 28s 767ms, eta: 06h 26m 17s 088ms
2020-06-10T16:53:10 INFO: progress: 1500/22000, train/total_loss: 0.6402, train/total_loss/avg: 0.6250, train/hateful_memes/cross_entropy: 0.6402, train/hateful_memes/cross_entropy/avg: 0.6250, max mem: 8311.0, experiment: run, epoch: 3, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 53s 365ms, time_since_start: 31m 22s 133ms, eta: 06h 27m 43s 216ms
2020-06-10T16:55:03 INFO: progress: 1600/22000, train/total_loss: 0.6394, train/total_loss/avg: 0.6018, train/hateful_memes/cross_entropy: 0.6394, train/hateful_memes/cross_entropy/avg: 0.6018, max mem: 8311.0, experiment: run, epoch: 4, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00001, ups: 0.89, time: 01m 52s 622ms, time_since_start: 33m 14s 755ms, eta: 06h 23m 18s 042ms
2020-06-10T16:56:56 INFO: progress: 1700/22000, train/total_loss: 0.6394, train/total_loss/avg: 0.5965, train/hateful_memes/cross_entropy: 0.6394, train/hateful_memes/cross_entropy/avg: 0.5965, max mem: 8311.0, experiment: run, epoch: 4, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00001, ups: 0.89, time: 01m 52s 779ms, time_since_start: 35m 07s 535ms, eta: 06h 21m 57s 195ms
2020-06-10T16:58:50 INFO: progress: 1800/22000, train/total_loss: 0.5825, train/total_loss/avg: 0.5958, train/hateful_memes/cross_entropy: 0.5825, train/hateful_memes/cross_entropy/avg: 0.5958, max mem: 8311.0, experiment: run, epoch: 4, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 875ms, time_since_start: 37m 02s 411ms, eta: 06h 27m 08s 025ms
2020-06-10T17:00:45 INFO: progress: 1900/22000, train/total_loss: 0.5825, train/total_loss/avg: 0.5895, train/hateful_memes/cross_entropy: 0.5825, train/hateful_memes/cross_entropy/avg: 0.5895, max mem: 8311.0, experiment: run, epoch: 4, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 778ms, time_since_start: 38m 57s 190ms, eta: 06h 24m 53s 641ms
2020-06-10T17:02:39 INFO: progress: 2000/22000, train/total_loss: 0.5499, train/total_loss/avg: 0.5809, train/hateful_memes/cross_entropy: 0.5499, train/hateful_memes/cross_entropy/avg: 0.5809, max mem: 8311.0, experiment: run, epoch: 4, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 53s 885ms, time_since_start: 40m 51s 075ms, eta: 06h 19m 59s 783ms
2020-06-10T17:02:39 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T17:03:17 INFO: Evaluation time. Running on full validation set...
2020-06-10T17:05:02 INFO: progress: 2000/22000, val/total_loss: 0.7694, val/hateful_memes/cross_entropy: 0.7694, val/hateful_memes/accuracy: 0.6180, val/hateful_memes/binary_f1: 0.5140, val/hateful_memes/roc_auc: 0.7067, num_updates: 2000, epoch: 4, iterations: 2000, max_updates: 22000, val_time: 34s 173ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.706736
2020-06-10T17:05:04 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T17:06:57 INFO: progress: 2100/22000, train/total_loss: 0.5362, train/total_loss/avg: 0.5767, train/hateful_memes/cross_entropy: 0.5362, train/hateful_memes/cross_entropy/avg: 0.5767, max mem: 8311.0, experiment: run, epoch: 4, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 844ms, time_since_start: 45m 09s 403ms, eta: 06h 21m 16s 913ms
2020-06-10T17:08:53 INFO: progress: 2200/22000, train/total_loss: 0.5253, train/total_loss/avg: 0.5564, train/hateful_memes/cross_entropy: 0.5253, train/hateful_memes/cross_entropy/avg: 0.5564, max mem: 8311.0, experiment: run, epoch: 5, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 713ms, time_since_start: 47m 05s 117ms, eta: 06h 22m 14s 137ms
2020-06-10T17:10:50 INFO: progress: 2300/22000, train/total_loss: 0.5135, train/total_loss/avg: 0.5453, train/hateful_memes/cross_entropy: 0.5135, train/hateful_memes/cross_entropy/avg: 0.5453, max mem: 8311.0, experiment: run, epoch: 5, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 381ms, time_since_start: 49m 01s 498ms, eta: 06h 22m 30s 057ms
2020-06-10T17:12:45 INFO: progress: 2400/22000, train/total_loss: 0.5115, train/total_loss/avg: 0.5311, train/hateful_memes/cross_entropy: 0.5115, train/hateful_memes/cross_entropy/avg: 0.5311, max mem: 8311.0, experiment: run, epoch: 5, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 905ms, time_since_start: 50m 57s 403ms, eta: 06h 19m 153ms
2020-06-10T17:14:40 INFO: progress: 2500/22000, train/total_loss: 0.5084, train/total_loss/avg: 0.5264, train/hateful_memes/cross_entropy: 0.5084, train/hateful_memes/cross_entropy/avg: 0.5264, max mem: 8311.0, experiment: run, epoch: 5, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 530ms, time_since_start: 52m 51s 934ms, eta: 06h 12m 35s 844ms
2020-06-10T17:16:36 INFO: progress: 2600/22000, train/total_loss: 0.5084, train/total_loss/avg: 0.5284, train/hateful_memes/cross_entropy: 0.5084, train/hateful_memes/cross_entropy/avg: 0.5284, max mem: 8311.0, experiment: run, epoch: 5, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 160ms, time_since_start: 54m 48s 094ms, eta: 06h 15m 57s 593ms
2020-06-10T17:18:33 INFO: progress: 2700/22000, train/total_loss: 0.4923, train/total_loss/avg: 0.5126, train/hateful_memes/cross_entropy: 0.4923, train/hateful_memes/cross_entropy/avg: 0.5126, max mem: 8311.0, experiment: run, epoch: 6, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 856ms, time_since_start: 56m 44s 951ms, eta: 06h 16m 15s 924ms
2020-06-10T17:20:28 INFO: progress: 2800/22000, train/total_loss: 0.4766, train/total_loss/avg: 0.5035, train/hateful_memes/cross_entropy: 0.4766, train/hateful_memes/cross_entropy/avg: 0.5035, max mem: 8311.0, experiment: run, epoch: 6, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 694ms, time_since_start: 58m 39s 646ms, eta: 06h 07m 23s 317ms
2020-06-10T17:22:23 INFO: progress: 2900/22000, train/total_loss: 0.4503, train/total_loss/avg: 0.4873, train/hateful_memes/cross_entropy: 0.4503, train/hateful_memes/cross_entropy/avg: 0.4873, max mem: 8311.0, experiment: run, epoch: 6, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 996ms, time_since_start: 01h 34s 642ms, eta: 06h 06m 26s 369ms
2020-06-10T17:24:19 INFO: progress: 3000/22000, train/total_loss: 0.4176, train/total_loss/avg: 0.4793, train/hateful_memes/cross_entropy: 0.4176, train/hateful_memes/cross_entropy/avg: 0.4793, max mem: 8311.0, experiment: run, epoch: 6, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 855ms, time_since_start: 01h 02m 30s 498ms, eta: 06h 07m 14s 496ms
2020-06-10T17:24:19 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T17:24:53 INFO: Evaluation time. Running on full validation set...
2020-06-10T17:25:53 INFO: progress: 3000/22000, val/total_loss: 1.4013, val/hateful_memes/cross_entropy: 1.4013, val/hateful_memes/accuracy: 0.5840, val/hateful_memes/binary_f1: 0.3697, val/hateful_memes/roc_auc: 0.7037, num_updates: 3000, epoch: 6, iterations: 3000, max_updates: 22000, val_time: 12s 615ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.706736
2020-06-10T17:25:56 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T17:27:50 INFO: progress: 3100/22000, train/total_loss: 0.4141, train/total_loss/avg: 0.4652, train/hateful_memes/cross_entropy: 0.4141, train/hateful_memes/cross_entropy/avg: 0.4652, max mem: 8311.0, experiment: run, epoch: 6, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 291ms, time_since_start: 01h 06m 02s 281ms, eta: 06h 03m 31s 852ms
2020-06-10T17:29:45 INFO: progress: 3200/22000, train/total_loss: 0.2997, train/total_loss/avg: 0.4519, train/hateful_memes/cross_entropy: 0.2997, train/hateful_memes/cross_entropy/avg: 0.4519, max mem: 8311.0, experiment: run, epoch: 7, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 816ms, time_since_start: 01h 07m 57s 098ms, eta: 06h 07s 129ms
2020-06-10T17:31:42 INFO: progress: 3300/22000, train/total_loss: 0.2567, train/total_loss/avg: 0.4389, train/hateful_memes/cross_entropy: 0.2567, train/hateful_memes/cross_entropy/avg: 0.4389, max mem: 8311.0, experiment: run, epoch: 7, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 364ms, time_since_start: 01h 09m 53s 463ms, eta: 06h 03m 01s 967ms
2020-06-10T17:33:38 INFO: progress: 3400/22000, train/total_loss: 0.2546, train/total_loss/avg: 0.4276, train/hateful_memes/cross_entropy: 0.2546, train/hateful_memes/cross_entropy/avg: 0.4276, max mem: 8311.0, experiment: run, epoch: 7, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 762ms, time_since_start: 01h 11m 50s 225ms, eta: 06h 02m 19s 549ms
2020-06-10T17:35:35 INFO: progress: 3500/22000, train/total_loss: 0.2461, train/total_loss/avg: 0.4168, train/hateful_memes/cross_entropy: 0.2461, train/hateful_memes/cross_entropy/avg: 0.4168, max mem: 8311.0, experiment: run, epoch: 7, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 594ms, time_since_start: 01h 13m 46s 820ms, eta: 05h 59m 51s 586ms
2020-06-10T17:37:29 INFO: progress: 3600/22000, train/total_loss: 0.2057, train/total_loss/avg: 0.4091, train/hateful_memes/cross_entropy: 0.2057, train/hateful_memes/cross_entropy/avg: 0.4091, max mem: 8311.0, experiment: run, epoch: 7, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 535ms, time_since_start: 01h 15m 41s 355ms, eta: 05h 51m 35s 624ms
2020-06-10T17:39:24 INFO: progress: 3700/22000, train/total_loss: 0.1427, train/total_loss/avg: 0.3985, train/hateful_memes/cross_entropy: 0.1427, train/hateful_memes/cross_entropy/avg: 0.3985, max mem: 8311.0, experiment: run, epoch: 7, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 440ms, time_since_start: 01h 17m 35s 796ms, eta: 05h 49m 23s 578ms
2020-06-10T17:41:19 INFO: progress: 3800/22000, train/total_loss: 0.1314, train/total_loss/avg: 0.3914, train/hateful_memes/cross_entropy: 0.1314, train/hateful_memes/cross_entropy/avg: 0.3914, max mem: 8311.0, experiment: run, epoch: 8, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 215ms, time_since_start: 01h 19m 31s 012ms, eta: 05h 49m 50s 241ms
2020-06-10T17:43:16 INFO: progress: 3900/22000, train/total_loss: 0.1301, train/total_loss/avg: 0.3823, train/hateful_memes/cross_entropy: 0.1301, train/hateful_memes/cross_entropy/avg: 0.3823, max mem: 8311.0, experiment: run, epoch: 8, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 501ms, time_since_start: 01h 21m 27s 513ms, eta: 05h 51m 47s 834ms
2020-06-10T17:45:11 INFO: progress: 4000/22000, train/total_loss: 0.1016, train/total_loss/avg: 0.3739, train/hateful_memes/cross_entropy: 0.1016, train/hateful_memes/cross_entropy/avg: 0.3739, max mem: 8311.0, experiment: run, epoch: 8, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 265ms, time_since_start: 01h 23m 22s 779ms, eta: 05h 46m 08s 524ms
2020-06-10T17:45:11 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T17:45:45 INFO: Evaluation time. Running on full validation set...
2020-06-10T17:47:09 INFO: progress: 4000/22000, val/total_loss: 1.2402, val/hateful_memes/cross_entropy: 1.2402, val/hateful_memes/accuracy: 0.6340, val/hateful_memes/binary_f1: 0.5390, val/hateful_memes/roc_auc: 0.7228, num_updates: 4000, epoch: 8, iterations: 4000, max_updates: 22000, val_time: 10s 438ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T17:47:11 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T17:49:04 INFO: progress: 4100/22000, train/total_loss: 0.1016, train/total_loss/avg: 0.3687, train/hateful_memes/cross_entropy: 0.1016, train/hateful_memes/cross_entropy/avg: 0.3687, max mem: 8311.0, experiment: run, epoch: 8, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 106ms, time_since_start: 01h 27m 16s 420ms, eta: 05h 43m 44s 672ms
2020-06-10T17:50:59 INFO: progress: 4200/22000, train/total_loss: 0.1016, train/total_loss/avg: 0.3669, train/hateful_memes/cross_entropy: 0.1016, train/hateful_memes/cross_entropy/avg: 0.3669, max mem: 8311.0, experiment: run, epoch: 8, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 628ms, time_since_start: 01h 29m 11s 049ms, eta: 05h 40m 24s 348ms
2020-06-10T17:52:55 INFO: progress: 4300/22000, train/total_loss: 0.1016, train/total_loss/avg: 0.3614, train/hateful_memes/cross_entropy: 0.1016, train/hateful_memes/cross_entropy/avg: 0.3614, max mem: 8311.0, experiment: run, epoch: 9, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 065ms, time_since_start: 01h 31m 07s 114ms, eta: 05h 42m 44s 181ms
2020-06-10T17:54:51 INFO: progress: 4400/22000, train/total_loss: 0.0566, train/total_loss/avg: 0.3539, train/hateful_memes/cross_entropy: 0.0566, train/hateful_memes/cross_entropy/avg: 0.3539, max mem: 8311.0, experiment: run, epoch: 9, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 474ms, time_since_start: 01h 33m 02s 589ms, eta: 05h 39m 03s 850ms
2020-06-10T17:56:45 INFO: progress: 4500/22000, train/total_loss: 0.0566, train/total_loss/avg: 0.3485, train/hateful_memes/cross_entropy: 0.0566, train/hateful_memes/cross_entropy/avg: 0.3485, max mem: 8311.0, experiment: run, epoch: 9, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 806ms, time_since_start: 01h 34m 57s 396ms, eta: 05h 35m 11s 240ms
2020-06-10T17:58:42 INFO: progress: 4600/22000, train/total_loss: 0.0463, train/total_loss/avg: 0.3413, train/hateful_memes/cross_entropy: 0.0463, train/hateful_memes/cross_entropy/avg: 0.3413, max mem: 8311.0, experiment: run, epoch: 9, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 207ms, time_since_start: 01h 36m 53s 603ms, eta: 05h 37m 20s 356ms
2020-06-10T18:00:38 INFO: progress: 4700/22000, train/total_loss: 0.0441, train/total_loss/avg: 0.3342, train/hateful_memes/cross_entropy: 0.0441, train/hateful_memes/cross_entropy/avg: 0.3342, max mem: 8311.0, experiment: run, epoch: 9, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 119ms, time_since_start: 01h 38m 49s 723ms, eta: 05h 35m 08s 789ms
2020-06-10T18:02:33 INFO: progress: 4800/22000, train/total_loss: 0.0440, train/total_loss/avg: 0.3273, train/hateful_memes/cross_entropy: 0.0440, train/hateful_memes/cross_entropy/avg: 0.3273, max mem: 8311.0, experiment: run, epoch: 10, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 872ms, time_since_start: 01h 40m 44s 595ms, eta: 05h 29m 37s 806ms
2020-06-10T18:04:27 INFO: progress: 4900/22000, train/total_loss: 0.0441, train/total_loss/avg: 0.3221, train/hateful_memes/cross_entropy: 0.0441, train/hateful_memes/cross_entropy/avg: 0.3221, max mem: 8311.0, experiment: run, epoch: 10, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 200ms, time_since_start: 01h 42m 38s 796ms, eta: 05h 25m 47s 815ms
2020-06-10T18:06:23 INFO: progress: 5000/22000, train/total_loss: 0.0440, train/total_loss/avg: 0.3158, train/hateful_memes/cross_entropy: 0.0440, train/hateful_memes/cross_entropy/avg: 0.3158, max mem: 8311.0, experiment: run, epoch: 10, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 964ms, time_since_start: 01h 44m 34s 760ms, eta: 05h 28m 53s 642ms
2020-06-10T18:06:23 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T18:06:57 INFO: Evaluation time. Running on full validation set...
2020-06-10T18:08:00 INFO: progress: 5000/22000, val/total_loss: 1.6260, val/hateful_memes/cross_entropy: 1.6260, val/hateful_memes/accuracy: 0.6060, val/hateful_memes/binary_f1: 0.4661, val/hateful_memes/roc_auc: 0.7213, num_updates: 5000, epoch: 10, iterations: 5000, max_updates: 22000, val_time: 10s 525ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T18:08:02 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T18:09:55 INFO: progress: 5100/22000, train/total_loss: 0.0391, train/total_loss/avg: 0.3101, train/hateful_memes/cross_entropy: 0.0391, train/hateful_memes/cross_entropy/avg: 0.3101, max mem: 8311.0, experiment: run, epoch: 10, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 661ms, time_since_start: 01h 48m 06s 797ms, eta: 05h 23m 17s 136ms
2020-06-10T18:11:51 INFO: progress: 5200/22000, train/total_loss: 0.0395, train/total_loss/avg: 0.3049, train/hateful_memes/cross_entropy: 0.0395, train/hateful_memes/cross_entropy/avg: 0.3049, max mem: 8311.0, experiment: run, epoch: 10, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 895ms, time_since_start: 01h 50m 02s 692ms, eta: 05h 24m 49s 886ms
2020-06-10T18:13:47 INFO: progress: 5300/22000, train/total_loss: 0.0440, train/total_loss/avg: 0.3003, train/hateful_memes/cross_entropy: 0.0440, train/hateful_memes/cross_entropy/avg: 0.3003, max mem: 8311.0, experiment: run, epoch: 10, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 620ms, time_since_start: 01h 51m 59s 313ms, eta: 05h 24m 55s 076ms
2020-06-10T18:15:44 INFO: progress: 5400/22000, train/total_loss: 0.0395, train/total_loss/avg: 0.2950, train/hateful_memes/cross_entropy: 0.0395, train/hateful_memes/cross_entropy/avg: 0.2950, max mem: 8311.0, experiment: run, epoch: 11, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 232ms, time_since_start: 01h 53m 55s 545ms, eta: 05h 21m 53s 864ms
2020-06-10T18:17:39 INFO: progress: 5500/22000, train/total_loss: 0.0370, train/total_loss/avg: 0.2896, train/hateful_memes/cross_entropy: 0.0370, train/hateful_memes/cross_entropy/avg: 0.2896, max mem: 8311.0, experiment: run, epoch: 11, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 235ms, time_since_start: 01h 55m 50s 781ms, eta: 05h 17m 12s 863ms
2020-06-10T18:19:34 INFO: progress: 5600/22000, train/total_loss: 0.0316, train/total_loss/avg: 0.2848, train/hateful_memes/cross_entropy: 0.0316, train/hateful_memes/cross_entropy/avg: 0.2848, max mem: 8311.0, experiment: run, epoch: 11, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 875ms, time_since_start: 01h 57m 45s 656ms, eta: 05h 14m 18s 377ms
2020-06-10T18:21:28 INFO: progress: 5700/22000, train/total_loss: 0.0346, train/total_loss/avg: 0.2804, train/hateful_memes/cross_entropy: 0.0346, train/hateful_memes/cross_entropy/avg: 0.2804, max mem: 8311.0, experiment: run, epoch: 11, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 589ms, time_since_start: 01h 59m 40s 246ms, eta: 05h 11m 36s 846ms
2020-06-10T18:23:25 INFO: progress: 5800/22000, train/total_loss: 0.0316, train/total_loss/avg: 0.2756, train/hateful_memes/cross_entropy: 0.0316, train/hateful_memes/cross_entropy/avg: 0.2756, max mem: 8311.0, experiment: run, epoch: 11, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 645ms, time_since_start: 02h 01m 36s 891ms, eta: 05h 15m 15s 434ms
2020-06-10T18:25:21 INFO: progress: 5900/22000, train/total_loss: 0.0257, train/total_loss/avg: 0.2711, train/hateful_memes/cross_entropy: 0.0257, train/hateful_memes/cross_entropy/avg: 0.2711, max mem: 8311.0, experiment: run, epoch: 12, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 104ms, time_since_start: 02h 03m 32s 996ms, eta: 05h 11m 51s 578ms
2020-06-10T18:27:16 INFO: progress: 6000/22000, train/total_loss: 0.0198, train/total_loss/avg: 0.2667, train/hateful_memes/cross_entropy: 0.0198, train/hateful_memes/cross_entropy/avg: 0.2667, max mem: 8311.0, experiment: run, epoch: 12, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 915ms, time_since_start: 02h 05m 27s 911ms, eta: 05h 06m 44s 798ms
2020-06-10T18:27:16 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T18:27:51 INFO: Evaluation time. Running on full validation set...
2020-06-10T18:28:49 INFO: progress: 6000/22000, val/total_loss: 1.8415, val/hateful_memes/cross_entropy: 1.8415, val/hateful_memes/accuracy: 0.6300, val/hateful_memes/binary_f1: 0.5244, val/hateful_memes/roc_auc: 0.6964, num_updates: 6000, epoch: 12, iterations: 6000, max_updates: 22000, val_time: 10s 398ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T18:28:51 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T18:30:45 INFO: progress: 6100/22000, train/total_loss: 0.0197, train/total_loss/avg: 0.2624, train/hateful_memes/cross_entropy: 0.0197, train/hateful_memes/cross_entropy/avg: 0.2624, max mem: 8311.0, experiment: run, epoch: 12, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 843ms, time_since_start: 02h 08m 57s 163ms, eta: 05h 07m 17s 522ms
2020-06-10T18:32:40 INFO: progress: 6200/22000, train/total_loss: 0.0197, train/total_loss/avg: 0.2603, train/hateful_memes/cross_entropy: 0.0197, train/hateful_memes/cross_entropy/avg: 0.2603, max mem: 8311.0, experiment: run, epoch: 12, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 934ms, time_since_start: 02h 10m 52s 098ms, eta: 05h 02m 57s 848ms
2020-06-10T18:34:35 INFO: progress: 6300/22000, train/total_loss: 0.0115, train/total_loss/avg: 0.2562, train/hateful_memes/cross_entropy: 0.0115, train/hateful_memes/cross_entropy/avg: 0.2562, max mem: 8311.0, experiment: run, epoch: 12, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 026ms, time_since_start: 02h 12m 47s 124ms, eta: 05h 01m 17s 214ms
2020-06-10T18:36:30 INFO: progress: 6400/22000, train/total_loss: 0.0101, train/total_loss/avg: 0.2523, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.2523, max mem: 8311.0, experiment: run, epoch: 13, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 099ms, time_since_start: 02h 14m 42s 223ms, eta: 04h 59m 33s 430ms
2020-06-10T18:38:26 INFO: progress: 6500/22000, train/total_loss: 0.0101, train/total_loss/avg: 0.2486, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.2486, max mem: 8311.0, experiment: run, epoch: 13, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 206ms, time_since_start: 02h 16m 37s 430ms, eta: 04h 57m 54s 855ms
2020-06-10T18:40:21 INFO: progress: 6600/22000, train/total_loss: 0.0082, train/total_loss/avg: 0.2449, train/hateful_memes/cross_entropy: 0.0082, train/hateful_memes/cross_entropy/avg: 0.2449, max mem: 8311.0, experiment: run, epoch: 13, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 066ms, time_since_start: 02h 18m 32s 496ms, eta: 04h 55m 37s 898ms
2020-06-10T18:42:15 INFO: progress: 6700/22000, train/total_loss: 0.0101, train/total_loss/avg: 0.2416, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.2416, max mem: 8311.0, experiment: run, epoch: 13, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 931ms, time_since_start: 02h 20m 27s 428ms, eta: 04h 53m 22s 134ms
2020-06-10T18:44:11 INFO: progress: 6800/22000, train/total_loss: 0.0101, train/total_loss/avg: 0.2381, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.2381, max mem: 8311.0, experiment: run, epoch: 13, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 229ms, time_since_start: 02h 22m 22s 658ms, eta: 04h 52m 12s 454ms
2020-06-10T18:46:06 INFO: progress: 6900/22000, train/total_loss: 0.0101, train/total_loss/avg: 0.2350, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.2350, max mem: 8311.0, experiment: run, epoch: 13, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 264ms, time_since_start: 02h 24m 17s 922ms, eta: 04h 50m 22s 378ms
2020-06-10T18:48:01 INFO: progress: 7000/22000, train/total_loss: 0.0101, train/total_loss/avg: 0.2318, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.2318, max mem: 8311.0, experiment: run, epoch: 14, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 752ms, time_since_start: 02h 26m 12s 675ms, eta: 04h 47m 10s 077ms
2020-06-10T18:48:01 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T18:48:35 INFO: Evaluation time. Running on full validation set...
2020-06-10T18:49:40 INFO: progress: 7000/22000, val/total_loss: 2.2542, val/hateful_memes/cross_entropy: 2.2542, val/hateful_memes/accuracy: 0.6020, val/hateful_memes/binary_f1: 0.4488, val/hateful_memes/roc_auc: 0.7041, num_updates: 7000, epoch: 14, iterations: 7000, max_updates: 22000, val_time: 12s 419ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T18:49:41 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T18:51:35 INFO: progress: 7100/22000, train/total_loss: 0.0101, train/total_loss/avg: 0.2287, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.2287, max mem: 8311.0, experiment: run, epoch: 14, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 428ms, time_since_start: 02h 29m 47s 396ms, eta: 04h 46m 56s 030ms
2020-06-10T18:53:30 INFO: progress: 7200/22000, train/total_loss: 0.0095, train/total_loss/avg: 0.2256, train/hateful_memes/cross_entropy: 0.0095, train/hateful_memes/cross_entropy/avg: 0.2256, max mem: 8311.0, experiment: run, epoch: 14, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 977ms, time_since_start: 02h 31m 42s 374ms, eta: 04h 43m 53s 757ms
2020-06-10T18:55:25 INFO: progress: 7300/22000, train/total_loss: 0.0095, train/total_loss/avg: 0.2232, train/hateful_memes/cross_entropy: 0.0095, train/hateful_memes/cross_entropy/avg: 0.2232, max mem: 8311.0, experiment: run, epoch: 14, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 743ms, time_since_start: 02h 33m 37s 118ms, eta: 04h 41m 24s 208ms
2020-06-10T18:57:20 INFO: progress: 7400/22000, train/total_loss: 0.0065, train/total_loss/avg: 0.2202, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.2202, max mem: 8311.0, experiment: run, epoch: 14, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 733ms, time_since_start: 02h 35m 31s 851ms, eta: 04h 39m 27s 850ms
2020-06-10T18:59:15 INFO: progress: 7500/22000, train/total_loss: 0.0095, train/total_loss/avg: 0.2194, train/hateful_memes/cross_entropy: 0.0095, train/hateful_memes/cross_entropy/avg: 0.2194, max mem: 8311.0, experiment: run, epoch: 15, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 880ms, time_since_start: 02h 37m 26s 732ms, eta: 04h 37m 54s 322ms
2020-06-10T19:01:10 INFO: progress: 7600/22000, train/total_loss: 0.0065, train/total_loss/avg: 0.2166, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.2166, max mem: 8311.0, experiment: run, epoch: 15, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 805ms, time_since_start: 02h 39m 21s 538ms, eta: 04h 35m 48s 584ms
2020-06-10T19:03:04 INFO: progress: 7700/22000, train/total_loss: 0.0058, train/total_loss/avg: 0.2138, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.2138, max mem: 8311.0, experiment: run, epoch: 15, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 865ms, time_since_start: 02h 41m 16s 403ms, eta: 04h 34m 02s 210ms
2020-06-10T19:05:00 INFO: progress: 7800/22000, train/total_loss: 0.0058, train/total_loss/avg: 0.2111, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.2111, max mem: 8311.0, experiment: run, epoch: 15, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 204ms, time_since_start: 02h 43m 11s 608ms, eta: 04h 32m 55s 364ms
2020-06-10T19:06:56 INFO: progress: 7900/22000, train/total_loss: 0.0057, train/total_loss/avg: 0.2084, train/hateful_memes/cross_entropy: 0.0057, train/hateful_memes/cross_entropy/avg: 0.2084, max mem: 8311.0, experiment: run, epoch: 15, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 347ms, time_since_start: 02h 45m 07s 955ms, eta: 04h 33m 41s 427ms
2020-06-10T19:08:51 INFO: progress: 8000/22000, train/total_loss: 0.0058, train/total_loss/avg: 0.2060, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.2060, max mem: 8311.0, experiment: run, epoch: 16, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 010ms, time_since_start: 02h 47m 02s 965ms, eta: 04h 28m 37s 529ms
2020-06-10T19:08:51 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T19:09:26 INFO: Evaluation time. Running on full validation set...
2020-06-10T19:10:27 INFO: progress: 8000/22000, val/total_loss: 1.8058, val/hateful_memes/cross_entropy: 1.8058, val/hateful_memes/accuracy: 0.6240, val/hateful_memes/binary_f1: 0.5179, val/hateful_memes/roc_auc: 0.7153, num_updates: 8000, epoch: 16, iterations: 8000, max_updates: 22000, val_time: 14s 432ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T19:10:29 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T19:12:25 INFO: progress: 8100/22000, train/total_loss: 0.0065, train/total_loss/avg: 0.2039, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.2039, max mem: 8311.0, experiment: run, epoch: 16, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00001, ups: 0.85, time: 01m 57s 085ms, time_since_start: 02h 50m 36s 458ms, eta: 04h 31m 31s 207ms
2020-06-10T19:14:20 INFO: progress: 8200/22000, train/total_loss: 0.0065, train/total_loss/avg: 0.2021, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.2021, max mem: 8311.0, experiment: run, epoch: 16, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 843ms, time_since_start: 02h 52m 32s 302ms, eta: 04h 26m 42s 407ms
2020-06-10T19:16:16 INFO: progress: 8300/22000, train/total_loss: 0.0034, train/total_loss/avg: 0.1997, train/hateful_memes/cross_entropy: 0.0034, train/hateful_memes/cross_entropy/avg: 0.1997, max mem: 8311.0, experiment: run, epoch: 16, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 442ms, time_since_start: 02h 54m 27s 744ms, eta: 04h 23m 51s 383ms
2020-06-10T19:18:11 INFO: progress: 8400/22000, train/total_loss: 0.0034, train/total_loss/avg: 0.1973, train/hateful_memes/cross_entropy: 0.0034, train/hateful_memes/cross_entropy/avg: 0.1973, max mem: 8311.0, experiment: run, epoch: 16, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 321ms, time_since_start: 02h 56m 23s 065ms, eta: 04h 21m 39s 380ms
2020-06-10T19:20:08 INFO: progress: 8500/22000, train/total_loss: 0.0034, train/total_loss/avg: 0.1951, train/hateful_memes/cross_entropy: 0.0034, train/hateful_memes/cross_entropy/avg: 0.1951, max mem: 8311.0, experiment: run, epoch: 16, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 802ms, time_since_start: 02h 58m 19s 867ms, eta: 04h 23m 04s 064ms
2020-06-10T19:22:04 INFO: progress: 8600/22000, train/total_loss: 0.0034, train/total_loss/avg: 0.1928, train/hateful_memes/cross_entropy: 0.0034, train/hateful_memes/cross_entropy/avg: 0.1928, max mem: 8311.0, experiment: run, epoch: 17, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 558ms, time_since_start: 03h 16s 426ms, eta: 04h 20m 34s 487ms
2020-06-10T19:24:00 INFO: progress: 8700/22000, train/total_loss: 0.0033, train/total_loss/avg: 0.1906, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1906, max mem: 8311.0, experiment: run, epoch: 17, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 620ms, time_since_start: 03h 02m 12s 047ms, eta: 04h 16m 32s 960ms
2020-06-10T19:25:54 INFO: progress: 8800/22000, train/total_loss: 0.0025, train/total_loss/avg: 0.1885, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1885, max mem: 8311.0, experiment: run, epoch: 17, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 315ms, time_since_start: 03h 04m 06s 363ms, eta: 04h 11m 44s 780ms
2020-06-10T19:27:51 INFO: progress: 8900/22000, train/total_loss: 0.0025, train/total_loss/avg: 0.1863, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1863, max mem: 8311.0, experiment: run, epoch: 17, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 184ms, time_since_start: 03h 06m 02s 547ms, eta: 04h 13m 55s 395ms
2020-06-10T19:29:47 INFO: progress: 9000/22000, train/total_loss: 0.0025, train/total_loss/avg: 0.1844, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1844, max mem: 8311.0, experiment: run, epoch: 17, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 153ms, time_since_start: 03h 07m 58s 701ms, eta: 04h 11m 55s 098ms
2020-06-10T19:29:47 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T19:30:22 INFO: Evaluation time. Running on full validation set...
2020-06-10T19:30:32 INFO: progress: 9000/22000, val/total_loss: 2.4940, val/hateful_memes/cross_entropy: 2.4940, val/hateful_memes/accuracy: 0.6020, val/hateful_memes/binary_f1: 0.4330, val/hateful_memes/roc_auc: 0.7200, num_updates: 9000, epoch: 17, iterations: 9000, max_updates: 22000, val_time: 10s 487ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T19:30:34 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T19:32:28 INFO: progress: 9100/22000, train/total_loss: 0.0021, train/total_loss/avg: 0.1823, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1823, max mem: 8311.0, experiment: run, epoch: 18, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 929ms, time_since_start: 03h 10m 40s 389ms, eta: 04h 09m 29s 831ms
2020-06-10T19:34:25 INFO: progress: 9200/22000, train/total_loss: 0.0025, train/total_loss/avg: 0.1804, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1804, max mem: 8311.0, experiment: run, epoch: 18, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 981ms, time_since_start: 03h 12m 37s 370ms, eta: 04h 09m 48s 565ms
2020-06-10T19:36:22 INFO: progress: 9300/22000, train/total_loss: 0.0025, train/total_loss/avg: 0.1786, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1786, max mem: 8311.0, experiment: run, epoch: 18, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 787ms, time_since_start: 03h 14m 34s 158ms, eta: 04h 07m 26s 805ms
2020-06-10T19:38:19 INFO: progress: 9400/22000, train/total_loss: 0.0025, train/total_loss/avg: 0.1795, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1795, max mem: 8311.0, experiment: run, epoch: 18, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 539ms, time_since_start: 03h 16m 30s 697ms, eta: 04h 04m 58s 710ms
2020-06-10T19:40:14 INFO: progress: 9500/22000, train/total_loss: 0.0025, train/total_loss/avg: 0.1776, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1776, max mem: 8311.0, experiment: run, epoch: 18, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 229ms, time_since_start: 03h 18m 25s 927ms, eta: 04h 18s 066ms
2020-06-10T19:42:08 INFO: progress: 9600/22000, train/total_loss: 0.0025, train/total_loss/avg: 0.1758, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1758, max mem: 8311.0, experiment: run, epoch: 19, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 362ms, time_since_start: 03h 20m 20s 289ms, eta: 03h 56m 35s 136ms
2020-06-10T19:44:03 INFO: progress: 9700/22000, train/total_loss: 0.0025, train/total_loss/avg: 0.1740, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1740, max mem: 8311.0, experiment: run, epoch: 19, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 495ms, time_since_start: 03h 22m 14s 785ms, eta: 03h 54m 57s 012ms
2020-06-10T19:45:57 INFO: progress: 9800/22000, train/total_loss: 0.0025, train/total_loss/avg: 0.1723, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1723, max mem: 8311.0, experiment: run, epoch: 19, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 444ms, time_since_start: 03h 24m 09s 229ms, eta: 03h 52m 56s 159ms
2020-06-10T19:47:52 INFO: progress: 9900/22000, train/total_loss: 0.0035, train/total_loss/avg: 0.1706, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1706, max mem: 8311.0, experiment: run, epoch: 19, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 622ms, time_since_start: 03h 26m 03s 851ms, eta: 03h 51m 23s 177ms
2020-06-10T19:49:47 INFO: progress: 10000/22000, train/total_loss: 0.0035, train/total_loss/avg: 0.1690, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1690, max mem: 8311.0, experiment: run, epoch: 19, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 746ms, time_since_start: 03h 27m 58s 598ms, eta: 03h 49m 43s 338ms
2020-06-10T19:49:47 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T19:50:22 INFO: Evaluation time. Running on full validation set...
2020-06-10T19:50:34 INFO: progress: 10000/22000, val/total_loss: 2.1098, val/hateful_memes/cross_entropy: 2.1098, val/hateful_memes/accuracy: 0.6300, val/hateful_memes/binary_f1: 0.5244, val/hateful_memes/roc_auc: 0.7152, num_updates: 10000, epoch: 19, iterations: 10000, max_updates: 22000, val_time: 12s 330ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T19:50:35 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T19:52:29 INFO: progress: 10100/22000, train/total_loss: 0.0025, train/total_loss/avg: 0.1674, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1674, max mem: 8311.0, experiment: run, epoch: 19, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 113ms, time_since_start: 03h 30m 41s 192ms, eta: 03h 48m 32s 252ms
2020-06-10T19:54:25 INFO: progress: 10200/22000, train/total_loss: 0.0025, train/total_loss/avg: 0.1660, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1660, max mem: 8311.0, experiment: run, epoch: 20, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 763ms, time_since_start: 03h 32m 36s 955ms, eta: 03h 47m 53s 761ms
2020-06-10T19:56:20 INFO: progress: 10300/22000, train/total_loss: 0.0025, train/total_loss/avg: 0.1644, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1644, max mem: 8311.0, experiment: run, epoch: 20, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 188ms, time_since_start: 03h 34m 32s 144ms, eta: 03h 44m 50s 585ms
2020-06-10T19:58:15 INFO: progress: 10400/22000, train/total_loss: 0.0024, train/total_loss/avg: 0.1629, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1629, max mem: 8311.0, experiment: run, epoch: 20, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 116ms, time_since_start: 03h 36m 27s 261ms, eta: 03h 42m 46s 912ms
2020-06-10T20:00:11 INFO: progress: 10500/22000, train/total_loss: 0.0018, train/total_loss/avg: 0.1613, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1613, max mem: 8311.0, experiment: run, epoch: 20, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 309ms, time_since_start: 03h 38m 22s 571ms, eta: 03h 41m 13s 895ms
2020-06-10T20:02:06 INFO: progress: 10600/22000, train/total_loss: 0.0018, train/total_loss/avg: 0.1598, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1598, max mem: 8311.0, experiment: run, epoch: 20, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 539ms, time_since_start: 03h 40m 18s 110ms, eta: 03h 39m 44s 624ms
2020-06-10T20:04:02 INFO: progress: 10700/22000, train/total_loss: 0.0018, train/total_loss/avg: 0.1583, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1583, max mem: 8311.0, experiment: run, epoch: 21, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 380ms, time_since_start: 03h 42m 13s 491ms, eta: 03h 37m 31s 068ms
2020-06-10T20:05:56 INFO: progress: 10800/22000, train/total_loss: 0.0024, train/total_loss/avg: 0.1569, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1569, max mem: 8311.0, experiment: run, epoch: 21, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 641ms, time_since_start: 03h 44m 08s 132ms, eta: 03h 34m 12s 656ms
2020-06-10T20:07:51 INFO: progress: 10900/22000, train/total_loss: 0.0027, train/total_loss/avg: 0.1555, train/hateful_memes/cross_entropy: 0.0027, train/hateful_memes/cross_entropy/avg: 0.1555, max mem: 8311.0, experiment: run, epoch: 21, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 463ms, time_since_start: 03h 46m 02s 596ms, eta: 03h 31m 58s 160ms
2020-06-10T20:09:46 INFO: progress: 11000/22000, train/total_loss: 0.0024, train/total_loss/avg: 0.1541, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1541, max mem: 8311.0, experiment: run, epoch: 21, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 937ms, time_since_start: 03h 47m 57s 533ms, eta: 03h 30m 55s 763ms
2020-06-10T20:09:46 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T20:10:21 INFO: Evaluation time. Running on full validation set...
2020-06-10T20:10:33 INFO: progress: 11000/22000, val/total_loss: 2.3792, val/hateful_memes/cross_entropy: 2.3792, val/hateful_memes/accuracy: 0.6200, val/hateful_memes/binary_f1: 0.4892, val/hateful_memes/roc_auc: 0.7160, num_updates: 11000, epoch: 21, iterations: 11000, max_updates: 22000, val_time: 12s 616ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T20:10:35 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T20:12:29 INFO: progress: 11100/22000, train/total_loss: 0.0024, train/total_loss/avg: 0.1527, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1527, max mem: 8311.0, experiment: run, epoch: 21, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 199ms, time_since_start: 03h 50m 40s 647ms, eta: 03h 29m 29s 349ms
2020-06-10T20:14:23 INFO: progress: 11200/22000, train/total_loss: 0.0018, train/total_loss/avg: 0.1513, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1513, max mem: 8311.0, experiment: run, epoch: 22, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 752ms, time_since_start: 03h 52m 35s 399ms, eta: 03h 26m 45s 632ms
2020-06-10T20:16:18 INFO: progress: 11300/22000, train/total_loss: 0.0018, train/total_loss/avg: 0.1500, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1500, max mem: 8311.0, experiment: run, epoch: 22, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 680ms, time_since_start: 03h 54m 30s 079ms, eta: 03h 24m 43s 063ms
2020-06-10T20:18:13 INFO: progress: 11400/22000, train/total_loss: 0.0018, train/total_loss/avg: 0.1488, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1488, max mem: 8311.0, experiment: run, epoch: 22, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 280ms, time_since_start: 03h 56m 25s 360ms, eta: 03h 23m 52s 001ms
2020-06-10T20:20:10 INFO: progress: 11500/22000, train/total_loss: 0.0024, train/total_loss/avg: 0.1475, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1475, max mem: 8311.0, experiment: run, epoch: 22, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 263ms, time_since_start: 03h 58m 21s 623ms, eta: 03h 23m 39s 834ms
2020-06-10T20:22:05 INFO: progress: 11600/22000, train/total_loss: 0.0018, train/total_loss/avg: 0.1462, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1462, max mem: 8311.0, experiment: run, epoch: 22, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 589ms, time_since_start: 04h 17s 213ms, eta: 03h 20m 33s 354ms
2020-06-10T20:24:01 INFO: progress: 11700/22000, train/total_loss: 0.0018, train/total_loss/avg: 0.1450, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1450, max mem: 8311.0, experiment: run, epoch: 22, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 923ms, time_since_start: 04h 02m 13s 137ms, eta: 03h 19m 12s 100ms
2020-06-10T20:25:57 INFO: progress: 11800/22000, train/total_loss: 0.0015, train/total_loss/avg: 0.1438, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1438, max mem: 8311.0, experiment: run, epoch: 23, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 867ms, time_since_start: 04h 04m 09s 004ms, eta: 03h 17m 10s 310ms
2020-06-10T20:27:54 INFO: progress: 11900/22000, train/total_loss: 0.0015, train/total_loss/avg: 0.1429, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1429, max mem: 8311.0, experiment: run, epoch: 23, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 532ms, time_since_start: 04h 06m 05s 537ms, eta: 03h 16m 21s 584ms
2020-06-10T20:29:50 INFO: progress: 12000/22000, train/total_loss: 0.0015, train/total_loss/avg: 0.1417, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1417, max mem: 8311.0, experiment: run, epoch: 23, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 648ms, time_since_start: 04h 08m 02s 186ms, eta: 03h 14m 36s 502ms
2020-06-10T20:29:50 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T20:30:25 INFO: Evaluation time. Running on full validation set...
2020-06-10T20:30:37 INFO: progress: 12000/22000, val/total_loss: 2.8295, val/hateful_memes/cross_entropy: 2.8295, val/hateful_memes/accuracy: 0.5880, val/hateful_memes/binary_f1: 0.3977, val/hateful_memes/roc_auc: 0.6921, num_updates: 12000, epoch: 23, iterations: 12000, max_updates: 22000, val_time: 11s 788ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T20:30:39 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T20:32:33 INFO: progress: 12100/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1405, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1405, max mem: 8311.0, experiment: run, epoch: 23, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 529ms, time_since_start: 04h 10m 44s 846ms, eta: 03h 10m 48s 848ms
2020-06-10T20:34:30 INFO: progress: 12200/22000, train/total_loss: 0.0011, train/total_loss/avg: 0.1394, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.1394, max mem: 8311.0, experiment: run, epoch: 23, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0., ups: 0.85, time: 01m 57s 195ms, time_since_start: 04h 12m 42s 041ms, eta: 03h 11m 36s 656ms
2020-06-10T20:36:28 INFO: progress: 12300/22000, train/total_loss: 0.0010, train/total_loss/avg: 0.1383, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1383, max mem: 8311.0, experiment: run, epoch: 24, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0., ups: 0.85, time: 01m 57s 405ms, time_since_start: 04h 14m 39s 447ms, eta: 03h 09m 59s 709ms
2020-06-10T20:38:23 INFO: progress: 12400/22000, train/total_loss: 0.0010, train/total_loss/avg: 0.1372, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1372, max mem: 8311.0, experiment: run, epoch: 24, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 666ms, time_since_start: 04h 16m 35s 113ms, eta: 03h 05m 15s 047ms
2020-06-10T20:40:18 INFO: progress: 12500/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1362, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1362, max mem: 8311.0, experiment: run, epoch: 24, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 363ms, time_since_start: 04h 18m 29s 476ms, eta: 03h 01m 15s 402ms
2020-06-10T20:42:14 INFO: progress: 12600/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1352, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1352, max mem: 8311.0, experiment: run, epoch: 24, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 158ms, time_since_start: 04h 20m 25s 635ms, eta: 03h 02m 09s 819ms
2020-06-10T20:44:11 INFO: progress: 12700/22000, train/total_loss: 0.0015, train/total_loss/avg: 0.1351, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1351, max mem: 8311.0, experiment: run, epoch: 24, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 981ms, time_since_start: 04h 22m 22s 616ms, eta: 03h 01m 30s 158ms
2020-06-10T20:46:07 INFO: progress: 12800/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1340, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1340, max mem: 8311.0, experiment: run, epoch: 25, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 106ms, time_since_start: 04h 24m 18s 723ms, eta: 02h 58m 12s 464ms
2020-06-10T20:48:02 INFO: progress: 12900/22000, train/total_loss: 0.0010, train/total_loss/avg: 0.1330, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1330, max mem: 8311.0, experiment: run, epoch: 25, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 538ms, time_since_start: 04h 26m 14s 261ms, eta: 02h 55m 24s 485ms
2020-06-10T20:49:58 INFO: progress: 13000/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1320, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1320, max mem: 8311.0, experiment: run, epoch: 25, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 379ms, time_since_start: 04h 28m 09s 641ms, eta: 02h 53m 14s 567ms
2020-06-10T20:49:58 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T20:50:31 INFO: Evaluation time. Running on full validation set...
2020-06-10T20:50:42 INFO: progress: 13000/22000, val/total_loss: 2.5878, val/hateful_memes/cross_entropy: 2.5878, val/hateful_memes/accuracy: 0.6180, val/hateful_memes/binary_f1: 0.5013, val/hateful_memes/roc_auc: 0.7026, num_updates: 13000, epoch: 25, iterations: 13000, max_updates: 22000, val_time: 10s 533ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T20:50:43 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T20:52:38 INFO: progress: 13100/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1310, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1310, max mem: 8311.0, experiment: run, epoch: 25, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 824ms, time_since_start: 04h 30m 49s 564ms, eta: 02h 51m 58s 658ms
2020-06-10T20:54:33 INFO: progress: 13200/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1300, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1300, max mem: 8311.0, experiment: run, epoch: 25, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 287ms, time_since_start: 04h 32m 44s 852ms, eta: 02h 49m 15s 414ms
2020-06-10T20:56:28 INFO: progress: 13300/22000, train/total_loss: 0.0012, train/total_loss/avg: 0.1298, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1298, max mem: 8311.0, experiment: run, epoch: 25, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 745ms, time_since_start: 04h 34m 39s 597ms, eta: 02h 46m 32s 848ms
2020-06-10T20:58:24 INFO: progress: 13400/22000, train/total_loss: 0.0012, train/total_loss/avg: 0.1288, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1288, max mem: 8311.0, experiment: run, epoch: 26, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 216ms, time_since_start: 04h 36m 35s 814ms, eta: 02h 46m 44s 614ms
2020-06-10T21:00:19 INFO: progress: 13500/22000, train/total_loss: 0.0008, train/total_loss/avg: 0.1279, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1279, max mem: 8311.0, experiment: run, epoch: 26, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 491ms, time_since_start: 04h 38m 31s 305ms, eta: 02h 43m 46s 561ms
2020-06-10T21:02:15 INFO: progress: 13600/22000, train/total_loss: 0.0008, train/total_loss/avg: 0.1269, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1269, max mem: 8311.0, experiment: run, epoch: 26, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 190ms, time_since_start: 04h 40m 26s 496ms, eta: 02h 41m 25s 694ms
2020-06-10T21:04:09 INFO: progress: 13700/22000, train/total_loss: 0.0008, train/total_loss/avg: 0.1260, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1260, max mem: 8311.0, experiment: run, epoch: 26, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 913ms, time_since_start: 04h 42m 21s 409ms, eta: 02h 39m 07s 348ms
2020-06-10T21:06:04 INFO: progress: 13800/22000, train/total_loss: 0.0008, train/total_loss/avg: 0.1251, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1251, max mem: 8311.0, experiment: run, epoch: 26, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 785ms, time_since_start: 04h 44m 16s 195ms, eta: 02h 37m 01s 851ms
2020-06-10T21:07:59 INFO: progress: 13900/22000, train/total_loss: 0.0008, train/total_loss/avg: 0.1243, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1243, max mem: 8311.0, experiment: run, epoch: 27, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 576ms, time_since_start: 04h 46m 10s 771ms, eta: 02h 34m 49s 965ms
2020-06-10T21:09:54 INFO: progress: 14000/22000, train/total_loss: 0.0008, train/total_loss/avg: 0.1236, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1236, max mem: 8311.0, experiment: run, epoch: 27, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 013ms, time_since_start: 04h 48m 05s 785ms, eta: 02h 33m 30s 288ms
2020-06-10T21:09:54 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T21:10:31 INFO: Evaluation time. Running on full validation set...
2020-06-10T21:10:42 INFO: progress: 14000/22000, val/total_loss: 2.8673, val/hateful_memes/cross_entropy: 2.8673, val/hateful_memes/accuracy: 0.5940, val/hateful_memes/binary_f1: 0.4116, val/hateful_memes/roc_auc: 0.7040, num_updates: 14000, epoch: 27, iterations: 14000, max_updates: 22000, val_time: 10s 442ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T21:10:43 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T21:12:38 INFO: progress: 14100/22000, train/total_loss: 0.0010, train/total_loss/avg: 0.1227, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1227, max mem: 8311.0, experiment: run, epoch: 27, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 203ms, time_since_start: 04h 50m 50s 237ms, eta: 02h 33m 09s 288ms
2020-06-10T21:14:33 INFO: progress: 14200/22000, train/total_loss: 0.0010, train/total_loss/avg: 0.1218, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1218, max mem: 8311.0, experiment: run, epoch: 27, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 560ms, time_since_start: 04h 52m 44s 797ms, eta: 02h 29m 04s 625ms
2020-06-10T21:16:29 INFO: progress: 14300/22000, train/total_loss: 0.0012, train/total_loss/avg: 0.1210, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1210, max mem: 8311.0, experiment: run, epoch: 27, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 678ms, time_since_start: 04h 54m 40s 476ms, eta: 02h 28m 36s 141ms
2020-06-10T21:18:25 INFO: progress: 14400/22000, train/total_loss: 0.0012, train/total_loss/avg: 0.1202, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1202, max mem: 8311.0, experiment: run, epoch: 28, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 632ms, time_since_start: 04h 56m 37s 108ms, eta: 02h 27m 52s 940ms
2020-06-10T21:20:20 INFO: progress: 14500/22000, train/total_loss: 0.0012, train/total_loss/avg: 0.1194, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1194, max mem: 8311.0, experiment: run, epoch: 28, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 999ms, time_since_start: 04h 58m 32s 108ms, eta: 02h 23m 53s 577ms
2020-06-10T21:22:14 INFO: progress: 14600/22000, train/total_loss: 0.0012, train/total_loss/avg: 0.1185, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1185, max mem: 8311.0, experiment: run, epoch: 28, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 259ms, time_since_start: 05h 26s 367ms, eta: 02h 21m 03s 623ms
2020-06-10T21:24:09 INFO: progress: 14700/22000, train/total_loss: 0.0010, train/total_loss/avg: 0.1177, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1177, max mem: 8311.0, experiment: run, epoch: 28, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 775ms, time_since_start: 05h 02m 21s 142ms, eta: 02h 19m 47s 005ms
2020-06-10T21:26:04 INFO: progress: 14800/22000, train/total_loss: 0.0010, train/total_loss/avg: 0.1169, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1169, max mem: 8311.0, experiment: run, epoch: 28, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 675ms, time_since_start: 05h 04m 15s 818ms, eta: 02h 17m 44s 870ms
2020-06-10T21:27:58 INFO: progress: 14900/22000, train/total_loss: 0.0012, train/total_loss/avg: 0.1162, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1162, max mem: 8311.0, experiment: run, epoch: 29, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 521ms, time_since_start: 05h 06m 10s 339ms, eta: 02h 15m 39s 156ms
2020-06-10T21:29:53 INFO: progress: 15000/22000, train/total_loss: 0.0010, train/total_loss/avg: 0.1154, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1154, max mem: 8311.0, experiment: run, epoch: 29, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 866ms, time_since_start: 05h 08m 05s 205ms, eta: 02h 14m 08s 664ms
2020-06-10T21:29:53 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T21:30:29 INFO: Evaluation time. Running on full validation set...
2020-06-10T21:30:39 INFO: progress: 15000/22000, val/total_loss: 2.9737, val/hateful_memes/cross_entropy: 2.9737, val/hateful_memes/accuracy: 0.5980, val/hateful_memes/binary_f1: 0.4370, val/hateful_memes/roc_auc: 0.7099, num_updates: 15000, epoch: 29, iterations: 15000, max_updates: 22000, val_time: 10s 764ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T21:30:41 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T21:32:35 INFO: progress: 15100/22000, train/total_loss: 0.0008, train/total_loss/avg: 0.1146, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1146, max mem: 8311.0, experiment: run, epoch: 29, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 201ms, time_since_start: 05h 10m 46s 830ms, eta: 02h 12m 36s 830ms
2020-06-10T21:34:30 INFO: progress: 15200/22000, train/total_loss: 0.0010, train/total_loss/avg: 0.1140, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1140, max mem: 8311.0, experiment: run, epoch: 29, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 672ms, time_since_start: 05h 12m 41s 502ms, eta: 02h 10m 05s 529ms
2020-06-10T21:36:24 INFO: progress: 15300/22000, train/total_loss: 0.0010, train/total_loss/avg: 0.1133, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1133, max mem: 8311.0, experiment: run, epoch: 29, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 786ms, time_since_start: 05h 14m 36s 289ms, eta: 02h 08m 18s 409ms
2020-06-10T21:38:19 INFO: progress: 15400/22000, train/total_loss: 0.0008, train/total_loss/avg: 0.1125, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1125, max mem: 8311.0, experiment: run, epoch: 29, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 855ms, time_since_start: 05h 16m 31s 144ms, eta: 02h 06m 28s 012ms
2020-06-10T21:40:14 INFO: progress: 15500/22000, train/total_loss: 0.0008, train/total_loss/avg: 0.1118, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1118, max mem: 8311.0, experiment: run, epoch: 30, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 013ms, time_since_start: 05h 18m 26s 158ms, eta: 02h 04m 43s 334ms
2020-06-10T21:42:09 INFO: progress: 15600/22000, train/total_loss: 0.0008, train/total_loss/avg: 0.1111, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1111, max mem: 8311.0, experiment: run, epoch: 30, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 845ms, time_since_start: 05h 20m 21s 003ms, eta: 02h 02m 37s 443ms
2020-06-10T21:44:04 INFO: progress: 15700/22000, train/total_loss: 0.0008, train/total_loss/avg: 0.1104, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1104, max mem: 8311.0, experiment: run, epoch: 30, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 757ms, time_since_start: 05h 22m 15s 760ms, eta: 02h 36s 958ms
2020-06-10T21:45:59 INFO: progress: 15800/22000, train/total_loss: 0.0006, train/total_loss/avg: 0.1097, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1097, max mem: 8311.0, experiment: run, epoch: 30, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 987ms, time_since_start: 05h 24m 10s 748ms, eta: 01h 58m 56s 359ms
2020-06-10T21:47:54 INFO: progress: 15900/22000, train/total_loss: 0.0006, train/total_loss/avg: 0.1090, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1090, max mem: 8311.0, experiment: run, epoch: 30, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 364ms, time_since_start: 05h 26m 06s 113ms, eta: 01h 57m 24s 281ms
2020-06-10T21:49:49 INFO: progress: 16000/22000, train/total_loss: 0.0006, train/total_loss/avg: 0.1083, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1083, max mem: 8311.0, experiment: run, epoch: 31, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 857ms, time_since_start: 05h 28m 970ms, eta: 01h 54m 58s 336ms
2020-06-10T21:49:49 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T21:50:27 INFO: Evaluation time. Running on full validation set...
2020-06-10T21:50:37 INFO: progress: 16000/22000, val/total_loss: 2.8266, val/hateful_memes/cross_entropy: 2.8266, val/hateful_memes/accuracy: 0.6080, val/hateful_memes/binary_f1: 0.4674, val/hateful_memes/roc_auc: 0.7122, num_updates: 16000, epoch: 31, iterations: 16000, max_updates: 22000, val_time: 10s 526ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T21:50:39 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T21:52:32 INFO: progress: 16100/22000, train/total_loss: 0.0006, train/total_loss/avg: 0.1077, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1077, max mem: 8311.0, experiment: run, epoch: 31, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 078ms, time_since_start: 05h 30m 44s 425ms, eta: 01h 53m 16s 393ms
2020-06-10T21:54:27 INFO: progress: 16200/22000, train/total_loss: 0.0006, train/total_loss/avg: 0.1070, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1070, max mem: 8311.0, experiment: run, epoch: 31, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 842ms, time_since_start: 05h 32m 39s 267ms, eta: 01h 51m 07s 518ms
2020-06-10T21:56:22 INFO: progress: 16300/22000, train/total_loss: 0.0005, train/total_loss/avg: 0.1063, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1063, max mem: 8311.0, experiment: run, epoch: 31, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 653ms, time_since_start: 05h 34m 33s 920ms, eta: 01h 49m 01s 766ms
2020-06-10T21:58:17 INFO: progress: 16400/22000, train/total_loss: 0.0004, train/total_loss/avg: 0.1057, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1057, max mem: 8311.0, experiment: run, epoch: 31, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 701ms, time_since_start: 05h 36m 28s 622ms, eta: 01h 47m 09s 732ms
2020-06-10T22:00:11 INFO: progress: 16500/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.1050, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1050, max mem: 8311.0, experiment: run, epoch: 32, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 589ms, time_since_start: 05h 38m 23s 212ms, eta: 01h 45m 08s 733ms
2020-06-10T22:02:06 INFO: progress: 16600/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.1044, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1044, max mem: 8311.0, experiment: run, epoch: 32, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 800ms, time_since_start: 05h 40m 18s 012ms, eta: 01h 43m 25s 416ms
2020-06-10T22:04:01 INFO: progress: 16700/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.1038, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1038, max mem: 8311.0, experiment: run, epoch: 32, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 913ms, time_since_start: 05h 42m 12s 926ms, eta: 01h 41m 36s 531ms
2020-06-10T22:05:56 INFO: progress: 16800/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.1032, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1032, max mem: 8311.0, experiment: run, epoch: 32, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 775ms, time_since_start: 05h 44m 07s 702ms, eta: 01h 39m 34s 319ms
2020-06-10T22:07:51 INFO: progress: 16900/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.1025, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1025, max mem: 8311.0, experiment: run, epoch: 32, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 779ms, time_since_start: 05h 46m 02s 482ms, eta: 01h 37m 39s 624ms
2020-06-10T22:09:46 INFO: progress: 17000/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.1019, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1019, max mem: 8311.0, experiment: run, epoch: 32, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 708ms, time_since_start: 05h 47m 58s 190ms, eta: 01h 36m 31s 201ms
2020-06-10T22:09:46 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T22:10:22 INFO: Evaluation time. Running on full validation set...
2020-06-10T22:10:32 INFO: progress: 17000/22000, val/total_loss: 2.9242, val/hateful_memes/cross_entropy: 2.9242, val/hateful_memes/accuracy: 0.6000, val/hateful_memes/binary_f1: 0.4681, val/hateful_memes/roc_auc: 0.7048, num_updates: 17000, epoch: 32, iterations: 17000, max_updates: 22000, val_time: 10s 514ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T22:10:34 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T22:12:28 INFO: progress: 17100/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.1013, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1013, max mem: 8311.0, experiment: run, epoch: 33, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 873ms, time_since_start: 05h 50m 40s 138ms, eta: 01h 34m 43s 475ms
2020-06-10T22:14:25 INFO: progress: 17200/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.1008, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1008, max mem: 8311.0, experiment: run, epoch: 33, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 592ms, time_since_start: 05h 52m 36s 730ms, eta: 01h 33m 22s 031ms
2020-06-10T22:16:22 INFO: progress: 17300/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.1002, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1002, max mem: 8311.0, experiment: run, epoch: 33, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0., ups: 0.85, time: 01m 57s 523ms, time_since_start: 05h 54m 34s 254ms, eta: 01h 32m 09s 126ms
2020-06-10T22:18:17 INFO: progress: 17400/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0996, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0996, max mem: 8311.0, experiment: run, epoch: 33, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 010ms, time_since_start: 05h 56m 29s 264ms, eta: 01h 28m 15s 762ms
2020-06-10T22:20:12 INFO: progress: 17500/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0990, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0990, max mem: 8311.0, experiment: run, epoch: 33, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 972ms, time_since_start: 05h 58m 24s 237ms, eta: 01h 26m 18s 938ms
2020-06-10T22:22:07 INFO: progress: 17600/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0985, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0985, max mem: 8311.0, experiment: run, epoch: 34, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 054ms, time_since_start: 06h 19s 291ms, eta: 01h 24m 27s 443ms
2020-06-10T22:24:02 INFO: progress: 17700/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0979, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0979, max mem: 8311.0, experiment: run, epoch: 34, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 809ms, time_since_start: 06h 02m 14s 100ms, eta: 01h 22m 21s 752ms
2020-06-10T22:25:57 INFO: progress: 17800/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0974, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0974, max mem: 8311.0, experiment: run, epoch: 34, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 881ms, time_since_start: 06h 04m 08s 981ms, eta: 01h 20m 29s 829ms
2020-06-10T22:27:51 INFO: progress: 17900/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0968, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0968, max mem: 8311.0, experiment: run, epoch: 34, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 310ms, time_since_start: 06h 06m 03s 292ms, eta: 01h 18m 11s 400ms
2020-06-10T22:29:46 INFO: progress: 18000/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0963, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0963, max mem: 8311.0, experiment: run, epoch: 34, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 218ms, time_since_start: 06h 07m 57s 510ms, eta: 01h 16m 13s 314ms
2020-06-10T22:29:46 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T22:30:21 INFO: Evaluation time. Running on full validation set...
2020-06-10T22:30:32 INFO: progress: 18000/22000, val/total_loss: 3.1460, val/hateful_memes/cross_entropy: 3.1460, val/hateful_memes/accuracy: 0.6060, val/hateful_memes/binary_f1: 0.4543, val/hateful_memes/roc_auc: 0.7056, num_updates: 18000, epoch: 34, iterations: 18000, max_updates: 22000, val_time: 10s 374ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T22:30:33 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T22:32:27 INFO: progress: 18100/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0957, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0957, max mem: 8311.0, experiment: run, epoch: 35, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 336ms, time_since_start: 06h 10m 39s 176ms, eta: 01h 15m 02s 611ms
2020-06-10T22:34:22 INFO: progress: 18200/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0952, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0952, max mem: 8311.0, experiment: run, epoch: 35, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 648ms, time_since_start: 06h 12m 33s 825ms, eta: 01h 12m 41s 018ms
2020-06-10T22:36:17 INFO: progress: 18300/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0947, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0947, max mem: 8311.0, experiment: run, epoch: 35, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 744ms, time_since_start: 06h 14m 28s 569ms, eta: 01h 10m 49s 785ms
2020-06-10T22:38:11 INFO: progress: 18400/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0942, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0942, max mem: 8311.0, experiment: run, epoch: 35, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 815ms, time_since_start: 06h 16m 23s 384ms, eta: 01h 08m 57s 485ms
2020-06-10T22:40:07 INFO: progress: 18500/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0937, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0937, max mem: 8311.0, experiment: run, epoch: 35, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 123ms, time_since_start: 06h 18m 18s 507ms, eta: 01h 07m 13s 334ms
2020-06-10T22:42:01 INFO: progress: 18600/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0932, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0932, max mem: 8311.0, experiment: run, epoch: 35, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 804ms, time_since_start: 06h 20m 13s 312ms, eta: 01h 05m 07s 268ms
2020-06-10T22:43:56 INFO: progress: 18700/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0927, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0927, max mem: 8311.0, experiment: run, epoch: 36, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 763ms, time_since_start: 06h 22m 08s 076ms, eta: 01h 03m 10s 994ms
2020-06-10T22:45:51 INFO: progress: 18800/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0922, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0922, max mem: 8311.0, experiment: run, epoch: 36, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 594ms, time_since_start: 06h 24m 02s 670ms, eta: 01h 01m 10s 683ms
2020-06-10T22:47:45 INFO: progress: 18900/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0917, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0917, max mem: 8311.0, experiment: run, epoch: 36, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 276ms, time_since_start: 06h 25m 56s 947ms, eta: 59m 06s 117ms
2020-06-10T22:49:40 INFO: progress: 19000/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0912, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0912, max mem: 8311.0, experiment: run, epoch: 36, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 728ms, time_since_start: 06h 27m 51s 675ms, eta: 57m 25s 289ms
2020-06-10T22:49:40 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T22:50:15 INFO: Evaluation time. Running on full validation set...
2020-06-10T22:50:26 INFO: progress: 19000/22000, val/total_loss: 3.2516, val/hateful_memes/cross_entropy: 3.2516, val/hateful_memes/accuracy: 0.5920, val/hateful_memes/binary_f1: 0.4333, val/hateful_memes/roc_auc: 0.7078, num_updates: 19000, epoch: 36, iterations: 19000, max_updates: 22000, val_time: 10s 345ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T22:50:27 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T22:52:21 INFO: progress: 19100/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0907, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0907, max mem: 8311.0, experiment: run, epoch: 36, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 309ms, time_since_start: 06h 30m 32s 989ms, eta: 55m 47s 319ms
2020-06-10T22:54:15 INFO: progress: 19200/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0902, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0902, max mem: 8311.0, experiment: run, epoch: 37, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 358ms, time_since_start: 06h 32m 27s 348ms, eta: 53m 25s 243ms
2020-06-10T22:56:10 INFO: progress: 19300/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0898, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0898, max mem: 8311.0, experiment: run, epoch: 37, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 660ms, time_since_start: 06h 34m 22s 008ms, eta: 51m 38s 927ms
2020-06-10T22:58:05 INFO: progress: 19400/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0893, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0893, max mem: 8311.0, experiment: run, epoch: 37, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 669ms, time_since_start: 06h 36m 16s 677ms, eta: 49m 44s 377ms
2020-06-10T22:59:59 INFO: progress: 19500/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0888, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0888, max mem: 8311.0, experiment: run, epoch: 37, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 387ms, time_since_start: 06h 38m 11s 065ms, eta: 47m 42s 558ms
2020-06-10T23:01:54 INFO: progress: 19600/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0884, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0884, max mem: 8311.0, experiment: run, epoch: 37, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 897ms, time_since_start: 06h 40m 05s 962ms, eta: 46m 288ms
2020-06-10T23:03:49 INFO: progress: 19700/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0879, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0879, max mem: 8311.0, experiment: run, epoch: 38, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 661ms, time_since_start: 06h 42m 624ms, eta: 43m 59s 845ms
2020-06-10T23:05:44 INFO: progress: 19800/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0875, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0875, max mem: 8311.0, experiment: run, epoch: 38, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 093ms, time_since_start: 06h 43m 55s 717ms, eta: 42m 14s 593ms
2020-06-10T23:07:39 INFO: progress: 19900/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0871, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0871, max mem: 8311.0, experiment: run, epoch: 38, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 958ms, time_since_start: 06h 45m 50s 676ms, eta: 40m 16s 542ms
2020-06-10T23:09:35 INFO: progress: 20000/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0867, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0867, max mem: 8311.0, experiment: run, epoch: 38, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 265ms, time_since_start: 06h 47m 46s 942ms, eta: 38m 47s 644ms
2020-06-10T23:09:35 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T23:10:13 INFO: Evaluation time. Running on full validation set...
2020-06-10T23:10:23 INFO: progress: 20000/22000, val/total_loss: 3.1680, val/hateful_memes/cross_entropy: 3.1680, val/hateful_memes/accuracy: 0.6060, val/hateful_memes/binary_f1: 0.4661, val/hateful_memes/roc_auc: 0.7034, num_updates: 20000, epoch: 38, iterations: 20000, max_updates: 22000, val_time: 10s 559ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T23:10:25 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T23:12:19 INFO: progress: 20100/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0862, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0862, max mem: 8311.0, experiment: run, epoch: 38, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 676ms, time_since_start: 06h 50m 31s 013ms, eta: 36m 40s 044ms
2020-06-10T23:14:16 INFO: progress: 20200/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0858, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0858, max mem: 8311.0, experiment: run, epoch: 38, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 710ms, time_since_start: 06h 52m 27s 723ms, eta: 35m 02s 884ms
2020-06-10T23:16:12 INFO: progress: 20300/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0854, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0854, max mem: 8311.0, experiment: run, epoch: 39, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 689ms, time_since_start: 06h 54m 24s 412ms, eta: 33m 05s 700ms
2020-06-10T23:18:09 INFO: progress: 20400/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0850, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0850, max mem: 8311.0, experiment: run, epoch: 39, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 066ms, time_since_start: 06h 56m 20s 479ms, eta: 30m 58s 926ms
2020-06-10T23:20:03 INFO: progress: 20500/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0846, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0846, max mem: 8311.0, experiment: run, epoch: 39, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 774ms, time_since_start: 06h 58m 15s 254ms, eta: 28m 43s 346ms
2020-06-10T23:21:59 INFO: progress: 20600/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0841, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0841, max mem: 8311.0, experiment: run, epoch: 39, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 587ms, time_since_start: 07h 10s 841ms, eta: 26m 59s 837ms
2020-06-10T23:23:54 INFO: progress: 20700/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0837, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0837, max mem: 8311.0, experiment: run, epoch: 39, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 814ms, time_since_start: 07h 02m 05s 656ms, eta: 24m 54s 082ms
2020-06-10T23:25:50 INFO: progress: 20800/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0833, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0833, max mem: 8311.0, experiment: run, epoch: 40, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 592ms, time_since_start: 07h 04m 02s 248ms, eta: 23m 20s 511ms
2020-06-10T23:27:48 INFO: progress: 20900/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0829, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0829, max mem: 8311.0, experiment: run, epoch: 40, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 0.85, time: 01m 57s 248ms, time_since_start: 07h 05m 59s 497ms, eta: 21m 31s 024ms
2020-06-10T23:29:43 INFO: progress: 21000/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0825, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0825, max mem: 8311.0, experiment: run, epoch: 40, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 666ms, time_since_start: 07h 07m 55s 163ms, eta: 19m 17s 818ms
2020-06-10T23:29:43 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T23:30:19 INFO: Evaluation time. Running on full validation set...
2020-06-10T23:30:29 INFO: progress: 21000/22000, val/total_loss: 3.1944, val/hateful_memes/cross_entropy: 3.1944, val/hateful_memes/accuracy: 0.6080, val/hateful_memes/binary_f1: 0.4615, val/hateful_memes/roc_auc: 0.7038, num_updates: 21000, epoch: 40, iterations: 21000, max_updates: 22000, val_time: 10s 520ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T23:30:31 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T23:32:25 INFO: progress: 21100/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0821, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0821, max mem: 8311.0, experiment: run, epoch: 40, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 612ms, time_since_start: 07h 10m 36s 903ms, eta: 17m 21s 557ms
2020-06-10T23:34:21 INFO: progress: 21200/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0818, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0818, max mem: 8311.0, experiment: run, epoch: 40, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 958ms, time_since_start: 07h 12m 32s 861ms, eta: 15m 28s 592ms
2020-06-10T23:36:18 INFO: progress: 21300/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0814, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0814, max mem: 8311.0, experiment: run, epoch: 41, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 669ms, time_since_start: 07h 14m 29s 530ms, eta: 13m 37s 502ms
2020-06-10T23:38:13 INFO: progress: 21400/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0810, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0810, max mem: 8311.0, experiment: run, epoch: 41, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 609ms, time_since_start: 07h 16m 25s 140ms, eta: 11m 34s 352ms
2020-06-10T23:40:08 INFO: progress: 21500/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0806, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0806, max mem: 8311.0, experiment: run, epoch: 41, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 551ms, time_since_start: 07h 18m 19s 692ms, eta: 09m 33s 332ms
2020-06-10T23:42:04 INFO: progress: 21600/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0802, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0802, max mem: 8311.0, experiment: run, epoch: 41, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 548ms, time_since_start: 07h 20m 16s 241ms, eta: 07m 46s 660ms
2020-06-10T23:44:01 INFO: progress: 21700/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0799, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0799, max mem: 8311.0, experiment: run, epoch: 41, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 825ms, time_since_start: 07h 22m 13s 066ms, eta: 05m 50s 827ms
2020-06-10T23:45:56 INFO: progress: 21800/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0795, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0795, max mem: 8311.0, experiment: run, epoch: 41, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 248ms, time_since_start: 07h 24m 08s 315ms, eta: 03m 50s 727ms
2020-06-10T23:47:51 INFO: progress: 21900/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0791, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0791, max mem: 8311.0, experiment: run, epoch: 42, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 008ms, time_since_start: 07h 26m 03s 323ms, eta: 01m 55s 123ms
2020-06-10T23:49:48 INFO: progress: 22000/22000, train/total_loss: 0.0001, train/total_loss/avg: 0.0788, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0788, max mem: 8311.0, experiment: run, epoch: 42, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 511ms, time_since_start: 07h 27m 59s 834ms, eta: 0ms
2020-06-10T23:49:48 INFO: Checkpoint time. Saving a checkpoint.
2020-06-10T23:50:24 INFO: Evaluation time. Running on full validation set...
2020-06-10T23:50:34 INFO: progress: 22000/22000, val/total_loss: 3.1806, val/hateful_memes/cross_entropy: 3.1806, val/hateful_memes/accuracy: 0.6080, val/hateful_memes/binary_f1: 0.4674, val/hateful_memes/roc_auc: 0.7042, num_updates: 22000, epoch: 42, iterations: 22000, max_updates: 22000, val_time: 10s 397ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T23:50:36 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

2020-06-10T23:50:36 INFO: Stepping into final validation check
2020-06-10T23:50:36 INFO: Evaluation time. Running on full validation set...
2020-06-10T23:50:47 INFO: progress: 22001/22000, val/total_loss: 3.1806, val/hateful_memes/cross_entropy: 3.1806, val/hateful_memes/accuracy: 0.6080, val/hateful_memes/binary_f1: 0.4674, val/hateful_memes/roc_auc: 0.7042, num_updates: 22001, epoch: 42, iterations: 22001, max_updates: 22000, val_time: 10s 683ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722784
2020-06-10T23:50:47 INFO: Restoring checkpoint
2020-06-10T23:50:47 INFO: Loading checkpoint

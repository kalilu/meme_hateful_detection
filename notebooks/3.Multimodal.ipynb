{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PATH_CURRENT = '/home/jupyter'\n",
    "PATH_MODEL = f'{PATH_CURRENT}/meme_hateful_detection/models'\n",
    "\n",
    "# module_path = PATH_CURRENT\n",
    "if PATH_CURRENT not in sys.path:\n",
    "    sys.path.append(PATH_CURRENT)\n",
    "if PATH_MODEL not in sys.path:\n",
    "    sys.path.append(PATH_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Baseline         | Model Key      | Pretrained Key                                   | Baseine Config                                                     |\n",
    "|------------------|----------------|--------------------------------------------------|------------------------------------------------------------|\n",
    "| Image-Grid       | unimodal_image | unimodal_image.hateful_memes.images              | projects/hateful_memes/configs/unimodal/image.yaml         |\n",
    "| Image-Region     | unimodal_image | unimodal_image.hateful_memes.features            | projects/hateful_memes/configs/unimodal/with_features.yaml |\n",
    "| Text BERT        | unimodal_text  | unimodal_text.hateful_memes.bert                 | projects/hateful_memes/configs/unimodal/bert.yaml          |\n",
    "| Late Fusion      | late_fusion    | late_fusion.hateful_memes                        | projects/hateful_memes/configs/late_fusion/defaults.yaml   |\n",
    "| ConcatBERT       | concat_bert    | concat_bert.hateful_memes                        | projects/hateful_memes/configs/concat_bert/defaults.yaml   |\n",
    "| MMBT-Grid        | mmbt           | mmbt.hateful_memes.images                        | projects/hateful_memes/configs/mmbt/defaults.yaml          |\n",
    "| MMBT-Region      | mmbt           | mmbt.hateful_memes.features                      | projects/hateful_memes/configs/mmbt/with_features.yaml     |\n",
    "| ViLBERT          | vilbert        | vilbert.finetuned.hateful_memes.direct           | projects/hateful_memes/configs/vilbert/defaults.yaml       |\n",
    "| Visual BERT      | visual_bert    | visual_bert.finetuned.hateful_memes.direct       | projects/hateful_memes/configs/visual_bert/direct.yaml     |\n",
    "| ViLBERT CC       | vilbert        | vilbert.finetuned.hateful_memes.from_cc_original | projects/hateful_memes/configs/vilbert/from_cc.yaml        |\n",
    "| Visual BERT COCO | visual_bert    | visual_bert.finetuned.hateful_memes.from_coco    | projects/hateful_memes/configs/visual_bert/from_coco.yaml  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>model_key</th>\n",
       "      <th>pretrained_key</th>\n",
       "      <th>baseline_config</th>\n",
       "      <th>custom_config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image-Grid</td>\n",
       "      <td>unimodal_image</td>\n",
       "      <td>unimodal_image.hateful_memes.images</td>\n",
       "      <td>configs/unimodal/image.yaml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image-Region</td>\n",
       "      <td>unimodal_image</td>\n",
       "      <td>unimodal_image.hateful_memes.features</td>\n",
       "      <td>configs/unimodal/with_features.yaml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Text BERT</td>\n",
       "      <td>unimodal_text</td>\n",
       "      <td>unimodal_text.hateful_memes.bert</td>\n",
       "      <td>configs/unimodal/bert.yaml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Late Fusion</td>\n",
       "      <td>late_fusion</td>\n",
       "      <td>late_fusion.hateful_memes</td>\n",
       "      <td>configs/late_fusion/defaults.yaml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ConcatBERT</td>\n",
       "      <td>concat_bert</td>\n",
       "      <td>concat_bert.hateful_memes</td>\n",
       "      <td>configs/concat_bert/defaults.yaml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MMBT-Grid</td>\n",
       "      <td>mmbt</td>\n",
       "      <td>mmbt.hateful_memes.images</td>\n",
       "      <td>configs/mmbt/defaults.yaml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MMBT-Region</td>\n",
       "      <td>mmbt</td>\n",
       "      <td>mmbt.hateful_memes.features</td>\n",
       "      <td>configs/mmbt/with_features.yaml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ViLBERT</td>\n",
       "      <td>vilbert</td>\n",
       "      <td>vilbert.finetuned.hateful_memes.direct</td>\n",
       "      <td>configs/vilbert/defaults.yaml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Visual BERT</td>\n",
       "      <td>visual_bert</td>\n",
       "      <td>visual_bert.finetuned.hateful_memes.direct</td>\n",
       "      <td>configs/visual_bert/direct.yaml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ViLBERT CC</td>\n",
       "      <td>vilbert</td>\n",
       "      <td>vilbert.finetuned.hateful_memes.from_cc_original</td>\n",
       "      <td>configs/vilbert/from_cc.yaml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Visual BERT COCO</td>\n",
       "      <td>visual_bert</td>\n",
       "      <td>visual_bert.finetuned.hateful_memes.from_coco</td>\n",
       "      <td>configs/visual_bert/from_coco.yaml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            baseline       model_key  \\\n",
       "0         Image-Grid  unimodal_image   \n",
       "1       Image-Region  unimodal_image   \n",
       "2          Text BERT   unimodal_text   \n",
       "3        Late Fusion     late_fusion   \n",
       "4         ConcatBERT     concat_bert   \n",
       "5          MMBT-Grid            mmbt   \n",
       "6        MMBT-Region            mmbt   \n",
       "7            ViLBERT         vilbert   \n",
       "8        Visual BERT     visual_bert   \n",
       "9         ViLBERT CC         vilbert   \n",
       "10  Visual BERT COCO     visual_bert   \n",
       "\n",
       "                                      pretrained_key  \\\n",
       "0                unimodal_image.hateful_memes.images   \n",
       "1              unimodal_image.hateful_memes.features   \n",
       "2                   unimodal_text.hateful_memes.bert   \n",
       "3                          late_fusion.hateful_memes   \n",
       "4                          concat_bert.hateful_memes   \n",
       "5                          mmbt.hateful_memes.images   \n",
       "6                        mmbt.hateful_memes.features   \n",
       "7             vilbert.finetuned.hateful_memes.direct   \n",
       "8         visual_bert.finetuned.hateful_memes.direct   \n",
       "9   vilbert.finetuned.hateful_memes.from_cc_original   \n",
       "10     visual_bert.finetuned.hateful_memes.from_coco   \n",
       "\n",
       "                        baseline_config  custom_config  \n",
       "0           configs/unimodal/image.yaml            NaN  \n",
       "1   configs/unimodal/with_features.yaml            NaN  \n",
       "2            configs/unimodal/bert.yaml            NaN  \n",
       "3     configs/late_fusion/defaults.yaml            NaN  \n",
       "4     configs/concat_bert/defaults.yaml            NaN  \n",
       "5            configs/mmbt/defaults.yaml            NaN  \n",
       "6       configs/mmbt/with_features.yaml            NaN  \n",
       "7         configs/vilbert/defaults.yaml            NaN  \n",
       "8       configs/visual_bert/direct.yaml            NaN  \n",
       "9          configs/vilbert/from_cc.yaml            NaN  \n",
       "10   configs/visual_bert/from_coco.yaml            NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_config = pd.read_csv(f'{PATH_MODEL}/model_config.csv', sep = ';')\n",
    "df_model_config.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/unimodal/image.yaml', 'model=unimodal_image', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_zoo=unimodal_image.hateful_memes.images', 'evaluation.predict=true'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/unimodal/image.yaml\n",
      "Overriding option model to unimodal_image\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to unimodal_image.hateful_memes.images\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 27976961\n",
      "Logging to: ./save/logs/train_2020-05-30T20:15:27.log\n",
      "2020-05-30T20:15:28 INFO: Loading datasets\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /home/jupyter/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n",
      "Downloading extras.tar.gz: 100%|██████████████| 211k/211k [00:00<00:00, 323kB/s]\n",
      "[ Starting checksum for extras.tar.gz]\n",
      "[ Checksum successful for extras.tar.gz]\n",
      "Unpacking extras.tar.gz\n",
      "/home/jupyter/.cache/torch/mmf/glove.6B.zip: 862MB [06:26, 2.23MB/s]            \n",
      "100%|███████████████████████████████▉| 399286/400000 [00:38<00:00, 10824.34it/s]Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /home/jupyter/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n",
      "\n",
      "  0%|                                                | 0.00/230M [00:00<?, ?B/s]\u001b[A\n",
      "  1%|▎                                      | 2.06M/230M [00:00<00:11, 21.2MB/s]\u001b[A\n",
      "  3%|█                                      | 6.55M/230M [00:00<00:09, 25.3MB/s]\u001b[A\n",
      "  6%|██▏                                    | 12.8M/230M [00:00<00:07, 31.1MB/s]\u001b[A\n",
      "  7%|██▉                                    | 17.2M/230M [00:00<00:06, 34.5MB/s]\u001b[A\n",
      "  9%|███▌                                   | 21.2M/230M [00:00<00:06, 35.9MB/s]\u001b[A\n",
      " 12%|████▌                                  | 26.9M/230M [00:00<00:05, 39.3MB/s]\u001b[A\n",
      " 14%|█████▍                                 | 32.0M/230M [00:00<00:04, 42.7MB/s]\u001b[A\n",
      " 16%|██████▎                                | 37.2M/230M [00:00<00:04, 45.3MB/s]\u001b[A\n",
      " 19%|███████▍                               | 43.9M/230M [00:00<00:03, 50.4MB/s]\u001b[A\n",
      " 22%|████████▍                              | 49.7M/230M [00:01<00:03, 50.8MB/s]\u001b[A\n",
      " 24%|█████████▎                             | 54.9M/230M [00:01<00:03, 51.7MB/s]\u001b[A\n",
      " 26%|██████████▎                            | 60.9M/230M [00:01<00:03, 54.6MB/s]\u001b[A\n",
      " 29%|███████████▍                           | 67.5M/230M [00:01<00:02, 58.0MB/s]\u001b[A\n",
      " 32%|████████████▋                          | 74.7M/230M [00:01<00:02, 62.2MB/s]\u001b[A\n",
      " 35%|█████████████▋                         | 80.8M/230M [00:01<00:02, 61.9MB/s]\u001b[A\n",
      " 38%|██████████████▊                        | 87.6M/230M [00:01<00:02, 61.5MB/s]\u001b[A\n",
      " 41%|███████████████▊                       | 93.6M/230M [00:01<00:02, 48.5MB/s]\u001b[A\n",
      " 43%|████████████████▋                      | 98.7M/230M [00:02<00:03, 40.2MB/s]\u001b[A\n",
      " 45%|█████████████████▉                      | 103M/230M [00:02<00:03, 40.5MB/s]\u001b[A\n",
      " 47%|██████████████████▋                     | 108M/230M [00:02<00:03, 41.7MB/s]\u001b[A\n",
      " 49%|███████████████████▍                    | 112M/230M [00:02<00:02, 42.3MB/s]\u001b[A\n",
      " 52%|████████████████████▋                   | 119M/230M [00:02<00:02, 47.9MB/s]\u001b[A\n",
      " 54%|█████████████████████▌                  | 124M/230M [00:02<00:02, 49.6MB/s]\u001b[A\n",
      " 56%|██████████████████████▍                 | 129M/230M [00:02<00:02, 46.7MB/s]\u001b[A\n",
      " 58%|███████████████████████▎                | 135M/230M [00:02<00:02, 49.5MB/s]\u001b[A\n",
      " 61%|████████████████████████▏               | 139M/230M [00:02<00:02, 45.3MB/s]\u001b[A\n",
      " 63%|█████████████████████████▏              | 145M/230M [00:03<00:01, 49.0MB/s]\u001b[A\n",
      " 66%|██████████████████████████▏             | 151M/230M [00:03<00:01, 50.0MB/s]\u001b[A\n",
      " 68%|███████████████████████████             | 156M/230M [00:03<00:01, 49.8MB/s]\u001b[A\n",
      " 70%|███████████████████████████▉            | 161M/230M [00:03<00:01, 49.7MB/s]\u001b[A\n",
      " 72%|████████████████████████████▊           | 166M/230M [00:03<00:01, 36.0MB/s]\u001b[A\n",
      " 75%|█████████████████████████████▊          | 172M/230M [00:03<00:01, 41.2MB/s]\u001b[A\n",
      " 77%|██████████████████████████████▋         | 176M/230M [00:03<00:01, 36.5MB/s]\u001b[A\n",
      " 79%|███████████████████████████████▌        | 182M/230M [00:03<00:01, 40.7MB/s]\u001b[A\n",
      " 82%|████████████████████████████████▌       | 188M/230M [00:04<00:01, 44.3MB/s]\u001b[A\n",
      " 84%|█████████████████████████████████▌      | 193M/230M [00:04<00:00, 46.6MB/s]\u001b[A\n",
      " 87%|██████████████████████████████████▋     | 200M/230M [00:04<00:00, 51.8MB/s]\u001b[A\n",
      " 89%|███████████████████████████████████▌    | 205M/230M [00:04<00:00, 49.0MB/s]\u001b[A\n",
      " 91%|████████████████████████████████████▍   | 210M/230M [00:04<00:00, 47.9MB/s]\u001b[A\n",
      " 94%|█████████████████████████████████████▍  | 215M/230M [00:04<00:00, 47.6MB/s]\u001b[A\n",
      " 96%|██████████████████████████████████████▏ | 220M/230M [00:04<00:00, 47.2MB/s]\u001b[A\n",
      " 98%|███████████████████████████████████████ | 225M/230M [00:04<00:00, 47.6MB/s]\u001b[A\n",
      "100%|████████████████████████████████████████| 230M/230M [00:05<00:00, 47.6MB/s]\u001b[A\n",
      "2020-05-30T20:23:01 INFO: CUDA Device 0 is: Tesla T4\n",
      "100%|███████████████████████████████▉| 399286/400000 [00:50<00:00, 10824.34it/s]2020-05-30T20:23:36 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:23:36 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/unimodal_image/unimodal_image.finetuned.hateful_memes_images.tar.gz to /home/jupyter/.cache/torch/mmf/data/models/unimodal_image.hateful_memes.images/unimodal_image.finetuned.hateful_memes_images.tar.gz ]\n",
      "\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz: 0.00B [00:00, ?B/s]\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:   0%| | 32.8k/     \u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:   1%| | 1.25M/\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:   4%| | 9.11M/\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:   8%| | 17.1M/\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  11%| | 25.8M/\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  15%|▏| 34.0M/\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  19%|▏| 42.7M/\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  23%|▏| 51.2M/\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  27%|▎| 59.6M/\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  30%|▎| 68.2M/\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  34%|▎| 76.8M/\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  38%|▍| 85.2M/\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  42%|▍| 94.3M/\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  46%|▍| 103M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  50%|▍| 112M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  54%|▌| 120M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  57%|▌| 129M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  61%|▌| 137M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  65%|▋| 145M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  69%|▋| 154M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  72%|▋| 163M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  76%|▊| 172M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  80%|▊| 180M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  84%|▊| 188M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  87%|▊| 196M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  91%|▉| 205M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz:  95%|▉| 214M/2\u001b[A\n",
      "Downloading unimodal_image.finetuned.hateful_memes_images.tar.gz: 100%|█| 225M/2\u001b[A\n",
      "[ Starting checksum for unimodal_image.finetuned.hateful_memes_images.tar.gz]\n",
      "[ Checksum successful for unimodal_image.finetuned.hateful_memes_images.tar.gz]\n",
      "Unpacking unimodal_image.finetuned.hateful_memes_images.tar.gz\n",
      "2020-05-30T20:23:44 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:23:44 INFO: Checkpoint loaded\n",
      "2020-05-30T20:23:44 INFO: Starting test inference predictions\n",
      "2020-05-30T20:23:44 INFO: Predicting for hateful_memes\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▍                                          | 1/32 [00:01<00:48,  1.56s/it]\u001b[A\n",
      "  6%|██▊                                         | 2/32 [00:01<00:34,  1.16s/it]\u001b[A\n",
      "  9%|████▏                                       | 3/32 [00:02<00:25,  1.13it/s]\u001b[A\n",
      " 12%|█████▌                                      | 4/32 [00:02<00:19,  1.45it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:02<00:15,  1.79it/s]\u001b[A\n",
      " 19%|████████▎                                   | 6/32 [00:02<00:12,  2.16it/s]\u001b[A\n",
      " 22%|█████████▋                                  | 7/32 [00:02<00:09,  2.53it/s]\u001b[A\n",
      " 25%|███████████                                 | 8/32 [00:03<00:08,  2.87it/s]\u001b[A\n",
      " 28%|████████████▍                               | 9/32 [00:03<00:07,  3.16it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:03<00:06,  3.40it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 11/32 [00:03<00:05,  3.60it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 12/32 [00:04<00:05,  3.76it/s]\u001b[A\n",
      " 41%|█████████████████▍                         | 13/32 [00:04<00:04,  3.86it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:04<00:04,  3.94it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:04<00:04,  4.00it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 16/32 [00:05<00:03,  4.04it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 17/32 [00:05<00:03,  4.07it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:05<00:03,  4.09it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:05<00:03,  4.10it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:06<00:02,  4.11it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 21/32 [00:06<00:02,  4.12it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:06<00:02,  4.12it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:06<00:02,  4.13it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:07<00:01,  4.14it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:07<00:01,  4.14it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 26/32 [00:07<00:01,  4.13it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:07<00:01,  4.14it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:08<00:00,  4.13it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:08<00:00,  4.13it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:08<00:00,  4.14it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 32/32 [00:08<00:00,  3.59it/s]\u001b[A\n",
      "2020-05-30T20:23:53 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/notebooks/save/hateful_memes_unimodal_image_27976961/reports/hateful_memes_run_test_2020-05-30T20:23:53.csv\n",
      "2020-05-30T20:23:53 INFO: Finished predicting\n",
      "100%|████████████████████████████████▉| 399286/400000 [01:39<00:00, 3995.94it/s]\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/unimodal/with_features.yaml', 'model=unimodal_image', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_zoo=unimodal_image.hateful_memes.features', 'evaluation.predict=true'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/unimodal/with_features.yaml\n",
      "Overriding option model to unimodal_image\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to unimodal_image.hateful_memes.features\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 56729426\n",
      "Logging to: ./save/logs/train_2020-05-30T20:23:56.log\n",
      "2020-05-30T20:23:56 INFO: Loading datasets\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz to /home/jupyter/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n",
      "Downloading features.tar.gz: 100%|█████████| 8.44G/8.44G [02:29<00:00, 56.6MB/s]\n",
      "[ Starting checksum for features.tar.gz]\n",
      "[ Checksum successful for features.tar.gz]\n",
      "Unpacking features.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/detectron/vmb_weights.tar.gz to /home/jupyter/.cache/torch/mmf/data/models/detectron.defaults/vmb_weights.tar.gz ]\n",
      "Downloading vmb_weights.tar.gz: 100%|██████| 15.5M/15.5M [00:00<00:00, 26.3MB/s]\n",
      "[ Starting checksum for vmb_weights.tar.gz]\n",
      "[ Checksum successful for vmb_weights.tar.gz]\n",
      "Unpacking vmb_weights.tar.gz\n",
      "2020-05-30T20:30:11 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:30:15 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:30:15 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/unimodal_image/unimodal_image.finetuned.hateful_memes_features.tar.gz to /home/jupyter/.cache/torch/mmf/data/models/unimodal_image.hateful_memes.features/unimodal_image.finetuned.hateful_memes_features.tar.gz ]\n",
      "Downloading unimodal_image.finetuned.hateful_memes_features.tar.gz: 100%|█| 23.5       \n",
      "[ Starting checksum for unimodal_image.finetuned.hateful_memes_features.tar.gz]\n",
      "[ Checksum successful for unimodal_image.finetuned.hateful_memes_features.tar.gz]\n",
      "Unpacking unimodal_image.finetuned.hateful_memes_features.tar.gz\n",
      "2020-05-30T20:30:20 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:30:20 INFO: Checkpoint loaded\n",
      "2020-05-30T20:30:20 INFO: Starting test inference predictions\n",
      "2020-05-30T20:30:20 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 32/32 [00:02<00:00, 13.33it/s]\n",
      "2020-05-30T20:30:22 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/notebooks/save/hateful_memes_unimodal_image_56729426/reports/hateful_memes_run_test_2020-05-30T20:30:22.csv\n",
      "2020-05-30T20:30:22 INFO: Finished predicting\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/unimodal/bert.yaml', 'model=unimodal_text', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_zoo=unimodal_text.hateful_memes.bert', 'evaluation.predict=true'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/unimodal/bert.yaml\n",
      "Overriding option model to unimodal_text\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to unimodal_text.hateful_memes.bert\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 32663602\n",
      "Logging to: ./save/logs/train_2020-05-30T20:30:32.log\n",
      "2020-05-30T20:30:32 INFO: Loading datasets\n",
      "2020-05-30T20:31:09 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:31:13 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:31:13 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/unimodal_text/unimodal_text.finetuned.hateful_memes_bert.tar.gz to /home/jupyter/.cache/torch/mmf/data/models/unimodal_text.hateful_memes.bert/unimodal_text.finetuned.hateful_memes_bert.tar.gz ]\n",
      "Downloading unimodal_text.finetuned.hateful_memes_bert.tar.gz: 100%|█| 410M/410M  \n",
      "[ Starting checksum for unimodal_text.finetuned.hateful_memes_bert.tar.gz]\n",
      "[ Checksum successful for unimodal_text.finetuned.hateful_memes_bert.tar.gz]\n",
      "Unpacking unimodal_text.finetuned.hateful_memes_bert.tar.gz\n",
      "2020-05-30T20:31:26 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:31:26 INFO: Checkpoint loaded\n",
      "2020-05-30T20:31:26 INFO: Starting test inference predictions\n",
      "2020-05-30T20:31:26 INFO: Predicting for hateful_memes\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:22<00:00,  2.80s/it]\n",
      "2020-05-30T20:31:48 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/notebooks/save/hateful_memes_unimodal_text_32663602/reports/hateful_memes_run_test_2020-05-30T20:31:48.csv\n",
      "2020-05-30T20:31:48 INFO: Finished predicting\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/late_fusion/defaults.yaml', 'model=late_fusion', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_zoo=late_fusion.hateful_memes', 'evaluation.predict=true'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/late_fusion/defaults.yaml\n",
      "Overriding option model to late_fusion\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to late_fusion.hateful_memes\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 52588314\n",
      "Logging to: ./save/logs/train_2020-05-30T20:31:52.log\n",
      "2020-05-30T20:31:52 INFO: Loading datasets\n",
      "2020-05-30T20:32:00 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:32:04 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:32:04 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/late_fusion/late_fusion.finetuned.hateful_memes.tar.gz to /home/jupyter/.cache/torch/mmf/data/models/late_fusion.hateful_memes/late_fusion.finetuned.hateful_memes.tar.gz ]\n",
      "Downloading late_fusion.finetuned.hateful_memes.tar.gz: 100%|█| 635M/635M [00:25\n",
      "[ Starting checksum for late_fusion.finetuned.hateful_memes.tar.gz]\n",
      "[ Checksum successful for late_fusion.finetuned.hateful_memes.tar.gz]\n",
      "Unpacking late_fusion.finetuned.hateful_memes.tar.gz\n",
      "2020-05-30T20:32:39 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:32:39 INFO: Checkpoint loaded\n",
      "2020-05-30T20:32:39 INFO: Starting test inference predictions\n",
      "2020-05-30T20:32:39 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 16/16 [00:17<00:00,  1.12s/it]\n",
      "2020-05-30T20:32:56 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/notebooks/save/hateful_memes_late_fusion_52588314/reports/hateful_memes_run_test_2020-05-30T20:32:56.csv\n",
      "2020-05-30T20:32:56 INFO: Finished predicting\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/concat_bert/defaults.yaml', 'model=concat_bert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_zoo=concat_bert.hateful_memes', 'evaluation.predict=true'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/concat_bert/defaults.yaml\n",
      "Overriding option model to concat_bert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to concat_bert.hateful_memes\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 326711\n",
      "Logging to: ./save/logs/train_2020-05-30T20:33:00.log\n",
      "2020-05-30T20:33:00 INFO: Loading datasets\n",
      "2020-05-30T20:33:05 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:33:09 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:33:09 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/concat_bert/concat_bert.finetuned.hateful_memes.tar.gz to /home/jupyter/.cache/torch/mmf/data/models/concat_bert.hateful_memes/concat_bert.finetuned.hateful_memes.tar.gz ]\n",
      "Downloading concat_bert.finetuned.hateful_memes.tar.gz: 100%|█| 633M/633M [00:08\n",
      "[ Starting checksum for concat_bert.finetuned.hateful_memes.tar.gz]\n",
      "[ Checksum successful for concat_bert.finetuned.hateful_memes.tar.gz]\n",
      "Unpacking concat_bert.finetuned.hateful_memes.tar.gz\n",
      "2020-05-30T20:33:27 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:33:27 INFO: Checkpoint loaded\n",
      "2020-05-30T20:33:27 INFO: Starting test inference predictions\n",
      "2020-05-30T20:33:27 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 16/16 [00:16<00:00,  1.00s/it]\n",
      "2020-05-30T20:33:43 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/notebooks/save/hateful_memes_concat_bert_326711/reports/hateful_memes_run_test_2020-05-30T20:33:43.csv\n",
      "2020-05-30T20:33:43 INFO: Finished predicting\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/mmbt/defaults.yaml', 'model=mmbt', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_zoo=mmbt.hateful_memes.images', 'evaluation.predict=true'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/mmbt/defaults.yaml\n",
      "Overriding option model to mmbt\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to mmbt.hateful_memes.images\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 51666747\n",
      "Logging to: ./save/logs/train_2020-05-30T20:33:51.log\n",
      "2020-05-30T20:33:51 INFO: Loading datasets\n",
      "2020-05-30T20:33:57 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:34:01 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:34:01 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/mmbt/mmbt.finetuned.hateful_memes_images.tar.gz to /home/jupyter/.cache/torch/mmf/data/models/mmbt.hateful_memes.images/mmbt.finetuned.hateful_memes_images.tar.gz ]\n",
      "Downloading mmbt.finetuned.hateful_memes_images.tar.gz: 100%|█| 630M/630M [00:11\n",
      "[ Starting checksum for mmbt.finetuned.hateful_memes_images.tar.gz]\n",
      "[ Checksum successful for mmbt.finetuned.hateful_memes_images.tar.gz]\n",
      "Unpacking mmbt.finetuned.hateful_memes_images.tar.gz\n",
      "2020-05-30T20:34:22 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:34:22 INFO: Checkpoint loaded\n",
      "2020-05-30T20:34:22 INFO: Starting test inference predictions\n",
      "2020-05-30T20:34:22 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 32/32 [00:16<00:00,  1.93it/s]\n",
      "2020-05-30T20:34:38 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/notebooks/save/hateful_memes_mmbt_51666747/reports/hateful_memes_run_test_2020-05-30T20:34:38.csv\n",
      "2020-05-30T20:34:38 INFO: Finished predicting\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/mmbt/with_features.yaml', 'model=mmbt', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_zoo=mmbt.hateful_memes.features', 'evaluation.predict=true'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/mmbt/with_features.yaml\n",
      "Overriding option model to mmbt\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to mmbt.hateful_memes.features\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 42056732\n",
      "Logging to: ./save/logs/train_2020-05-30T20:34:42.log\n",
      "2020-05-30T20:34:42 INFO: Loading datasets\n",
      "2020-05-30T20:34:45 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:34:49 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:34:49 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/mmbt/mmbt.finetuned.hateful_memes_features.tar.gz to /home/jupyter/.cache/torch/mmf/data/models/mmbt.hateful_memes.features/mmbt.finetuned.hateful_memes_features.tar.gz ]\n",
      "Downloading mmbt.finetuned.hateful_memes_features.tar.gz: 100%|█| 429M/429M [00:\n",
      "[ Starting checksum for mmbt.finetuned.hateful_memes_features.tar.gz]\n",
      "[ Checksum successful for mmbt.finetuned.hateful_memes_features.tar.gz]\n",
      "Unpacking mmbt.finetuned.hateful_memes_features.tar.gz\n",
      "2020-05-30T20:35:02 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:35:02 INFO: Checkpoint loaded\n",
      "2020-05-30T20:35:02 INFO: Starting test inference predictions\n",
      "2020-05-30T20:35:02 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 32/32 [00:17<00:00,  1.88it/s]\n",
      "2020-05-30T20:35:19 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/notebooks/save/hateful_memes_mmbt_42056732/reports/hateful_memes_run_test_2020-05-30T20:35:19.csv\n",
      "2020-05-30T20:35:19 INFO: Finished predicting\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct', 'evaluation.predict=true'])\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/vilbert/defaults.yaml\n",
      "Overriding option model to vilbert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.direct\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 23123893\n",
      "Logging to: ./save/logs/train_2020-05-30T20:35:23.log\n",
      "2020-05-30T20:35:23 INFO: Loading datasets\n",
      "2020-05-30T20:35:28 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:35:32 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:35:32 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.finetuned.hateful_memes_direct.tar.gz to /home/jupyter/.cache/torch/mmf/data/models/vilbert.finetuned.hateful_memes.direct/vilbert.finetuned.hateful_memes_direct.tar.gz ]\n",
      "Downloading vilbert.finetuned.hateful_memes_direct.tar.gz: 100%|█| 918M/918M [00\n",
      "[ Starting checksum for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
      "[ Checksum successful for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
      "Unpacking vilbert.finetuned.hateful_memes_direct.tar.gz\n",
      "2020-05-30T20:36:07 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:36:07 INFO: Checkpoint loaded\n",
      "2020-05-30T20:36:07 INFO: Starting test inference predictions\n",
      "2020-05-30T20:36:07 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 32/32 [00:20<00:00,  1.53it/s]\n",
      "2020-05-30T20:36:28 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/notebooks/save/hateful_memes_vilbert_23123893/reports/hateful_memes_run_test_2020-05-30T20:36:28.csv\n",
      "2020-05-30T20:36:28 INFO: Finished predicting\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/visual_bert/direct.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct', 'evaluation.predict=true'])\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/visual_bert/direct.yaml\n",
      "Overriding option model to visual_bert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.direct\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 32122689\n",
      "Logging to: ./save/logs/train_2020-05-30T20:36:32.log\n",
      "2020-05-30T20:36:32 INFO: Loading datasets\n",
      "2020-05-30T20:36:35 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:36:39 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:36:39 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.finetuned.hateful_memes_direct.tar.gz to /home/jupyter/.cache/torch/mmf/data/models/visual_bert.finetuned.hateful_memes.direct/visual_bert.finetuned.hateful_memes_direct.tar.gz ]\n",
      "Downloading visual_bert.finetuned.hateful_memes_direct.tar.gz: 100%|█| 415M/415M  \n",
      "[ Starting checksum for visual_bert.finetuned.hateful_memes_direct.tar.gz]\n",
      "[ Checksum successful for visual_bert.finetuned.hateful_memes_direct.tar.gz]\n",
      "Unpacking visual_bert.finetuned.hateful_memes_direct.tar.gz\n",
      "2020-05-30T20:36:55 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:36:55 INFO: Checkpoint loaded\n",
      "2020-05-30T20:36:55 INFO: Starting test inference predictions\n",
      "2020-05-30T20:36:55 INFO: Predicting for hateful_memes\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:19<00:00,  2.39s/it]\n",
      "2020-05-30T20:37:14 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/notebooks/save/hateful_memes_visual_bert_32122689/reports/hateful_memes_run_test_2020-05-30T20:37:14.csv\n",
      "2020-05-30T20:37:14 INFO: Finished predicting\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/vilbert/from_cc.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original', 'evaluation.predict=true'])\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/vilbert/from_cc.yaml\n",
      "Overriding option model to vilbert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.from_cc_original\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 20465190\n",
      "Logging to: ./save/logs/train_2020-05-30T20:37:20.log\n",
      "2020-05-30T20:37:20 INFO: Loading datasets\n",
      "2020-05-30T20:37:25 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:37:29 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:37:29 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.finetuned.hateful_memes_from_cc.tar.gz to /home/jupyter/.cache/torch/mmf/data/models/vilbert.finetuned.hateful_memes.from_cc_original/vilbert.finetuned.hateful_memes_from_cc.tar.gz ]\n",
      "Downloading vilbert.finetuned.hateful_memes_from_cc.tar.gz: 100%|█| 918M/918M [0\n",
      "[ Starting checksum for vilbert.finetuned.hateful_memes_from_cc.tar.gz]\n",
      "[ Checksum successful for vilbert.finetuned.hateful_memes_from_cc.tar.gz]\n",
      "Unpacking vilbert.finetuned.hateful_memes_from_cc.tar.gz\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.v_embeddings.image_embeddings.weight from model.bert.v_embeddings.image_embeddings.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.v_embeddings.image_embeddings.bias from model.bert.v_embeddings.image_embeddings.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.v_embeddings.image_location_embeddings.weight from model.bert.v_embeddings.image_location_embeddings.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.v_embeddings.image_location_embeddings.bias from model.bert.v_embeddings.image_location_embeddings.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.v_embeddings.LayerNorm.weight from model.bert.v_embeddings.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.v_embeddings.LayerNorm.bias from model.bert.v_embeddings.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "2020-05-30T20:37:53 INFO: Copying model.bert.encoder.v_layer.0.attention.self.query.weight from model.bert.encoder.v_layer.0.attention.self.query.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.attention.self.query.bias from model.bert.encoder.v_layer.0.attention.self.query.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.attention.self.key.weight from model.bert.encoder.v_layer.0.attention.self.key.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.attention.self.key.bias from model.bert.encoder.v_layer.0.attention.self.key.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.attention.self.value.weight from model.bert.encoder.v_layer.0.attention.self.value.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.attention.self.value.bias from model.bert.encoder.v_layer.0.attention.self.value.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.attention.output.dense.weight from model.bert.encoder.v_layer.0.attention.output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.attention.output.dense.bias from model.bert.encoder.v_layer.0.attention.output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.intermediate.dense.weight from model.bert.encoder.v_layer.0.intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.intermediate.dense.bias from model.bert.encoder.v_layer.0.intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.output.dense.weight from model.bert.encoder.v_layer.0.output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.output.dense.bias from model.bert.encoder.v_layer.0.output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.output.LayerNorm.weight from model.bert.encoder.v_layer.0.output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.0.output.LayerNorm.bias from model.bert.encoder.v_layer.0.output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.attention.self.query.weight from model.bert.encoder.v_layer.1.attention.self.query.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.attention.self.query.bias from model.bert.encoder.v_layer.1.attention.self.query.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.attention.self.key.weight from model.bert.encoder.v_layer.1.attention.self.key.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.attention.self.key.bias from model.bert.encoder.v_layer.1.attention.self.key.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.attention.self.value.weight from model.bert.encoder.v_layer.1.attention.self.value.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.attention.self.value.bias from model.bert.encoder.v_layer.1.attention.self.value.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.attention.output.dense.weight from model.bert.encoder.v_layer.1.attention.output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.attention.output.dense.bias from model.bert.encoder.v_layer.1.attention.output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.intermediate.dense.weight from model.bert.encoder.v_layer.1.intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.intermediate.dense.bias from model.bert.encoder.v_layer.1.intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.output.dense.weight from model.bert.encoder.v_layer.1.output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.output.dense.bias from model.bert.encoder.v_layer.1.output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.output.LayerNorm.weight from model.bert.encoder.v_layer.1.output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.1.output.LayerNorm.bias from model.bert.encoder.v_layer.1.output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.attention.self.query.weight from model.bert.encoder.v_layer.2.attention.self.query.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.attention.self.query.bias from model.bert.encoder.v_layer.2.attention.self.query.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.attention.self.key.weight from model.bert.encoder.v_layer.2.attention.self.key.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.attention.self.key.bias from model.bert.encoder.v_layer.2.attention.self.key.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.attention.self.value.weight from model.bert.encoder.v_layer.2.attention.self.value.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.attention.self.value.bias from model.bert.encoder.v_layer.2.attention.self.value.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.attention.output.dense.weight from model.bert.encoder.v_layer.2.attention.output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.attention.output.dense.bias from model.bert.encoder.v_layer.2.attention.output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.intermediate.dense.weight from model.bert.encoder.v_layer.2.intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.intermediate.dense.bias from model.bert.encoder.v_layer.2.intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.output.dense.weight from model.bert.encoder.v_layer.2.output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.output.dense.bias from model.bert.encoder.v_layer.2.output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.output.LayerNorm.weight from model.bert.encoder.v_layer.2.output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.2.output.LayerNorm.bias from model.bert.encoder.v_layer.2.output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.attention.self.query.weight from model.bert.encoder.v_layer.3.attention.self.query.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.attention.self.query.bias from model.bert.encoder.v_layer.3.attention.self.query.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.attention.self.key.weight from model.bert.encoder.v_layer.3.attention.self.key.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.attention.self.key.bias from model.bert.encoder.v_layer.3.attention.self.key.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.attention.self.value.weight from model.bert.encoder.v_layer.3.attention.self.value.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.attention.self.value.bias from model.bert.encoder.v_layer.3.attention.self.value.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.attention.output.dense.weight from model.bert.encoder.v_layer.3.attention.output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.attention.output.dense.bias from model.bert.encoder.v_layer.3.attention.output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.intermediate.dense.weight from model.bert.encoder.v_layer.3.intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.intermediate.dense.bias from model.bert.encoder.v_layer.3.intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.output.dense.weight from model.bert.encoder.v_layer.3.output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.output.dense.bias from model.bert.encoder.v_layer.3.output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.output.LayerNorm.weight from model.bert.encoder.v_layer.3.output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.3.output.LayerNorm.bias from model.bert.encoder.v_layer.3.output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.attention.self.query.weight from model.bert.encoder.v_layer.4.attention.self.query.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.attention.self.query.bias from model.bert.encoder.v_layer.4.attention.self.query.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.attention.self.key.weight from model.bert.encoder.v_layer.4.attention.self.key.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.attention.self.key.bias from model.bert.encoder.v_layer.4.attention.self.key.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.attention.self.value.weight from model.bert.encoder.v_layer.4.attention.self.value.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.attention.self.value.bias from model.bert.encoder.v_layer.4.attention.self.value.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.attention.output.dense.weight from model.bert.encoder.v_layer.4.attention.output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.attention.output.dense.bias from model.bert.encoder.v_layer.4.attention.output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.intermediate.dense.weight from model.bert.encoder.v_layer.4.intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.intermediate.dense.bias from model.bert.encoder.v_layer.4.intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.output.dense.weight from model.bert.encoder.v_layer.4.output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.output.dense.bias from model.bert.encoder.v_layer.4.output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.output.LayerNorm.weight from model.bert.encoder.v_layer.4.output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.4.output.LayerNorm.bias from model.bert.encoder.v_layer.4.output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.attention.self.query.weight from model.bert.encoder.v_layer.5.attention.self.query.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.attention.self.query.bias from model.bert.encoder.v_layer.5.attention.self.query.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.attention.self.key.weight from model.bert.encoder.v_layer.5.attention.self.key.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.attention.self.key.bias from model.bert.encoder.v_layer.5.attention.self.key.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.attention.self.value.weight from model.bert.encoder.v_layer.5.attention.self.value.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.attention.self.value.bias from model.bert.encoder.v_layer.5.attention.self.value.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.attention.output.dense.weight from model.bert.encoder.v_layer.5.attention.output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.attention.output.dense.bias from model.bert.encoder.v_layer.5.attention.output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.intermediate.dense.weight from model.bert.encoder.v_layer.5.intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.intermediate.dense.bias from model.bert.encoder.v_layer.5.intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.output.dense.weight from model.bert.encoder.v_layer.5.output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.output.dense.bias from model.bert.encoder.v_layer.5.output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.output.LayerNorm.weight from model.bert.encoder.v_layer.5.output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.v_layer.5.output.LayerNorm.bias from model.bert.encoder.v_layer.5.output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biattention.query1.weight from model.bert.encoder.c_layer.0.biattention.query1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biattention.query1.bias from model.bert.encoder.c_layer.0.biattention.query1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biattention.key1.weight from model.bert.encoder.c_layer.0.biattention.key1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biattention.key1.bias from model.bert.encoder.c_layer.0.biattention.key1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biattention.value1.weight from model.bert.encoder.c_layer.0.biattention.value1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biattention.value1.bias from model.bert.encoder.c_layer.0.biattention.value1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biattention.query2.weight from model.bert.encoder.c_layer.0.biattention.query2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biattention.query2.bias from model.bert.encoder.c_layer.0.biattention.query2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biattention.key2.weight from model.bert.encoder.c_layer.0.biattention.key2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biattention.key2.bias from model.bert.encoder.c_layer.0.biattention.key2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biattention.value2.weight from model.bert.encoder.c_layer.0.biattention.value2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biattention.value2.bias from model.bert.encoder.c_layer.0.biattention.value2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense1.weight from model.bert.encoder.c_layer.0.biOutput.dense1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense1.bias from model.bert.encoder.c_layer.0.biOutput.dense1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense1.weight from model.bert.encoder.c_layer.0.biOutput.q_dense1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense1.bias from model.bert.encoder.c_layer.0.biOutput.q_dense1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense2.weight from model.bert.encoder.c_layer.0.biOutput.dense2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense2.bias from model.bert.encoder.c_layer.0.biOutput.dense2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense2.weight from model.bert.encoder.c_layer.0.biOutput.q_dense2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense2.bias from model.bert.encoder.c_layer.0.biOutput.q_dense2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.v_intermediate.dense.weight from model.bert.encoder.c_layer.0.v_intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.v_intermediate.dense.bias from model.bert.encoder.c_layer.0.v_intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.v_output.dense.weight from model.bert.encoder.c_layer.0.v_output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.v_output.dense.bias from model.bert.encoder.c_layer.0.v_output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.v_output.LayerNorm.weight from model.bert.encoder.c_layer.0.v_output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.v_output.LayerNorm.bias from model.bert.encoder.c_layer.0.v_output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.t_intermediate.dense.weight from model.bert.encoder.c_layer.0.t_intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.t_intermediate.dense.bias from model.bert.encoder.c_layer.0.t_intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.t_output.dense.weight from model.bert.encoder.c_layer.0.t_output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.t_output.dense.bias from model.bert.encoder.c_layer.0.t_output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.t_output.LayerNorm.weight from model.bert.encoder.c_layer.0.t_output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.0.t_output.LayerNorm.bias from model.bert.encoder.c_layer.0.t_output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biattention.query1.weight from model.bert.encoder.c_layer.1.biattention.query1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biattention.query1.bias from model.bert.encoder.c_layer.1.biattention.query1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biattention.key1.weight from model.bert.encoder.c_layer.1.biattention.key1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biattention.key1.bias from model.bert.encoder.c_layer.1.biattention.key1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biattention.value1.weight from model.bert.encoder.c_layer.1.biattention.value1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biattention.value1.bias from model.bert.encoder.c_layer.1.biattention.value1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biattention.query2.weight from model.bert.encoder.c_layer.1.biattention.query2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biattention.query2.bias from model.bert.encoder.c_layer.1.biattention.query2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biattention.key2.weight from model.bert.encoder.c_layer.1.biattention.key2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biattention.key2.bias from model.bert.encoder.c_layer.1.biattention.key2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biattention.value2.weight from model.bert.encoder.c_layer.1.biattention.value2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biattention.value2.bias from model.bert.encoder.c_layer.1.biattention.value2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense1.weight from model.bert.encoder.c_layer.1.biOutput.dense1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense1.bias from model.bert.encoder.c_layer.1.biOutput.dense1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense1.weight from model.bert.encoder.c_layer.1.biOutput.q_dense1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense1.bias from model.bert.encoder.c_layer.1.biOutput.q_dense1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense2.weight from model.bert.encoder.c_layer.1.biOutput.dense2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense2.bias from model.bert.encoder.c_layer.1.biOutput.dense2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense2.weight from model.bert.encoder.c_layer.1.biOutput.q_dense2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense2.bias from model.bert.encoder.c_layer.1.biOutput.q_dense2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.v_intermediate.dense.weight from model.bert.encoder.c_layer.1.v_intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.v_intermediate.dense.bias from model.bert.encoder.c_layer.1.v_intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.v_output.dense.weight from model.bert.encoder.c_layer.1.v_output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.v_output.dense.bias from model.bert.encoder.c_layer.1.v_output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.v_output.LayerNorm.weight from model.bert.encoder.c_layer.1.v_output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.v_output.LayerNorm.bias from model.bert.encoder.c_layer.1.v_output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.t_intermediate.dense.weight from model.bert.encoder.c_layer.1.t_intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.t_intermediate.dense.bias from model.bert.encoder.c_layer.1.t_intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.t_output.dense.weight from model.bert.encoder.c_layer.1.t_output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.t_output.dense.bias from model.bert.encoder.c_layer.1.t_output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.t_output.LayerNorm.weight from model.bert.encoder.c_layer.1.t_output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.1.t_output.LayerNorm.bias from model.bert.encoder.c_layer.1.t_output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biattention.query1.weight from model.bert.encoder.c_layer.2.biattention.query1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biattention.query1.bias from model.bert.encoder.c_layer.2.biattention.query1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biattention.key1.weight from model.bert.encoder.c_layer.2.biattention.key1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biattention.key1.bias from model.bert.encoder.c_layer.2.biattention.key1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biattention.value1.weight from model.bert.encoder.c_layer.2.biattention.value1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biattention.value1.bias from model.bert.encoder.c_layer.2.biattention.value1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biattention.query2.weight from model.bert.encoder.c_layer.2.biattention.query2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biattention.query2.bias from model.bert.encoder.c_layer.2.biattention.query2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biattention.key2.weight from model.bert.encoder.c_layer.2.biattention.key2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biattention.key2.bias from model.bert.encoder.c_layer.2.biattention.key2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biattention.value2.weight from model.bert.encoder.c_layer.2.biattention.value2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biattention.value2.bias from model.bert.encoder.c_layer.2.biattention.value2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense1.weight from model.bert.encoder.c_layer.2.biOutput.dense1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense1.bias from model.bert.encoder.c_layer.2.biOutput.dense1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense1.weight from model.bert.encoder.c_layer.2.biOutput.q_dense1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense1.bias from model.bert.encoder.c_layer.2.biOutput.q_dense1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense2.weight from model.bert.encoder.c_layer.2.biOutput.dense2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense2.bias from model.bert.encoder.c_layer.2.biOutput.dense2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense2.weight from model.bert.encoder.c_layer.2.biOutput.q_dense2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense2.bias from model.bert.encoder.c_layer.2.biOutput.q_dense2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.v_intermediate.dense.weight from model.bert.encoder.c_layer.2.v_intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.v_intermediate.dense.bias from model.bert.encoder.c_layer.2.v_intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.v_output.dense.weight from model.bert.encoder.c_layer.2.v_output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.v_output.dense.bias from model.bert.encoder.c_layer.2.v_output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.v_output.LayerNorm.weight from model.bert.encoder.c_layer.2.v_output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.v_output.LayerNorm.bias from model.bert.encoder.c_layer.2.v_output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.t_intermediate.dense.weight from model.bert.encoder.c_layer.2.t_intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.t_intermediate.dense.bias from model.bert.encoder.c_layer.2.t_intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.t_output.dense.weight from model.bert.encoder.c_layer.2.t_output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.t_output.dense.bias from model.bert.encoder.c_layer.2.t_output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.t_output.LayerNorm.weight from model.bert.encoder.c_layer.2.t_output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.2.t_output.LayerNorm.bias from model.bert.encoder.c_layer.2.t_output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biattention.query1.weight from model.bert.encoder.c_layer.3.biattention.query1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biattention.query1.bias from model.bert.encoder.c_layer.3.biattention.query1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biattention.key1.weight from model.bert.encoder.c_layer.3.biattention.key1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biattention.key1.bias from model.bert.encoder.c_layer.3.biattention.key1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biattention.value1.weight from model.bert.encoder.c_layer.3.biattention.value1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biattention.value1.bias from model.bert.encoder.c_layer.3.biattention.value1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biattention.query2.weight from model.bert.encoder.c_layer.3.biattention.query2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biattention.query2.bias from model.bert.encoder.c_layer.3.biattention.query2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biattention.key2.weight from model.bert.encoder.c_layer.3.biattention.key2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biattention.key2.bias from model.bert.encoder.c_layer.3.biattention.key2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biattention.value2.weight from model.bert.encoder.c_layer.3.biattention.value2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biattention.value2.bias from model.bert.encoder.c_layer.3.biattention.value2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense1.weight from model.bert.encoder.c_layer.3.biOutput.dense1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense1.bias from model.bert.encoder.c_layer.3.biOutput.dense1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense1.weight from model.bert.encoder.c_layer.3.biOutput.q_dense1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense1.bias from model.bert.encoder.c_layer.3.biOutput.q_dense1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense2.weight from model.bert.encoder.c_layer.3.biOutput.dense2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense2.bias from model.bert.encoder.c_layer.3.biOutput.dense2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense2.weight from model.bert.encoder.c_layer.3.biOutput.q_dense2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense2.bias from model.bert.encoder.c_layer.3.biOutput.q_dense2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.v_intermediate.dense.weight from model.bert.encoder.c_layer.3.v_intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.v_intermediate.dense.bias from model.bert.encoder.c_layer.3.v_intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.v_output.dense.weight from model.bert.encoder.c_layer.3.v_output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.v_output.dense.bias from model.bert.encoder.c_layer.3.v_output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.v_output.LayerNorm.weight from model.bert.encoder.c_layer.3.v_output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.v_output.LayerNorm.bias from model.bert.encoder.c_layer.3.v_output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.t_intermediate.dense.weight from model.bert.encoder.c_layer.3.t_intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.t_intermediate.dense.bias from model.bert.encoder.c_layer.3.t_intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.t_output.dense.weight from model.bert.encoder.c_layer.3.t_output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.t_output.dense.bias from model.bert.encoder.c_layer.3.t_output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.t_output.LayerNorm.weight from model.bert.encoder.c_layer.3.t_output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.3.t_output.LayerNorm.bias from model.bert.encoder.c_layer.3.t_output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biattention.query1.weight from model.bert.encoder.c_layer.4.biattention.query1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biattention.query1.bias from model.bert.encoder.c_layer.4.biattention.query1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biattention.key1.weight from model.bert.encoder.c_layer.4.biattention.key1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biattention.key1.bias from model.bert.encoder.c_layer.4.biattention.key1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biattention.value1.weight from model.bert.encoder.c_layer.4.biattention.value1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biattention.value1.bias from model.bert.encoder.c_layer.4.biattention.value1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biattention.query2.weight from model.bert.encoder.c_layer.4.biattention.query2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biattention.query2.bias from model.bert.encoder.c_layer.4.biattention.query2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biattention.key2.weight from model.bert.encoder.c_layer.4.biattention.key2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biattention.key2.bias from model.bert.encoder.c_layer.4.biattention.key2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biattention.value2.weight from model.bert.encoder.c_layer.4.biattention.value2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biattention.value2.bias from model.bert.encoder.c_layer.4.biattention.value2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense1.weight from model.bert.encoder.c_layer.4.biOutput.dense1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense1.bias from model.bert.encoder.c_layer.4.biOutput.dense1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense1.weight from model.bert.encoder.c_layer.4.biOutput.q_dense1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense1.bias from model.bert.encoder.c_layer.4.biOutput.q_dense1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense2.weight from model.bert.encoder.c_layer.4.biOutput.dense2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense2.bias from model.bert.encoder.c_layer.4.biOutput.dense2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense2.weight from model.bert.encoder.c_layer.4.biOutput.q_dense2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense2.bias from model.bert.encoder.c_layer.4.biOutput.q_dense2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.v_intermediate.dense.weight from model.bert.encoder.c_layer.4.v_intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.v_intermediate.dense.bias from model.bert.encoder.c_layer.4.v_intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.v_output.dense.weight from model.bert.encoder.c_layer.4.v_output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.v_output.dense.bias from model.bert.encoder.c_layer.4.v_output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.v_output.LayerNorm.weight from model.bert.encoder.c_layer.4.v_output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.v_output.LayerNorm.bias from model.bert.encoder.c_layer.4.v_output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.t_intermediate.dense.weight from model.bert.encoder.c_layer.4.t_intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.t_intermediate.dense.bias from model.bert.encoder.c_layer.4.t_intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.t_output.dense.weight from model.bert.encoder.c_layer.4.t_output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.t_output.dense.bias from model.bert.encoder.c_layer.4.t_output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.t_output.LayerNorm.weight from model.bert.encoder.c_layer.4.t_output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.4.t_output.LayerNorm.bias from model.bert.encoder.c_layer.4.t_output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biattention.query1.weight from model.bert.encoder.c_layer.5.biattention.query1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biattention.query1.bias from model.bert.encoder.c_layer.5.biattention.query1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biattention.key1.weight from model.bert.encoder.c_layer.5.biattention.key1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biattention.key1.bias from model.bert.encoder.c_layer.5.biattention.key1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biattention.value1.weight from model.bert.encoder.c_layer.5.biattention.value1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biattention.value1.bias from model.bert.encoder.c_layer.5.biattention.value1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biattention.query2.weight from model.bert.encoder.c_layer.5.biattention.query2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biattention.query2.bias from model.bert.encoder.c_layer.5.biattention.query2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biattention.key2.weight from model.bert.encoder.c_layer.5.biattention.key2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biattention.key2.bias from model.bert.encoder.c_layer.5.biattention.key2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biattention.value2.weight from model.bert.encoder.c_layer.5.biattention.value2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biattention.value2.bias from model.bert.encoder.c_layer.5.biattention.value2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense1.weight from model.bert.encoder.c_layer.5.biOutput.dense1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense1.bias from model.bert.encoder.c_layer.5.biOutput.dense1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense1.weight from model.bert.encoder.c_layer.5.biOutput.q_dense1.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense1.bias from model.bert.encoder.c_layer.5.biOutput.q_dense1.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense2.weight from model.bert.encoder.c_layer.5.biOutput.dense2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense2.bias from model.bert.encoder.c_layer.5.biOutput.dense2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense2.weight from model.bert.encoder.c_layer.5.biOutput.q_dense2.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense2.bias from model.bert.encoder.c_layer.5.biOutput.q_dense2.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.v_intermediate.dense.weight from model.bert.encoder.c_layer.5.v_intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.v_intermediate.dense.bias from model.bert.encoder.c_layer.5.v_intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.v_output.dense.weight from model.bert.encoder.c_layer.5.v_output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.v_output.dense.bias from model.bert.encoder.c_layer.5.v_output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.v_output.LayerNorm.weight from model.bert.encoder.c_layer.5.v_output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.v_output.LayerNorm.bias from model.bert.encoder.c_layer.5.v_output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.t_intermediate.dense.weight from model.bert.encoder.c_layer.5.t_intermediate.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.t_intermediate.dense.bias from model.bert.encoder.c_layer.5.t_intermediate.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.t_output.dense.weight from model.bert.encoder.c_layer.5.t_output.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.t_output.dense.bias from model.bert.encoder.c_layer.5.t_output.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.t_output.LayerNorm.weight from model.bert.encoder.c_layer.5.t_output.LayerNorm.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.encoder.c_layer.5.t_output.LayerNorm.bias from model.bert.encoder.c_layer.5.t_output.LayerNorm.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.t_pooler.dense.weight from model.bert.t_pooler.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.t_pooler.dense.bias from model.bert.t_pooler.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.v_pooler.dense.weight from model.bert.v_pooler.dense.weight\n",
      "2020-05-30T20:37:54 INFO: Copying model.bert.v_pooler.dense.bias from model.bert.v_pooler.dense.bias\n",
      "2020-05-30T20:37:54 INFO: Pretrained model loaded\n",
      "2020-05-30T20:37:54 INFO: Starting test inference predictions\n",
      "2020-05-30T20:37:54 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 32/32 [00:19<00:00,  1.63it/s]\n",
      "2020-05-30T20:38:14 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/notebooks/save/hateful_memes_vilbert_20465190/reports/hateful_memes_run_test_2020-05-30T20:38:14.csv\n",
      "2020-05-30T20:38:14 INFO: Finished predicting\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/visual_bert/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco', 'evaluation.predict=true'])\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/visual_bert/from_coco.yaml\n",
      "Overriding option model to visual_bert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.from_coco\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 17421050\n",
      "Logging to: ./save/logs/train_2020-05-30T20:38:17.log\n",
      "2020-05-30T20:38:17 INFO: Loading datasets\n",
      "2020-05-30T20:38:20 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:38:25 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:38:25 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.finetuned.hateful_memes_from_coco.tar.gz to /home/jupyter/.cache/torch/mmf/data/models/visual_bert.finetuned.hateful_memes.from_coco/visual_bert.finetuned.hateful_memes_from_coco.tar.gz ]\n",
      "Downloading visual_bert.finetuned.hateful_memes_from_coco.tar.gz: 100%|█| 414M/4     \n",
      "[ Starting checksum for visual_bert.finetuned.hateful_memes_from_coco.tar.gz]\n",
      "[ Checksum successful for visual_bert.finetuned.hateful_memes_from_coco.tar.gz]\n",
      "Unpacking visual_bert.finetuned.hateful_memes_from_coco.tar.gz\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
      "2020-05-30T20:38:37 INFO: Copying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
      "2020-05-30T20:38:37 INFO: Pretrained model loaded\n",
      "2020-05-30T20:38:37 INFO: Starting test inference predictions\n",
      "2020-05-30T20:38:37 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 16/16 [00:16<00:00,  1.04s/it]\n",
      "2020-05-30T20:38:53 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/notebooks/save/hateful_memes_visual_bert_17421050/reports/hateful_memes_run_test_2020-05-30T20:38:53.csv\n",
      "2020-05-30T20:38:53 INFO: Finished predicting\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "for index, row in df_model_config.iterrows():\n",
    "    model_baseline = row['baseline']\n",
    "    model_key = row['model_key']\n",
    "    model_pretrained_key = row['pretrained_key']\n",
    "    baseline_config = row['baseline_config']\n",
    "    custom_config = row['custom_config']\n",
    "    str_exec = f'MMF_USER_DIR=\"..\" mmf_predict config={baseline_config} model={model_key} dataset=hateful_memes run_type=test checkpoint.resume_zoo={model_pretrained_key}'\n",
    "    !{str_exec}\n",
    "#     print(str_exec)\n",
    "#     MMF_USER_DIR=\".\" mmf_run config=\"configs/experiments/defaults.yaml\"  model=concat_vl dataset=hateful_memes training.num_workers=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmf.common.registry import registry\n",
    "\n",
    "model_cls = registry.get_model_class(\"unimodal_image\")\n",
    "model = model_cls.from_pretrained(\"unimodal_image.hateful_memes.images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/unimodal/image.yaml', 'model=unimodal_image', 'dataset=hateful_memes'])\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (../../../others/unimodal/configs/hateful_memes/image.yaml) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "Overriding option config to projects/hateful_memes/configs/unimodal/image.yaml\n",
      "Overriding option model to unimodal_image\n",
      "Overriding option datasets to hateful_memes\n",
      "Using seed 47455368\n",
      "Logging to: ./save/logs/train_2020-05-30T20:39:47.log\n",
      "2020-05-30T20:39:47 INFO: Loading datasets\n",
      "2020-05-30T20:39:56 WARNING: /home/jupyter/mmf/mmf/models/base_model.py:97: UserWarning: No losses are defined in model configuration. You are expected to return loss in your return dict from forward.\n",
      "  \"No losses are defined in model configuration. You are expected \"\n",
      "\n",
      "2020-05-30T20:39:56 INFO: CUDA Device 0 is: Tesla T4\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/mmf_run\", line 11, in <module>\n",
      "    load_entry_point('mmf', 'console_scripts', 'mmf_run')()\n",
      "  File \"/home/jupyter/mmf/mmf_cli/run.py\", line 89, in run\n",
      "    main(configuration, predict=predict)\n",
      "  File \"/home/jupyter/mmf/mmf_cli/run.py\", line 36, in main\n",
      "    trainer.load()\n",
      "  File \"/home/jupyter/mmf/mmf/trainers/base_trainer.py\", line 61, in load\n",
      "    self.load_model_and_optimizer()\n",
      "  File \"/home/jupyter/mmf/mmf/trainers/base_trainer.py\", line 124, in load_model_and_optimizer\n",
      "    self.optimizer = build_optimizer(self.model, self.config)\n",
      "  File \"/home/jupyter/mmf/mmf/utils/build.py\", line 161, in build_optimizer\n",
      "    \"Optimizer attributes must have a 'type' key \"\n",
      "ValueError: Optimizer attributes must have a 'type' key specifying the type of optimizer. (Custom or PyTorch)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "!mmf_run config=projects/hateful_memes/configs/unimodal/image.yaml model=unimodal_image dataset=hateful_memes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "!mmf_run config=projects/hateful_memes/configs/unimodal/image.yaml model=unimodal_image dataset=hateful_memes run_type=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/unimodal/image.yaml', 'model=unimodal_image', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=unimodal_image.hateful_memes.images'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/unimodal/image.yaml\n",
      "Overriding option model to unimodal_image\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to unimodal_image.hateful_memes.images\n",
      "Using seed 21721109\n",
      "Logging to: ./save/logs/train_2020-05-30T20:52:21.log\n",
      "2020-05-30T20:52:21 INFO: Loading datasets\n",
      "2020-05-30T20:52:25 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:52:29 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:52:29 INFO: Loading checkpoint\n",
      "2020-05-30T20:52:30 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:52:30 INFO: Checkpoint loaded\n",
      "2020-05-30T20:52:30 INFO: ===== Model =====\n",
      "2020-05-30T20:52:30 INFO: UnimodalModal(\n",
      "  (base): UnimodalBase(\n",
      "    (encoder): ResNet152ImageEncoder(\n",
      "      (model): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (4): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (6): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (7): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (6): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (7): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (8): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (9): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (10): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (11): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (12): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (13): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (14): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (15): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (16): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (17): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (18): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (19): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (20): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (21): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (22): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (23): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (24): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (25): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (26): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (27): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (28): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (29): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (30): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (31): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (32): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (33): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (34): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (35): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (classifier): MLPClassifer(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      (1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.5, inplace=False)\n",
      "      (8): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses()\n",
      ")\n",
      "2020-05-30T20:52:30 INFO: Total Parameters: 60312642. Trained Parameters: 60312642\n",
      "2020-05-30T20:52:30 INFO: Starting inference on val set\n",
      "16it [00:04,  3.38it/s]                                                         \n",
      "2020-05-30T20:52:35 INFO: progress: 0/22000, val/total_loss: 0.8242, val/hateful_memes/cross_entropy: 0.8242, val/hateful_memes/accuracy: 0.5160, val/hateful_memes/binary_f1: 0.2622, val/hateful_memes/roc_auc: 0.5754\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/unimodal/with_features.yaml', 'model=unimodal_image', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=unimodal_image.hateful_memes.features'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/unimodal/with_features.yaml\n",
      "Overriding option model to unimodal_image\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to unimodal_image.hateful_memes.features\n",
      "Using seed 38605950\n",
      "Logging to: ./save/logs/train_2020-05-30T20:52:38.log\n",
      "2020-05-30T20:52:38 INFO: Loading datasets\n",
      "2020-05-30T20:52:41 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:52:44 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:52:44 INFO: Loading checkpoint\n",
      "2020-05-30T20:52:46 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:52:46 INFO: Checkpoint loaded\n",
      "2020-05-30T20:52:46 INFO: ===== Model =====\n",
      "2020-05-30T20:52:46 INFO: UnimodalModal(\n",
      "  (base): UnimodalBase(\n",
      "    (encoder): FinetuneFasterRcnnFpnFc7(\n",
      "      (lc): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): MLPClassifer(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      (1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.5, inplace=False)\n",
      "      (8): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses()\n",
      ")\n",
      "2020-05-30T20:52:46 INFO: Total Parameters: 6365186. Trained Parameters: 6365186\n",
      "2020-05-30T20:52:46 INFO: Starting inference on val set\n",
      "16it [00:01, 12.07it/s]                                                         \n",
      "2020-05-30T20:52:47 INFO: progress: 0/22000, val/total_loss: 0.7530, val/hateful_memes/cross_entropy: 0.7530, val/hateful_memes/accuracy: 0.5340, val/hateful_memes/binary_f1: 0.2961, val/hateful_memes/roc_auc: 0.5836\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/unimodal/bert.yaml', 'model=unimodal_text', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=unimodal_text.hateful_memes.bert'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/unimodal/bert.yaml\n",
      "Overriding option model to unimodal_text\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to unimodal_text.hateful_memes.bert\n",
      "Using seed 51059854\n",
      "Logging to: ./save/logs/train_2020-05-30T20:52:51.log\n",
      "2020-05-30T20:52:51 INFO: Loading datasets\n",
      "2020-05-30T20:52:54 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:52:58 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:52:58 INFO: Loading checkpoint\n",
      "2020-05-30T20:53:03 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:53:03 INFO: Checkpoint loaded\n",
      "2020-05-30T20:53:03 INFO: ===== Model =====\n",
      "2020-05-30T20:53:03 INFO: UnimodalText(\n",
      "  (base): UnimodalBase(\n",
      "    (encoder): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): MLPClassifer(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.5, inplace=False)\n",
      "      (8): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses()\n",
      ")\n",
      "2020-05-30T20:53:03 INFO: Total Parameters: 110668034. Trained Parameters: 110668034\n",
      "2020-05-30T20:53:03 INFO: Starting inference on val set\n",
      "4it [00:05,  1.50s/it]                                                          \n",
      "2020-05-30T20:53:09 INFO: progress: 0/22000, val/total_loss: 0.7325, val/hateful_memes/cross_entropy: 0.7325, val/hateful_memes/accuracy: 0.5900, val/hateful_memes/binary_f1: 0.4862, val/hateful_memes/roc_auc: 0.6544\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/late_fusion/defaults.yaml', 'model=late_fusion', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=late_fusion.hateful_memes'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/late_fusion/defaults.yaml\n",
      "Overriding option model to late_fusion\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to late_fusion.hateful_memes\n",
      "Using seed 13277505\n",
      "Logging to: ./save/logs/train_2020-05-30T20:53:13.log\n",
      "2020-05-30T20:53:13 INFO: Loading datasets\n",
      "2020-05-30T20:53:18 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:53:22 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:53:22 INFO: Loading checkpoint\n",
      "2020-05-30T20:53:29 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:53:29 INFO: Checkpoint loaded\n",
      "2020-05-30T20:53:29 INFO: ===== Model =====\n",
      "2020-05-30T20:53:29 INFO: LateFusion(\n",
      "  (base): FusionBase(\n",
      "    (text): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (modal): ResNet152ImageEncoder(\n",
      "      (model): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (4): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (6): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (7): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (6): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (7): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (8): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (9): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (10): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (11): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (12): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (13): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (14): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (15): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (16): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (17): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (18): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (19): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (20): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (21): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (22): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (23): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (24): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (25): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (26): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (27): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (28): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (29): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (30): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (31): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (32): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (33): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (34): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (35): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (modal_classifier): MLPClassifer(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      (1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.5, inplace=False)\n",
      "      (8): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (text_classifier): MLPClassifer(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.5, inplace=False)\n",
      "      (8): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses()\n",
      ")\n",
      "2020-05-30T20:53:29 INFO: Total Parameters: 170980676. Trained Parameters: 170980676\n",
      "2020-05-30T20:53:29 INFO: Starting inference on val set\n",
      "8it [00:08,  1.10s/it]                                                          \n",
      "2020-05-30T20:53:38 INFO: progress: 0/22000, val/total_loss: 1.1010, val/hateful_memes/cross_entropy: 1.1010, val/hateful_memes/accuracy: 0.6300, val/hateful_memes/binary_f1: 0.5521, val/hateful_memes/roc_auc: 0.6632\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/concat_bert/defaults.yaml', 'model=concat_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=concat_bert.hateful_memes'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/concat_bert/defaults.yaml\n",
      "Overriding option model to concat_bert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to concat_bert.hateful_memes\n",
      "Using seed 42000986\n",
      "Logging to: ./save/logs/train_2020-05-30T20:53:41.log\n",
      "2020-05-30T20:53:41 INFO: Loading datasets\n",
      "2020-05-30T20:53:47 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:53:51 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:53:51 INFO: Loading checkpoint\n",
      "2020-05-30T20:53:58 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:53:58 INFO: Checkpoint loaded\n",
      "2020-05-30T20:53:58 INFO: ===== Model =====\n",
      "2020-05-30T20:53:58 INFO: ConcatBERT(\n",
      "  (base): FusionBase(\n",
      "    (text): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (modal): ResNet152ImageEncoder(\n",
      "      (model): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (4): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (6): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (7): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (6): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (7): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (8): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (9): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (10): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (11): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (12): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (13): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (14): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (15): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (16): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (17): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (18): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (19): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (20): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (21): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (22): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (23): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (24): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (25): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (26): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (27): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (28): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (29): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (30): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (31): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (32): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (33): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (34): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (35): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (classifier): MLPClassifer(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=2816, out_features=768, bias=True)\n",
      "      (1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.5, inplace=False)\n",
      "      (8): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses()\n",
      ")\n",
      "2020-05-30T20:53:58 INFO: Total Parameters: 170384706. Trained Parameters: 170384706\n",
      "2020-05-30T20:53:58 INFO: Starting inference on val set\n",
      "8it [00:08,  1.12s/it]                                                          \n",
      "2020-05-30T20:54:07 INFO: progress: 0/22000, val/total_loss: 0.8617, val/hateful_memes/cross_entropy: 0.8617, val/hateful_memes/accuracy: 0.5720, val/hateful_memes/binary_f1: 0.3886, val/hateful_memes/roc_auc: 0.6444\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/mmbt/defaults.yaml', 'model=mmbt', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=mmbt.hateful_memes.images'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/mmbt/defaults.yaml\n",
      "Overriding option model to mmbt\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to mmbt.hateful_memes.images\n",
      "Using seed 10789596\n",
      "Logging to: ./save/logs/train_2020-05-30T20:54:10.log\n",
      "2020-05-30T20:54:10 INFO: Loading datasets\n",
      "2020-05-30T20:54:16 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:54:19 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:54:19 INFO: Loading checkpoint\n",
      "2020-05-30T20:54:27 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:54:27 INFO: Checkpoint loaded\n",
      "2020-05-30T20:54:27 INFO: ===== Model =====\n",
      "2020-05-30T20:54:27 INFO: MMBT(\n",
      "  (model): MMBTForClassification(\n",
      "    (bert): MMBTBase(\n",
      "      (mmbt): MMBTModel(\n",
      "        (transformer): BertModel(\n",
      "          (embeddings): BertEmbeddings(\n",
      "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "            (position_embeddings): Embedding(512, 768)\n",
      "            (token_type_embeddings): Embedding(2, 768)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (encoder): BertEncoder(\n",
      "            (layer): ModuleList(\n",
      "              (0): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (4): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (5): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (6): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (7): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (8): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (9): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (10): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (11): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (pooler): BertPooler(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (activation): Tanh()\n",
      "          )\n",
      "        )\n",
      "        (modal_encoder): ModalEmbeddings(\n",
      "          (encoder): ResNet152ImageEncoder(\n",
      "            (model): Sequential(\n",
      "              (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "              (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "              (4): Sequential(\n",
      "                (0): Bottleneck(\n",
      "                  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                  (downsample): Sequential(\n",
      "                    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  )\n",
      "                )\n",
      "                (1): Bottleneck(\n",
      "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (2): Bottleneck(\n",
      "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (5): Sequential(\n",
      "                (0): Bottleneck(\n",
      "                  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                  (downsample): Sequential(\n",
      "                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  )\n",
      "                )\n",
      "                (1): Bottleneck(\n",
      "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (2): Bottleneck(\n",
      "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (3): Bottleneck(\n",
      "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (4): Bottleneck(\n",
      "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (5): Bottleneck(\n",
      "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (6): Bottleneck(\n",
      "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (7): Bottleneck(\n",
      "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (6): Sequential(\n",
      "                (0): Bottleneck(\n",
      "                  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                  (downsample): Sequential(\n",
      "                    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  )\n",
      "                )\n",
      "                (1): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (2): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (3): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (4): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (5): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (6): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (7): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (8): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (9): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (10): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (11): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (12): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (13): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (14): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (15): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (16): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (17): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (18): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (19): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (20): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (21): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (22): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (23): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (24): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (25): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (26): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (27): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (28): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (29): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (30): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (31): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (32): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (33): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (34): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (35): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (7): Sequential(\n",
      "                (0): Bottleneck(\n",
      "                  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                  (downsample): Sequential(\n",
      "                    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  )\n",
      "                )\n",
      "                (1): Bottleneck(\n",
      "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "                (2): Bottleneck(\n",
      "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (relu): ReLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "          )\n",
      "          (proj_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses()\n",
      ")\n",
      "2020-05-30T20:54:27 INFO: Total Parameters: 169793346. Trained Parameters: 169793346\n",
      "2020-05-30T20:54:27 INFO: Starting inference on val set\n",
      "16it [00:08,  1.84it/s]                                                         \n",
      "2020-05-30T20:54:35 INFO: progress: 0/22000, val/total_loss: 2.4264, val/hateful_memes/cross_entropy: 2.4264, val/hateful_memes/accuracy: 0.5780, val/hateful_memes/binary_f1: 0.4219, val/hateful_memes/roc_auc: 0.6820\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/mmbt/with_features.yaml', 'model=mmbt', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=mmbt.hateful_memes.features'])\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/mmbt/with_features.yaml\n",
      "Overriding option model to mmbt\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to mmbt.hateful_memes.features\n",
      "Using seed 39380616\n",
      "Logging to: ./save/logs/train_2020-05-30T20:54:39.log\n",
      "2020-05-30T20:54:39 INFO: Loading datasets\n",
      "2020-05-30T20:54:43 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:54:47 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:54:47 INFO: Loading checkpoint\n",
      "2020-05-30T20:54:52 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:54:52 INFO: Checkpoint loaded\n",
      "2020-05-30T20:54:52 INFO: ===== Model =====\n",
      "2020-05-30T20:54:52 INFO: MMBT(\n",
      "  (model): MMBTForClassification(\n",
      "    (bert): MMBTBase(\n",
      "      (mmbt): MMBTModel(\n",
      "        (transformer): BertModel(\n",
      "          (embeddings): BertEmbeddings(\n",
      "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "            (position_embeddings): Embedding(512, 768)\n",
      "            (token_type_embeddings): Embedding(2, 768)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (encoder): BertEncoder(\n",
      "            (layer): ModuleList(\n",
      "              (0): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (4): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (5): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (6): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (7): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (8): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (9): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (10): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (11): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (pooler): BertPooler(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (activation): Tanh()\n",
      "          )\n",
      "        )\n",
      "        (modal_encoder): ModalEmbeddings(\n",
      "          (encoder): FinetuneFasterRcnnFpnFc7(\n",
      "            (lc): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (proj_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses()\n",
      ")\n",
      "2020-05-30T20:54:52 INFO: Total Parameters: 115845890. Trained Parameters: 115845890\n",
      "2020-05-30T20:54:52 INFO: Starting inference on val set\n",
      "16it [00:08,  1.85it/s]                                                         \n",
      "2020-05-30T20:55:00 INFO: progress: 0/22000, val/total_loss: 0.9020, val/hateful_memes/cross_entropy: 0.9020, val/hateful_memes/accuracy: 0.5840, val/hateful_memes/binary_f1: 0.3500, val/hateful_memes/roc_auc: 0.7131\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct'])\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/vilbert/defaults.yaml\n",
      "Overriding option model to vilbert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.direct\n",
      "Using seed 4357455\n",
      "Logging to: ./save/logs/train_2020-05-30T20:55:04.log\n",
      "2020-05-30T20:55:04 INFO: Loading datasets\n",
      "2020-05-30T20:55:08 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:55:13 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:55:13 INFO: Loading checkpoint\n",
      "2020-05-30T20:55:22 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:55:22 INFO: Checkpoint loaded\n",
      "2020-05-30T20:55:22 INFO: ===== Model =====\n",
      "2020-05-30T20:55:22 INFO: ViLBERT(\n",
      "  (model): ViLBERTForClassification(\n",
      "    (bert): ViLBERTBase(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (v_embeddings): BertImageFeatureEmbeddings(\n",
      "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (v_layer): ModuleList(\n",
      "          (0): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (c_layer): ModuleList(\n",
      "          (0): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (t_pooler): BertTextPooler(\n",
      "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (v_pooler): BertImagePooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses()\n",
      ")\n",
      "2020-05-30T20:55:22 INFO: Total Parameters: 247780354. Trained Parameters: 247780354\n",
      "2020-05-30T20:55:22 INFO: Starting inference on val set\n",
      "16it [00:09,  1.62it/s]                                                         \n",
      "2020-05-30T20:55:32 INFO: progress: 0/22000, val/total_loss: 2.6754, val/hateful_memes/cross_entropy: 2.6754, val/hateful_memes/accuracy: 0.6220, val/hateful_memes/binary_f1: 0.4878, val/hateful_memes/roc_auc: 0.7004\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/visual_bert/direct.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct'])\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/visual_bert/direct.yaml\n",
      "Overriding option model to visual_bert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.direct\n",
      "Using seed 36219150\n",
      "Logging to: ./save/logs/train_2020-05-30T20:55:36.log\n",
      "2020-05-30T20:55:36 INFO: Loading datasets\n",
      "2020-05-30T20:55:39 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:55:43 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:55:43 INFO: Loading checkpoint\n",
      "2020-05-30T20:55:48 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-05-30T20:55:48 INFO: Checkpoint loaded\n",
      "2020-05-30T20:55:48 INFO: ===== Model =====\n",
      "2020-05-30T20:55:48 INFO: VisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses()\n",
      ")\n",
      "2020-05-30T20:55:48 INFO: Total Parameters: 112044290. Trained Parameters: 112044290\n",
      "2020-05-30T20:55:48 INFO: Starting inference on val set\n",
      "4it [00:09,  2.42s/it]                                                          \n",
      "2020-05-30T20:55:58 INFO: progress: 0/22000, val/total_loss: 1.3830, val/hateful_memes/cross_entropy: 1.3830, val/hateful_memes/accuracy: 0.6200, val/hateful_memes/binary_f1: 0.5250, val/hateful_memes/roc_auc: 0.7103\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/vilbert/from_cc.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original'])\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/vilbert/from_cc.yaml\n",
      "Overriding option model to vilbert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.from_cc_original\n",
      "Using seed 1834786\n",
      "Logging to: ./save/logs/train_2020-05-30T20:56:01.log\n",
      "2020-05-30T20:56:01 INFO: Loading datasets\n",
      "2020-05-30T20:56:06 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:56:10 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:56:10 INFO: Loading checkpoint\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.v_embeddings.image_embeddings.weight from model.bert.v_embeddings.image_embeddings.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.v_embeddings.image_embeddings.bias from model.bert.v_embeddings.image_embeddings.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.v_embeddings.image_location_embeddings.weight from model.bert.v_embeddings.image_location_embeddings.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.v_embeddings.image_location_embeddings.bias from model.bert.v_embeddings.image_location_embeddings.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.v_embeddings.LayerNorm.weight from model.bert.v_embeddings.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.v_embeddings.LayerNorm.bias from model.bert.v_embeddings.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "2020-05-30T20:56:11 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.attention.self.query.weight from model.bert.encoder.v_layer.0.attention.self.query.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.attention.self.query.bias from model.bert.encoder.v_layer.0.attention.self.query.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.attention.self.key.weight from model.bert.encoder.v_layer.0.attention.self.key.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.attention.self.key.bias from model.bert.encoder.v_layer.0.attention.self.key.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.attention.self.value.weight from model.bert.encoder.v_layer.0.attention.self.value.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.attention.self.value.bias from model.bert.encoder.v_layer.0.attention.self.value.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.attention.output.dense.weight from model.bert.encoder.v_layer.0.attention.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.attention.output.dense.bias from model.bert.encoder.v_layer.0.attention.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.intermediate.dense.weight from model.bert.encoder.v_layer.0.intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.intermediate.dense.bias from model.bert.encoder.v_layer.0.intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.output.dense.weight from model.bert.encoder.v_layer.0.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.output.dense.bias from model.bert.encoder.v_layer.0.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.output.LayerNorm.weight from model.bert.encoder.v_layer.0.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.0.output.LayerNorm.bias from model.bert.encoder.v_layer.0.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.attention.self.query.weight from model.bert.encoder.v_layer.1.attention.self.query.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.attention.self.query.bias from model.bert.encoder.v_layer.1.attention.self.query.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.attention.self.key.weight from model.bert.encoder.v_layer.1.attention.self.key.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.attention.self.key.bias from model.bert.encoder.v_layer.1.attention.self.key.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.attention.self.value.weight from model.bert.encoder.v_layer.1.attention.self.value.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.attention.self.value.bias from model.bert.encoder.v_layer.1.attention.self.value.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.attention.output.dense.weight from model.bert.encoder.v_layer.1.attention.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.attention.output.dense.bias from model.bert.encoder.v_layer.1.attention.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.intermediate.dense.weight from model.bert.encoder.v_layer.1.intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.intermediate.dense.bias from model.bert.encoder.v_layer.1.intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.output.dense.weight from model.bert.encoder.v_layer.1.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.output.dense.bias from model.bert.encoder.v_layer.1.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.output.LayerNorm.weight from model.bert.encoder.v_layer.1.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.1.output.LayerNorm.bias from model.bert.encoder.v_layer.1.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.attention.self.query.weight from model.bert.encoder.v_layer.2.attention.self.query.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.attention.self.query.bias from model.bert.encoder.v_layer.2.attention.self.query.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.attention.self.key.weight from model.bert.encoder.v_layer.2.attention.self.key.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.attention.self.key.bias from model.bert.encoder.v_layer.2.attention.self.key.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.attention.self.value.weight from model.bert.encoder.v_layer.2.attention.self.value.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.attention.self.value.bias from model.bert.encoder.v_layer.2.attention.self.value.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.attention.output.dense.weight from model.bert.encoder.v_layer.2.attention.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.attention.output.dense.bias from model.bert.encoder.v_layer.2.attention.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.intermediate.dense.weight from model.bert.encoder.v_layer.2.intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.intermediate.dense.bias from model.bert.encoder.v_layer.2.intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.output.dense.weight from model.bert.encoder.v_layer.2.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.output.dense.bias from model.bert.encoder.v_layer.2.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.output.LayerNorm.weight from model.bert.encoder.v_layer.2.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.2.output.LayerNorm.bias from model.bert.encoder.v_layer.2.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.attention.self.query.weight from model.bert.encoder.v_layer.3.attention.self.query.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.attention.self.query.bias from model.bert.encoder.v_layer.3.attention.self.query.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.attention.self.key.weight from model.bert.encoder.v_layer.3.attention.self.key.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.attention.self.key.bias from model.bert.encoder.v_layer.3.attention.self.key.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.attention.self.value.weight from model.bert.encoder.v_layer.3.attention.self.value.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.attention.self.value.bias from model.bert.encoder.v_layer.3.attention.self.value.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.attention.output.dense.weight from model.bert.encoder.v_layer.3.attention.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.attention.output.dense.bias from model.bert.encoder.v_layer.3.attention.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.intermediate.dense.weight from model.bert.encoder.v_layer.3.intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.intermediate.dense.bias from model.bert.encoder.v_layer.3.intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.output.dense.weight from model.bert.encoder.v_layer.3.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.output.dense.bias from model.bert.encoder.v_layer.3.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.output.LayerNorm.weight from model.bert.encoder.v_layer.3.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.3.output.LayerNorm.bias from model.bert.encoder.v_layer.3.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.attention.self.query.weight from model.bert.encoder.v_layer.4.attention.self.query.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.attention.self.query.bias from model.bert.encoder.v_layer.4.attention.self.query.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.attention.self.key.weight from model.bert.encoder.v_layer.4.attention.self.key.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.attention.self.key.bias from model.bert.encoder.v_layer.4.attention.self.key.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.attention.self.value.weight from model.bert.encoder.v_layer.4.attention.self.value.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.attention.self.value.bias from model.bert.encoder.v_layer.4.attention.self.value.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.attention.output.dense.weight from model.bert.encoder.v_layer.4.attention.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.attention.output.dense.bias from model.bert.encoder.v_layer.4.attention.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.intermediate.dense.weight from model.bert.encoder.v_layer.4.intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.intermediate.dense.bias from model.bert.encoder.v_layer.4.intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.output.dense.weight from model.bert.encoder.v_layer.4.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.output.dense.bias from model.bert.encoder.v_layer.4.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.output.LayerNorm.weight from model.bert.encoder.v_layer.4.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.4.output.LayerNorm.bias from model.bert.encoder.v_layer.4.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.attention.self.query.weight from model.bert.encoder.v_layer.5.attention.self.query.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.attention.self.query.bias from model.bert.encoder.v_layer.5.attention.self.query.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.attention.self.key.weight from model.bert.encoder.v_layer.5.attention.self.key.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.attention.self.key.bias from model.bert.encoder.v_layer.5.attention.self.key.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.attention.self.value.weight from model.bert.encoder.v_layer.5.attention.self.value.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.attention.self.value.bias from model.bert.encoder.v_layer.5.attention.self.value.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.attention.output.dense.weight from model.bert.encoder.v_layer.5.attention.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.attention.output.dense.bias from model.bert.encoder.v_layer.5.attention.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.intermediate.dense.weight from model.bert.encoder.v_layer.5.intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.intermediate.dense.bias from model.bert.encoder.v_layer.5.intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.output.dense.weight from model.bert.encoder.v_layer.5.output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.output.dense.bias from model.bert.encoder.v_layer.5.output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.output.LayerNorm.weight from model.bert.encoder.v_layer.5.output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.v_layer.5.output.LayerNorm.bias from model.bert.encoder.v_layer.5.output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biattention.query1.weight from model.bert.encoder.c_layer.0.biattention.query1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biattention.query1.bias from model.bert.encoder.c_layer.0.biattention.query1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biattention.key1.weight from model.bert.encoder.c_layer.0.biattention.key1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biattention.key1.bias from model.bert.encoder.c_layer.0.biattention.key1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biattention.value1.weight from model.bert.encoder.c_layer.0.biattention.value1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biattention.value1.bias from model.bert.encoder.c_layer.0.biattention.value1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biattention.query2.weight from model.bert.encoder.c_layer.0.biattention.query2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biattention.query2.bias from model.bert.encoder.c_layer.0.biattention.query2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biattention.key2.weight from model.bert.encoder.c_layer.0.biattention.key2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biattention.key2.bias from model.bert.encoder.c_layer.0.biattention.key2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biattention.value2.weight from model.bert.encoder.c_layer.0.biattention.value2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biattention.value2.bias from model.bert.encoder.c_layer.0.biattention.value2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense1.weight from model.bert.encoder.c_layer.0.biOutput.dense1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense1.bias from model.bert.encoder.c_layer.0.biOutput.dense1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense1.weight from model.bert.encoder.c_layer.0.biOutput.q_dense1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense1.bias from model.bert.encoder.c_layer.0.biOutput.q_dense1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense2.weight from model.bert.encoder.c_layer.0.biOutput.dense2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense2.bias from model.bert.encoder.c_layer.0.biOutput.dense2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense2.weight from model.bert.encoder.c_layer.0.biOutput.q_dense2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense2.bias from model.bert.encoder.c_layer.0.biOutput.q_dense2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.v_intermediate.dense.weight from model.bert.encoder.c_layer.0.v_intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.v_intermediate.dense.bias from model.bert.encoder.c_layer.0.v_intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.v_output.dense.weight from model.bert.encoder.c_layer.0.v_output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.v_output.dense.bias from model.bert.encoder.c_layer.0.v_output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.v_output.LayerNorm.weight from model.bert.encoder.c_layer.0.v_output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.v_output.LayerNorm.bias from model.bert.encoder.c_layer.0.v_output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.t_intermediate.dense.weight from model.bert.encoder.c_layer.0.t_intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.t_intermediate.dense.bias from model.bert.encoder.c_layer.0.t_intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.t_output.dense.weight from model.bert.encoder.c_layer.0.t_output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.t_output.dense.bias from model.bert.encoder.c_layer.0.t_output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.t_output.LayerNorm.weight from model.bert.encoder.c_layer.0.t_output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.0.t_output.LayerNorm.bias from model.bert.encoder.c_layer.0.t_output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biattention.query1.weight from model.bert.encoder.c_layer.1.biattention.query1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biattention.query1.bias from model.bert.encoder.c_layer.1.biattention.query1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biattention.key1.weight from model.bert.encoder.c_layer.1.biattention.key1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biattention.key1.bias from model.bert.encoder.c_layer.1.biattention.key1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biattention.value1.weight from model.bert.encoder.c_layer.1.biattention.value1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biattention.value1.bias from model.bert.encoder.c_layer.1.biattention.value1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biattention.query2.weight from model.bert.encoder.c_layer.1.biattention.query2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biattention.query2.bias from model.bert.encoder.c_layer.1.biattention.query2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biattention.key2.weight from model.bert.encoder.c_layer.1.biattention.key2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biattention.key2.bias from model.bert.encoder.c_layer.1.biattention.key2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biattention.value2.weight from model.bert.encoder.c_layer.1.biattention.value2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biattention.value2.bias from model.bert.encoder.c_layer.1.biattention.value2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense1.weight from model.bert.encoder.c_layer.1.biOutput.dense1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense1.bias from model.bert.encoder.c_layer.1.biOutput.dense1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense1.weight from model.bert.encoder.c_layer.1.biOutput.q_dense1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense1.bias from model.bert.encoder.c_layer.1.biOutput.q_dense1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense2.weight from model.bert.encoder.c_layer.1.biOutput.dense2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense2.bias from model.bert.encoder.c_layer.1.biOutput.dense2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense2.weight from model.bert.encoder.c_layer.1.biOutput.q_dense2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense2.bias from model.bert.encoder.c_layer.1.biOutput.q_dense2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.v_intermediate.dense.weight from model.bert.encoder.c_layer.1.v_intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.v_intermediate.dense.bias from model.bert.encoder.c_layer.1.v_intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.v_output.dense.weight from model.bert.encoder.c_layer.1.v_output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.v_output.dense.bias from model.bert.encoder.c_layer.1.v_output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.v_output.LayerNorm.weight from model.bert.encoder.c_layer.1.v_output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.v_output.LayerNorm.bias from model.bert.encoder.c_layer.1.v_output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.t_intermediate.dense.weight from model.bert.encoder.c_layer.1.t_intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.t_intermediate.dense.bias from model.bert.encoder.c_layer.1.t_intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.t_output.dense.weight from model.bert.encoder.c_layer.1.t_output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.t_output.dense.bias from model.bert.encoder.c_layer.1.t_output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.t_output.LayerNorm.weight from model.bert.encoder.c_layer.1.t_output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.1.t_output.LayerNorm.bias from model.bert.encoder.c_layer.1.t_output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biattention.query1.weight from model.bert.encoder.c_layer.2.biattention.query1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biattention.query1.bias from model.bert.encoder.c_layer.2.biattention.query1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biattention.key1.weight from model.bert.encoder.c_layer.2.biattention.key1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biattention.key1.bias from model.bert.encoder.c_layer.2.biattention.key1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biattention.value1.weight from model.bert.encoder.c_layer.2.biattention.value1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biattention.value1.bias from model.bert.encoder.c_layer.2.biattention.value1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biattention.query2.weight from model.bert.encoder.c_layer.2.biattention.query2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biattention.query2.bias from model.bert.encoder.c_layer.2.biattention.query2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biattention.key2.weight from model.bert.encoder.c_layer.2.biattention.key2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biattention.key2.bias from model.bert.encoder.c_layer.2.biattention.key2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biattention.value2.weight from model.bert.encoder.c_layer.2.biattention.value2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biattention.value2.bias from model.bert.encoder.c_layer.2.biattention.value2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense1.weight from model.bert.encoder.c_layer.2.biOutput.dense1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense1.bias from model.bert.encoder.c_layer.2.biOutput.dense1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense1.weight from model.bert.encoder.c_layer.2.biOutput.q_dense1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense1.bias from model.bert.encoder.c_layer.2.biOutput.q_dense1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense2.weight from model.bert.encoder.c_layer.2.biOutput.dense2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense2.bias from model.bert.encoder.c_layer.2.biOutput.dense2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense2.weight from model.bert.encoder.c_layer.2.biOutput.q_dense2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense2.bias from model.bert.encoder.c_layer.2.biOutput.q_dense2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.v_intermediate.dense.weight from model.bert.encoder.c_layer.2.v_intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.v_intermediate.dense.bias from model.bert.encoder.c_layer.2.v_intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.v_output.dense.weight from model.bert.encoder.c_layer.2.v_output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.v_output.dense.bias from model.bert.encoder.c_layer.2.v_output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.v_output.LayerNorm.weight from model.bert.encoder.c_layer.2.v_output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.v_output.LayerNorm.bias from model.bert.encoder.c_layer.2.v_output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.t_intermediate.dense.weight from model.bert.encoder.c_layer.2.t_intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.t_intermediate.dense.bias from model.bert.encoder.c_layer.2.t_intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.t_output.dense.weight from model.bert.encoder.c_layer.2.t_output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.t_output.dense.bias from model.bert.encoder.c_layer.2.t_output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.t_output.LayerNorm.weight from model.bert.encoder.c_layer.2.t_output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.2.t_output.LayerNorm.bias from model.bert.encoder.c_layer.2.t_output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biattention.query1.weight from model.bert.encoder.c_layer.3.biattention.query1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biattention.query1.bias from model.bert.encoder.c_layer.3.biattention.query1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biattention.key1.weight from model.bert.encoder.c_layer.3.biattention.key1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biattention.key1.bias from model.bert.encoder.c_layer.3.biattention.key1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biattention.value1.weight from model.bert.encoder.c_layer.3.biattention.value1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biattention.value1.bias from model.bert.encoder.c_layer.3.biattention.value1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biattention.query2.weight from model.bert.encoder.c_layer.3.biattention.query2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biattention.query2.bias from model.bert.encoder.c_layer.3.biattention.query2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biattention.key2.weight from model.bert.encoder.c_layer.3.biattention.key2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biattention.key2.bias from model.bert.encoder.c_layer.3.biattention.key2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biattention.value2.weight from model.bert.encoder.c_layer.3.biattention.value2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biattention.value2.bias from model.bert.encoder.c_layer.3.biattention.value2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense1.weight from model.bert.encoder.c_layer.3.biOutput.dense1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense1.bias from model.bert.encoder.c_layer.3.biOutput.dense1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense1.weight from model.bert.encoder.c_layer.3.biOutput.q_dense1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense1.bias from model.bert.encoder.c_layer.3.biOutput.q_dense1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense2.weight from model.bert.encoder.c_layer.3.biOutput.dense2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense2.bias from model.bert.encoder.c_layer.3.biOutput.dense2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense2.weight from model.bert.encoder.c_layer.3.biOutput.q_dense2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense2.bias from model.bert.encoder.c_layer.3.biOutput.q_dense2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.v_intermediate.dense.weight from model.bert.encoder.c_layer.3.v_intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.v_intermediate.dense.bias from model.bert.encoder.c_layer.3.v_intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.v_output.dense.weight from model.bert.encoder.c_layer.3.v_output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.v_output.dense.bias from model.bert.encoder.c_layer.3.v_output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.v_output.LayerNorm.weight from model.bert.encoder.c_layer.3.v_output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.v_output.LayerNorm.bias from model.bert.encoder.c_layer.3.v_output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.t_intermediate.dense.weight from model.bert.encoder.c_layer.3.t_intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.t_intermediate.dense.bias from model.bert.encoder.c_layer.3.t_intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.t_output.dense.weight from model.bert.encoder.c_layer.3.t_output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.t_output.dense.bias from model.bert.encoder.c_layer.3.t_output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.t_output.LayerNorm.weight from model.bert.encoder.c_layer.3.t_output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.3.t_output.LayerNorm.bias from model.bert.encoder.c_layer.3.t_output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biattention.query1.weight from model.bert.encoder.c_layer.4.biattention.query1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biattention.query1.bias from model.bert.encoder.c_layer.4.biattention.query1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biattention.key1.weight from model.bert.encoder.c_layer.4.biattention.key1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biattention.key1.bias from model.bert.encoder.c_layer.4.biattention.key1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biattention.value1.weight from model.bert.encoder.c_layer.4.biattention.value1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biattention.value1.bias from model.bert.encoder.c_layer.4.biattention.value1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biattention.query2.weight from model.bert.encoder.c_layer.4.biattention.query2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biattention.query2.bias from model.bert.encoder.c_layer.4.biattention.query2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biattention.key2.weight from model.bert.encoder.c_layer.4.biattention.key2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biattention.key2.bias from model.bert.encoder.c_layer.4.biattention.key2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biattention.value2.weight from model.bert.encoder.c_layer.4.biattention.value2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biattention.value2.bias from model.bert.encoder.c_layer.4.biattention.value2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense1.weight from model.bert.encoder.c_layer.4.biOutput.dense1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense1.bias from model.bert.encoder.c_layer.4.biOutput.dense1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense1.weight from model.bert.encoder.c_layer.4.biOutput.q_dense1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense1.bias from model.bert.encoder.c_layer.4.biOutput.q_dense1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense2.weight from model.bert.encoder.c_layer.4.biOutput.dense2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense2.bias from model.bert.encoder.c_layer.4.biOutput.dense2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense2.weight from model.bert.encoder.c_layer.4.biOutput.q_dense2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense2.bias from model.bert.encoder.c_layer.4.biOutput.q_dense2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.v_intermediate.dense.weight from model.bert.encoder.c_layer.4.v_intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.v_intermediate.dense.bias from model.bert.encoder.c_layer.4.v_intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.v_output.dense.weight from model.bert.encoder.c_layer.4.v_output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.v_output.dense.bias from model.bert.encoder.c_layer.4.v_output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.v_output.LayerNorm.weight from model.bert.encoder.c_layer.4.v_output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.v_output.LayerNorm.bias from model.bert.encoder.c_layer.4.v_output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.t_intermediate.dense.weight from model.bert.encoder.c_layer.4.t_intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.t_intermediate.dense.bias from model.bert.encoder.c_layer.4.t_intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.t_output.dense.weight from model.bert.encoder.c_layer.4.t_output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.t_output.dense.bias from model.bert.encoder.c_layer.4.t_output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.t_output.LayerNorm.weight from model.bert.encoder.c_layer.4.t_output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.4.t_output.LayerNorm.bias from model.bert.encoder.c_layer.4.t_output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biattention.query1.weight from model.bert.encoder.c_layer.5.biattention.query1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biattention.query1.bias from model.bert.encoder.c_layer.5.biattention.query1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biattention.key1.weight from model.bert.encoder.c_layer.5.biattention.key1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biattention.key1.bias from model.bert.encoder.c_layer.5.biattention.key1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biattention.value1.weight from model.bert.encoder.c_layer.5.biattention.value1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biattention.value1.bias from model.bert.encoder.c_layer.5.biattention.value1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biattention.query2.weight from model.bert.encoder.c_layer.5.biattention.query2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biattention.query2.bias from model.bert.encoder.c_layer.5.biattention.query2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biattention.key2.weight from model.bert.encoder.c_layer.5.biattention.key2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biattention.key2.bias from model.bert.encoder.c_layer.5.biattention.key2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biattention.value2.weight from model.bert.encoder.c_layer.5.biattention.value2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biattention.value2.bias from model.bert.encoder.c_layer.5.biattention.value2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense1.weight from model.bert.encoder.c_layer.5.biOutput.dense1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense1.bias from model.bert.encoder.c_layer.5.biOutput.dense1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense1.weight from model.bert.encoder.c_layer.5.biOutput.q_dense1.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense1.bias from model.bert.encoder.c_layer.5.biOutput.q_dense1.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense2.weight from model.bert.encoder.c_layer.5.biOutput.dense2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense2.bias from model.bert.encoder.c_layer.5.biOutput.dense2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense2.weight from model.bert.encoder.c_layer.5.biOutput.q_dense2.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense2.bias from model.bert.encoder.c_layer.5.biOutput.q_dense2.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.v_intermediate.dense.weight from model.bert.encoder.c_layer.5.v_intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.v_intermediate.dense.bias from model.bert.encoder.c_layer.5.v_intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.v_output.dense.weight from model.bert.encoder.c_layer.5.v_output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.v_output.dense.bias from model.bert.encoder.c_layer.5.v_output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.v_output.LayerNorm.weight from model.bert.encoder.c_layer.5.v_output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.v_output.LayerNorm.bias from model.bert.encoder.c_layer.5.v_output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.t_intermediate.dense.weight from model.bert.encoder.c_layer.5.t_intermediate.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.t_intermediate.dense.bias from model.bert.encoder.c_layer.5.t_intermediate.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.t_output.dense.weight from model.bert.encoder.c_layer.5.t_output.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.t_output.dense.bias from model.bert.encoder.c_layer.5.t_output.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.t_output.LayerNorm.weight from model.bert.encoder.c_layer.5.t_output.LayerNorm.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.encoder.c_layer.5.t_output.LayerNorm.bias from model.bert.encoder.c_layer.5.t_output.LayerNorm.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.t_pooler.dense.weight from model.bert.t_pooler.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.t_pooler.dense.bias from model.bert.t_pooler.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.v_pooler.dense.weight from model.bert.v_pooler.dense.weight\n",
      "2020-05-30T20:56:12 INFO: Copying model.bert.v_pooler.dense.bias from model.bert.v_pooler.dense.bias\n",
      "2020-05-30T20:56:12 INFO: Pretrained model loaded\n",
      "2020-05-30T20:56:12 INFO: ===== Model =====\n",
      "2020-05-30T20:56:12 INFO: ViLBERT(\n",
      "  (model): ViLBERTForClassification(\n",
      "    (bert): ViLBERTBase(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (v_embeddings): BertImageFeatureEmbeddings(\n",
      "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (v_layer): ModuleList(\n",
      "          (0): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (c_layer): ModuleList(\n",
      "          (0): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (t_pooler): BertTextPooler(\n",
      "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (v_pooler): BertImagePooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses()\n",
      ")\n",
      "2020-05-30T20:56:12 INFO: Total Parameters: 247780354. Trained Parameters: 247780354\n",
      "2020-05-30T20:56:12 INFO: Starting inference on val set\n",
      "16it [00:09,  1.62it/s]                                                         \n",
      "2020-05-30T20:56:22 INFO: progress: 0/22000, val/total_loss: 0.7503, val/hateful_memes/cross_entropy: 0.7503, val/hateful_memes/accuracy: 0.5640, val/hateful_memes/binary_f1: 0.3978, val/hateful_memes/roc_auc: 0.5824\n",
      "Namespace(config_override=None, local_rank=None, opts=['config=configs/visual_bert/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco'])\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/visual_bert/from_coco.yaml\n",
      "Overriding option model to visual_bert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.from_coco\n",
      "Using seed 25797415\n",
      "Logging to: ./save/logs/train_2020-05-30T20:56:25.log\n",
      "2020-05-30T20:56:25 INFO: Loading datasets\n",
      "2020-05-30T20:56:29 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-05-30T20:56:33 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-05-30T20:56:33 INFO: Loading checkpoint\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
      "2020-05-30T20:56:34 INFO: Copying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
      "2020-05-30T20:56:34 INFO: Pretrained model loaded\n",
      "2020-05-30T20:56:34 INFO: ===== Model =====\n",
      "2020-05-30T20:56:34 INFO: VisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses()\n",
      ")\n",
      "2020-05-30T20:56:34 INFO: Total Parameters: 112044290. Trained Parameters: 112044290\n",
      "2020-05-30T20:56:34 INFO: Starting inference on val set\n",
      "8it [00:08,  1.10s/it]                                                          \n",
      "2020-05-30T20:56:43 INFO: progress: 0/22000, val/total_loss: 0.7259, val/hateful_memes/cross_entropy: 0.7259, val/hateful_memes/accuracy: 0.4480, val/hateful_memes/binary_f1: 0.5519, val/hateful_memes/roc_auc: 0.3589\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "#Val\n",
    "for index, row in df_model_config.iterrows():\n",
    "    model_baseline = row['baseline']\n",
    "    model_key = row['model_key']\n",
    "    model_pretrained_key = row['pretrained_key']\n",
    "    baseline_config = row['baseline_config']\n",
    "    custom_config = row['custom_config']\n",
    "    str_exec = f'MMF_USER_DIR=\"..\" mmf_run config={baseline_config} model={model_key} dataset=hateful_memes run_type=val checkpoint.resume_zoo={model_pretrained_key}'\n",
    "    !{str_exec}\n",
    "#     print(str_exec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

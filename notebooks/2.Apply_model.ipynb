{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "PATH_CURRENT = '/home/jupyter/meme_hateful_detection'\n",
    "PATH_MODEL = f'{PATH_CURRENT}/models'\n",
    "PATH_DATA = f'{PATH_CURRENT}/data/raw'\n",
    "PATH_SAVE = f'{PATH_CURRENT}/save'\n",
    "PATH_LOGS = f'{PATH_CURRENT}/logs'\n",
    "PATH_TENSOR = f'{PATH_CURRENT}/tensor_logs'\n",
    "PATH_REPO = f'{PATH_SAVE}/reports'\n",
    "\n",
    "\n",
    "os.environ\n",
    "os.environ['MMF_DATA_DIR'] = PATH_DATA\n",
    "os.environ['MMF_SAVE_DIR'] = PATH_SAVE\n",
    "os.environ['MMF_LOG_DIR']  = PATH_LOGS\n",
    "os.environ['MMF_REPORT_DIR']  = PATH_REPO\n",
    "os.environ['MMF_TENSORBOARD_LOGDIR']  = PATH_TENSOR\n",
    "os.environ['MMF_USER_DIR']  = PATH_CURRENT\n",
    "os.environ['OC_DISABLE_DOT_ACCESS_WARNING'] = '1'\n",
    "# print(os.environ)\n",
    "\n",
    "if PATH_CURRENT not in sys.path:\n",
    "    sys.path.append(PATH_CURRENT)\n",
    "if PATH_MODEL not in sys.path:\n",
    "    sys.path.append(PATH_MODEL)\n",
    "if PATH_DATA not in sys.path:\n",
    "    sys.path.append(PATH_DATA)\n",
    "# print(sys.path)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Baseline         | Model Key      | Pretrained Key                                   | Baseine Config                                                     |\n",
    "|------------------|----------------|--------------------------------------------------|------------------------------------------------------------|\n",
    "| Image-Grid       | unimodal_image | unimodal_image.hateful_memes.images              | projects/hateful_memes/configs/unimodal/image.yaml         |\n",
    "| Image-Region     | unimodal_image | unimodal_image.hateful_memes.features            | projects/hateful_memes/configs/unimodal/with_features.yaml |\n",
    "| Text BERT        | unimodal_text  | unimodal_text.hateful_memes.bert                 | projects/hateful_memes/configs/unimodal/bert.yaml          |\n",
    "| Late Fusion      | late_fusion    | late_fusion.hateful_memes                        | projects/hateful_memes/configs/late_fusion/defaults.yaml   |\n",
    "| ConcatBERT       | concat_bert    | concat_bert.hateful_memes                        | projects/hateful_memes/configs/concat_bert/defaults.yaml   |\n",
    "| MMBT-Grid        | mmbt           | mmbt.hateful_memes.images                        | projects/hateful_memes/configs/mmbt/defaults.yaml          |\n",
    "| MMBT-Region      | mmbt           | mmbt.hateful_memes.features                      | projects/hateful_memes/configs/mmbt/with_features.yaml     |\n",
    "| ViLBERT          | vilbert        | vilbert.finetuned.hateful_memes.direct           | projects/hateful_memes/configs/vilbert/defaults.yaml       |\n",
    "| Visual BERT      | visual_bert    | visual_bert.finetuned.hateful_memes.direct       | projects/hateful_memes/configs/visual_bert/direct.yaml     |\n",
    "| ViLBERT CC       | vilbert        | vilbert.finetuned.hateful_memes.from_cc_original | projects/hateful_memes/configs/vilbert/from_cc.yaml        |\n",
    "| Visual BERT COCO | visual_bert    | visual_bert.finetuned.hateful_memes.from_coco    | projects/hateful_memes/configs/visual_bert/from_coco.yaml  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd ../data/raw/\n",
    "# !wget \"https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/Lnmwdnq3YcF7F3YsJncp.zip?AWSAccessKeyId=AKIAJYJLFLA7N3WRICBQ&Signature=Vc9g%2B2IZYWz%2B%2FOAv3Hum2akybC0%3D&Expires=1591925619\"\n",
    "# !mmf_convert_hm --zip_file={PATH_DATA}/facebook_memes.zip --password=KexZs4tn8hujn1nK --mmf_data_folder={PATH_DATA}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>model_key</th>\n",
       "      <th>pretrained_key</th>\n",
       "      <th>save_dir</th>\n",
       "      <th>baseline_config</th>\n",
       "      <th>custom_config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image-Grid</td>\n",
       "      <td>unimodal_image</td>\n",
       "      <td>unimodal_image.hateful_memes.images</td>\n",
       "      <td>unimodal_image_grid</td>\n",
       "      <td>configs/unimodal/image.yaml</td>\n",
       "      <td>configs/unimodal/image_custom.yaml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image-Region</td>\n",
       "      <td>unimodal_image</td>\n",
       "      <td>unimodal_image.hateful_memes.features</td>\n",
       "      <td>unimodal_image_region</td>\n",
       "      <td>configs/unimodal/with_features.yaml</td>\n",
       "      <td>configs/unimodal/with_features_custom.yaml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Text BERT</td>\n",
       "      <td>unimodal_text</td>\n",
       "      <td>unimodal_text.hateful_memes.bert</td>\n",
       "      <td>unimodal_text</td>\n",
       "      <td>configs/unimodal/bert.yaml</td>\n",
       "      <td>configs/unimodal/bert_custom.yaml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Late Fusion</td>\n",
       "      <td>late_fusion</td>\n",
       "      <td>late_fusion.hateful_memes</td>\n",
       "      <td>late_fusion</td>\n",
       "      <td>configs/late_fusion/defaults.yaml</td>\n",
       "      <td>configs/late_fusion/defaults_custom.yaml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ConcatBERT</td>\n",
       "      <td>concat_bert</td>\n",
       "      <td>concat_bert.hateful_memes</td>\n",
       "      <td>concat_bert</td>\n",
       "      <td>configs/concat_bert/defaults.yaml</td>\n",
       "      <td>configs/concat_bert/defaults_custom.yaml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       baseline       model_key                         pretrained_key  \\\n",
       "0    Image-Grid  unimodal_image    unimodal_image.hateful_memes.images   \n",
       "1  Image-Region  unimodal_image  unimodal_image.hateful_memes.features   \n",
       "2     Text BERT   unimodal_text       unimodal_text.hateful_memes.bert   \n",
       "3   Late Fusion     late_fusion              late_fusion.hateful_memes   \n",
       "4    ConcatBERT     concat_bert              concat_bert.hateful_memes   \n",
       "\n",
       "                save_dir                      baseline_config  \\\n",
       "0    unimodal_image_grid          configs/unimodal/image.yaml   \n",
       "1  unimodal_image_region  configs/unimodal/with_features.yaml   \n",
       "2          unimodal_text           configs/unimodal/bert.yaml   \n",
       "3            late_fusion    configs/late_fusion/defaults.yaml   \n",
       "4            concat_bert    configs/concat_bert/defaults.yaml   \n",
       "\n",
       "                                custom_config  \n",
       "0          configs/unimodal/image_custom.yaml  \n",
       "1  configs/unimodal/with_features_custom.yaml  \n",
       "2           configs/unimodal/bert_custom.yaml  \n",
       "3    configs/late_fusion/defaults_custom.yaml  \n",
       "4    configs/concat_bert/defaults_custom.yaml  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_config = pd.read_csv(f'{PATH_MODEL}/model_config.csv', sep = ',')\n",
    "df_model_config.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/unimodal_image_grid\" mmf_run config=configs/unimodal/image_custom.yaml model=unimodal_image dataset=hateful_memes run_type=train checkpoint.resume_zoo=unimodal_image.hateful_memes.images\n",
      "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/unimodal_image_region\" mmf_run config=configs/unimodal/with_features_custom.yaml model=unimodal_image dataset=hateful_memes run_type=train checkpoint.resume_zoo=unimodal_image.hateful_memes.features\n",
      "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/unimodal_text\" mmf_run config=configs/unimodal/bert_custom.yaml model=unimodal_text dataset=hateful_memes run_type=train checkpoint.resume_zoo=unimodal_text.hateful_memes.bert\n",
      "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/late_fusion\" mmf_run config=configs/late_fusion/defaults_custom.yaml model=late_fusion dataset=hateful_memes run_type=train checkpoint.resume_zoo=late_fusion.hateful_memes\n",
      "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/concat_bert\" mmf_run config=configs/concat_bert/defaults_custom.yaml model=concat_bert dataset=hateful_memes run_type=train checkpoint.resume_zoo=concat_bert.hateful_memes\n",
      "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/mmbt_grid\" mmf_run config=configs/mmbt/defaults_custom.yaml model=mmbt dataset=hateful_memes run_type=train checkpoint.resume_zoo=mmbt.hateful_memes.images\n",
      "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/mmbt_region\" mmf_run config=configs/mmbt/with_features_custom.yaml model=mmbt dataset=hateful_memes run_type=train checkpoint.resume_zoo=mmbt.hateful_memes.features\n",
      "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/vilbert_direct\" mmf_run config=configs/vilbert/defaults_custom.yaml model=vilbert dataset=hateful_memes run_type=train checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct\n",
      "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/visual_bert_direct\" mmf_run config=configs/visual_bert/direct_custom.yaml model=visual_bert dataset=hateful_memes run_type=train checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct\n",
      "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/vilbert_from_cc\" mmf_run config=configs/vilbert/from_cc_custom.yaml model=vilbert dataset=hateful_memes run_type=train checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original\n",
      "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/visual_bert_from_coco\" mmf_run config=configs/visual_bert/from_coco_custom.yaml model=visual_bert dataset=hateful_memes run_type=train checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco\n"
     ]
    }
   ],
   "source": [
    "#Training model \n",
    "for index, row in df_model_config.iterrows():\n",
    "    model_baseline = row['baseline']\n",
    "    model_key = row['model_key']\n",
    "    model_pretrained_key = row['pretrained_key']\n",
    "    baseline_config = row['baseline_config']\n",
    "    custom_config = row['custom_config']\n",
    "    save_dir = row['save_dir']\n",
    "    str_pret_eval = f'MMF_SAVE_DIR=\"{PATH_SAVE}/{save_dir}\" mmf_run config={custom_config} model={model_key} dataset=hateful_memes run_type=train checkpoint.resume_zoo={model_pretrained_key}'\n",
    "    print(str_pret_eval)\n",
    "#     !{str_pret_eval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/unimodal_image_grid\" mmf_run config=configs/unimodal/image_custom.yaml model=unimodal_image dataset=hateful_memes run_type=train checkpoint.resume_zoo=unimodal_image.hateful_memes.images\n",
    "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/unimodal_image_region\" mmf_run config=configs/unimodal/with_features_custom.yaml model=unimodal_image dataset=hateful_memes run_type=train checkpoint.resume_zoo=unimodal_image.hateful_memes.features\n",
    "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/unimodal_text\" mmf_run config=configs/unimodal/bert_custom.yaml model=unimodal_text dataset=hateful_memes run_type=train checkpoint.resume_zoo=unimodal_text.hateful_memes.bert\n",
    "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/late_fusion\" mmf_run config=configs/late_fusion/defaults_custom.yaml model=late_fusion dataset=hateful_memes run_type=train checkpoint.resume_zoo=late_fusion.hateful_memes\n",
    "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/concat_bert\" mmf_run config=configs/concat_bert/defaults_custom.yaml model=concat_bert dataset=hateful_memes run_type=train checkpoint.resume_zoo=concat_bert.hateful_memes\n",
    "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/mmbt_grid\" mmf_run config=configs/mmbt/defaults_custom.yaml model=mmbt dataset=hateful_memes run_type=train checkpoint.resume_zoo=mmbt.hateful_memes.images\n",
    "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/mmbt_region\" mmf_run config=configs/mmbt/with_features_custom.yaml model=mmbt dataset=hateful_memes run_type=train checkpoint.resume_zoo=mmbt.hateful_memes.features\n",
    "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/vilbert_direct\" mmf_run config=configs/vilbert/defaults_custom.yaml model=vilbert dataset=hateful_memes run_type=train checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct\n",
    "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/visual_bert_direct\" mmf_run config=configs/visual_bert/direct_custom.yaml model=visual_bert dataset=hateful_memes run_type=train checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct\n",
    "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/vilbert_from_cc\" mmf_run config=configs/vilbert/from_cc_custom.yaml model=vilbert dataset=hateful_memes run_type=train checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original\n",
    "MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/visual_bert_from_coco\" mmf_run config=configs/visual_bert/from_coco_custom.yaml model=visual_bert dataset=hateful_memes run_type=train checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/concat_bert/defaults_custom.yaml\n",
      "Overriding option model to concat_bert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to train\n",
      "Overriding option checkpoint.resume_zoo to concat_bert.hateful_memes\n",
      "Using seed 28702595\n",
      "Logging to: /home/jupyter/meme_hateful_detection/logs/train_2020-06-09T08:33:28.log\n",
      "2020-06-09T08:33:28 INFO: =====  Training Parameters    =====\n",
      "2020-06-09T08:33:28 INFO: {\n",
      "    \"batch_size\": 32,\n",
      "    \"checkpoint_interval\": 1000,\n",
      "    \"clip_gradients\": false,\n",
      "    \"clip_norm_mode\": \"all\",\n",
      "    \"dataset_size_proportional_sampling\": true,\n",
      "    \"device\": \"cuda\",\n",
      "    \"early_stop\": {\n",
      "        \"criteria\": \"hateful_memes/roc_auc\",\n",
      "        \"enabled\": false,\n",
      "        \"minimize\": false,\n",
      "        \"patience\": 4000\n",
      "    },\n",
      "    \"evaluate_metrics\": true,\n",
      "    \"evaluation_interval\": 1000,\n",
      "    \"experiment_name\": \"run\",\n",
      "    \"fast_read\": false,\n",
      "    \"find_unused_parameters\": false,\n",
      "    \"local_rank\": null,\n",
      "    \"log_detailed_config\": true,\n",
      "    \"log_format\": \"json\",\n",
      "    \"log_interval\": 100,\n",
      "    \"logger_level\": \"info\",\n",
      "    \"lr_ratio\": 0.1,\n",
      "    \"lr_scheduler\": true,\n",
      "    \"lr_steps\": [],\n",
      "    \"max_epochs\": null,\n",
      "    \"max_updates\": 22000,\n",
      "    \"num_workers\": 4,\n",
      "    \"pin_memory\": false,\n",
      "    \"seed\": 28702595,\n",
      "    \"should_not_log\": false,\n",
      "    \"tensorboard\": true,\n",
      "    \"trainer\": \"base_trainer\",\n",
      "    \"use_warmup\": false,\n",
      "    \"verbose_dump\": true,\n",
      "    \"warmup_factor\": 0.2,\n",
      "    \"warmup_iterations\": 1000\n",
      "}\n",
      "2020-06-09T08:33:28 INFO: ======  Dataset Attributes  ======\n",
      "2020-06-09T08:33:28 INFO: ======== hateful_memes =======\n",
      "2020-06-09T08:33:28 INFO: {\n",
      "    \"annotations\": {\n",
      "        \"test\": [\n",
      "            \"hateful_memes/defaults/annotations/test.jsonl\"\n",
      "        ],\n",
      "        \"train\": [\n",
      "            \"hateful_memes/defaults/annotations/train.jsonl\"\n",
      "        ],\n",
      "        \"val\": [\n",
      "            \"hateful_memes/defaults/annotations/dev.jsonl\"\n",
      "        ]\n",
      "    },\n",
      "    \"data_dir\": \"/home/jupyter/meme_hateful_detection/data/raw/datasets\",\n",
      "    \"depth_first\": false,\n",
      "    \"fast_read\": false,\n",
      "    \"features\": {\n",
      "        \"test\": [\n",
      "            \"hateful_memes/defaults/features/detectron.lmdb\"\n",
      "        ],\n",
      "        \"train\": [\n",
      "            \"hateful_memes/defaults/features/detectron.lmdb\"\n",
      "        ],\n",
      "        \"val\": [\n",
      "            \"hateful_memes/defaults/features/detectron.lmdb\"\n",
      "        ]\n",
      "    },\n",
      "    \"images\": {\n",
      "        \"test\": [\n",
      "            \"hateful_memes/defaults/images/\"\n",
      "        ],\n",
      "        \"train\": [\n",
      "            \"hateful_memes/defaults/images/\"\n",
      "        ],\n",
      "        \"val\": [\n",
      "            \"hateful_memes/defaults/images/\"\n",
      "        ]\n",
      "    },\n",
      "    \"max_features\": 100,\n",
      "    \"processors\": {\n",
      "        \"bbox_processor\": {\n",
      "            \"params\": {\n",
      "                \"max_length\": 50\n",
      "            },\n",
      "            \"type\": \"bbox\"\n",
      "        },\n",
      "        \"image_processor\": {\n",
      "            \"params\": {\n",
      "                \"transforms\": [\n",
      "                    {\n",
      "                        \"params\": {\n",
      "                            \"size\": [\n",
      "                                256,\n",
      "                                256\n",
      "                            ]\n",
      "                        },\n",
      "                        \"type\": \"Resize\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"params\": {\n",
      "                            \"size\": [\n",
      "                                224,\n",
      "                                224\n",
      "                            ]\n",
      "                        },\n",
      "                        \"type\": \"CenterCrop\"\n",
      "                    },\n",
      "                    \"ToTensor\",\n",
      "                    \"GrayScaleTo3Channels\",\n",
      "                    {\n",
      "                        \"params\": {\n",
      "                            \"mean\": [\n",
      "                                0.46777044,\n",
      "                                0.44531429,\n",
      "                                0.40661017\n",
      "                            ],\n",
      "                            \"std\": [\n",
      "                                0.12221994,\n",
      "                                0.12145835,\n",
      "                                0.14380469\n",
      "                            ]\n",
      "                        },\n",
      "                        \"type\": \"Normalize\"\n",
      "                    }\n",
      "                ]\n",
      "            },\n",
      "            \"type\": \"torchvision_transforms\"\n",
      "        },\n",
      "        \"text_processor\": {\n",
      "            \"params\": {\n",
      "                \"mask_probability\": 0,\n",
      "                \"max_length\": 14,\n",
      "                \"max_seq_length\": 128,\n",
      "                \"preprocessor\": {\n",
      "                    \"params\": {},\n",
      "                    \"type\": \"simple_sentence\"\n",
      "                },\n",
      "                \"tokenizer_config\": {\n",
      "                    \"params\": {\n",
      "                        \"do_lower_case\": true\n",
      "                    },\n",
      "                    \"type\": \"bert-base-uncased\"\n",
      "                },\n",
      "                \"vocab\": {\n",
      "                    \"embedding_name\": \"glove.6B.300d\",\n",
      "                    \"type\": \"intersected\",\n",
      "                    \"vocab_file\": \"hateful_memes/defaults/extras/vocabs/vocabulary_100k.txt\"\n",
      "                }\n",
      "            },\n",
      "            \"type\": \"bert_tokenizer\"\n",
      "        }\n",
      "    },\n",
      "    \"return_features_info\": false,\n",
      "    \"use_features\": false,\n",
      "    \"use_images\": true\n",
      "}\n",
      "2020-06-09T08:33:28 INFO: ======  Optimizer Attributes  ======\n",
      "2020-06-09T08:33:28 INFO: {\n",
      "    \"params\": {\n",
      "        \"eps\": 1e-08,\n",
      "        \"lr\": 1e-05\n",
      "    },\n",
      "    \"type\": \"adam_w\"\n",
      "}\n",
      "2020-06-09T08:33:28 INFO: ======  Model (concat_bert) Attributes  ======\n",
      "2020-06-09T08:33:28 INFO: {\n",
      "    \"bert_model_name\": \"bert-base-uncased\",\n",
      "    \"classifier\": {\n",
      "        \"params\": {\n",
      "            \"hidden_dim\": 768,\n",
      "            \"in_dim\": 205568,\n",
      "            \"num_layers\": 2,\n",
      "            \"out_dim\": 2\n",
      "        },\n",
      "        \"type\": \"mlp\"\n",
      "    },\n",
      "    \"direct_features_input\": false,\n",
      "    \"finetune_lr_multiplier\": 1,\n",
      "    \"freeze_complete_base\": false,\n",
      "    \"freeze_modal\": false,\n",
      "    \"freeze_text\": false,\n",
      "    \"losses\": [\n",
      "        {\n",
      "            \"type\": \"cross_entropy\"\n",
      "        }\n",
      "    ],\n",
      "    \"modal_encoder\": {\n",
      "        \"params\": {\n",
      "            \"num_output_features\": 1,\n",
      "            \"pool_type\": \"avg\",\n",
      "            \"pretrained\": true\n",
      "        },\n",
      "        \"type\": \"resnet152\"\n",
      "    },\n",
      "    \"modal_hidden_size\": 2048,\n",
      "    \"num_features\": 100,\n",
      "    \"num_labels\": 2,\n",
      "    \"text_encoder\": {\n",
      "        \"params\": {\n",
      "            \"bert_model_name\": \"bert-base-uncased\",\n",
      "            \"hidden_size\": 768,\n",
      "            \"num_attention_heads\": 12,\n",
      "            \"num_hidden_layers\": 12,\n",
      "            \"output_attentions\": false,\n",
      "            \"output_hidden_states\": false\n",
      "        },\n",
      "        \"type\": \"transformer\"\n",
      "    },\n",
      "    \"text_hidden_size\": 768\n",
      "}\n",
      "2020-06-09T08:33:28 INFO: Loading datasets\n",
      "2020-06-09T08:33:33 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-06-09T08:33:37 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-06-09T08:33:37 INFO: Loading checkpoint\n",
      "2020-06-09T08:33:39 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-06-09T08:33:39 INFO: Checkpoint loaded\n",
      "2020-06-09T08:33:39 INFO: ===== Model =====\n",
      "2020-06-09T08:33:39 INFO: ConcatBERT(\n",
      "  (base): FusionBase(\n",
      "    (text): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (modal): ResNet152ImageEncoder(\n",
      "      (model): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (4): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (6): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (7): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (6): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (7): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (8): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (9): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (10): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (11): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (12): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (13): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (14): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (15): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (16): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (17): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (18): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (19): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (20): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (21): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (22): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (23): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (24): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (25): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (26): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (27): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (28): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (29): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (30): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (31): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (32): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (33): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (34): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (35): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (classifier): MLPClassifer(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=2816, out_features=768, bias=True)\n",
      "      (1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.5, inplace=False)\n",
      "      (8): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses()\n",
      ")\n",
      "2020-06-09T08:33:39 INFO: Total Parameters: 170384706. Trained Parameters: 170384706\n",
      "2020-06-09T08:33:39 INFO: Starting training...\n",
      "2020-06-09T08:33:42 WARNING: /pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "\n",
      "2020-06-09T08:33:42 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T08:36:43 INFO: {\"progress\": \"100/22000\", \"train/total_loss\": \"0.5936\", \"train/total_loss/avg\": \"0.5936\", \"train/hateful_memes/cross_entropy\": \"0.5936\", \"train/hateful_memes/cross_entropy/avg\": \"0.5936\", \"train/hateful_memes/accuracy\": \"0.7500\", \"train/hateful_memes/accuracy/avg\": \"0.7500\", \"train/hateful_memes/binary_f1\": \"0.7143\", \"train/hateful_memes/binary_f1/avg\": \"0.7143\", \"train/hateful_memes/roc_auc\": \"0.8157\", \"train/hateful_memes/roc_auc/avg\": \"0.8157\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 1, \"num_updates\": 100, \"iterations\": 100, \"max_updates\": 22000, \"lr\": \"0.\", \"ups\": \"0.55\", \"time\": \"03m 03s 644ms\", \"time_since_start\": \"03m 14s 738ms\", \"eta\": \"11h 10m 18s 121ms\"}\n",
      "2020-06-09T08:36:45 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T08:39:50 INFO: {\"progress\": \"200/22000\", \"train/total_loss\": \"0.4071\", \"train/total_loss/avg\": \"0.5003\", \"train/hateful_memes/cross_entropy\": \"0.4071\", \"train/hateful_memes/cross_entropy/avg\": \"0.5003\", \"train/hateful_memes/accuracy\": \"0.7500\", \"train/hateful_memes/accuracy/avg\": \"0.7656\", \"train/hateful_memes/binary_f1\": \"0.7143\", \"train/hateful_memes/binary_f1/avg\": \"0.7275\", \"train/hateful_memes/roc_auc\": \"0.8157\", \"train/hateful_memes/roc_auc/avg\": \"0.8707\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 1, \"num_updates\": 200, \"iterations\": 200, \"max_updates\": 22000, \"lr\": \"0.\", \"ups\": \"0.54\", \"time\": \"03m 06s 911ms\", \"time_since_start\": \"06m 21s 649ms\", \"eta\": \"11h 19m 06s 654ms\"}\n",
      "2020-06-09T08:39:52 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T08:42:56 INFO: {\"progress\": \"300/22000\", \"train/total_loss\": \"0.4071\", \"train/total_loss/avg\": \"0.4576\", \"train/hateful_memes/cross_entropy\": \"0.4071\", \"train/hateful_memes/cross_entropy/avg\": \"0.4576\", \"train/hateful_memes/accuracy\": \"0.7812\", \"train/hateful_memes/accuracy/avg\": \"0.7812\", \"train/hateful_memes/binary_f1\": \"0.7143\", \"train/hateful_memes/binary_f1/avg\": \"0.7072\", \"train/hateful_memes/roc_auc\": \"0.8961\", \"train/hateful_memes/roc_auc/avg\": \"0.8792\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 2, \"num_updates\": 300, \"iterations\": 300, \"max_updates\": 22000, \"lr\": \"0.\", \"ups\": \"0.54\", \"time\": \"03m 06s 429ms\", \"time_since_start\": \"09m 28s 079ms\", \"eta\": \"11h 14m 15s 111ms\"}\n",
      "2020-06-09T08:42:58 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T08:46:03 INFO: {\"progress\": \"400/22000\", \"train/total_loss\": \"0.4071\", \"train/total_loss/avg\": \"0.4838\", \"train/hateful_memes/cross_entropy\": \"0.4071\", \"train/hateful_memes/cross_entropy/avg\": \"0.4838\", \"train/hateful_memes/accuracy\": \"0.7500\", \"train/hateful_memes/accuracy/avg\": \"0.7734\", \"train/hateful_memes/binary_f1\": \"0.6667\", \"train/hateful_memes/binary_f1/avg\": \"0.6804\", \"train/hateful_memes/roc_auc\": \"0.8157\", \"train/hateful_memes/roc_auc/avg\": \"0.8502\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 2, \"num_updates\": 400, \"iterations\": 400, \"max_updates\": 22000, \"lr\": \"0.\", \"ups\": \"0.54\", \"time\": \"03m 06s 588ms\", \"time_since_start\": \"12m 34s 667ms\", \"eta\": \"11h 11m 43s 172ms\"}\n",
      "2020-06-09T08:46:05 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T08:49:09 INFO: {\"progress\": \"500/22000\", \"train/total_loss\": \"0.4789\", \"train/total_loss/avg\": \"0.4828\", \"train/hateful_memes/cross_entropy\": \"0.4789\", \"train/hateful_memes/cross_entropy/avg\": \"0.4828\", \"train/hateful_memes/accuracy\": \"0.7812\", \"train/hateful_memes/accuracy/avg\": \"0.7750\", \"train/hateful_memes/binary_f1\": \"0.7143\", \"train/hateful_memes/binary_f1/avg\": \"0.6883\", \"train/hateful_memes/roc_auc\": \"0.8867\", \"train/hateful_memes/roc_auc/avg\": \"0.8575\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 2, \"num_updates\": 500, \"iterations\": 500, \"max_updates\": 22000, \"lr\": \"0.\", \"ups\": \"0.54\", \"time\": \"03m 06s 540ms\", \"time_since_start\": \"15m 41s 208ms\", \"eta\": \"11h 08m 26s 222ms\"}\n",
      "2020-06-09T08:49:11 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T08:52:16 INFO: {\"progress\": \"600/22000\", \"train/total_loss\": \"0.4071\", \"train/total_loss/avg\": \"0.4453\", \"train/hateful_memes/cross_entropy\": \"0.4071\", \"train/hateful_memes/cross_entropy/avg\": \"0.4453\", \"train/hateful_memes/accuracy\": \"0.7812\", \"train/hateful_memes/accuracy/avg\": \"0.8073\", \"train/hateful_memes/binary_f1\": \"0.7143\", \"train/hateful_memes/binary_f1/avg\": \"0.7292\", \"train/hateful_memes/roc_auc\": \"0.8867\", \"train/hateful_memes/roc_auc/avg\": \"0.8813\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 3, \"num_updates\": 600, \"iterations\": 600, \"max_updates\": 22000, \"lr\": \"0.\", \"ups\": \"0.54\", \"time\": \"03m 06s 959ms\", \"time_since_start\": \"18m 48s 168ms\", \"eta\": \"11h 06m 49s 363ms\"}\n",
      "2020-06-09T08:52:18 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T08:55:23 INFO: {\"progress\": \"700/22000\", \"train/total_loss\": \"0.4071\", \"train/total_loss/avg\": \"0.4384\", \"train/hateful_memes/cross_entropy\": \"0.4071\", \"train/hateful_memes/cross_entropy/avg\": \"0.4384\", \"train/hateful_memes/accuracy\": \"0.7812\", \"train/hateful_memes/accuracy/avg\": \"0.8080\", \"train/hateful_memes/binary_f1\": \"0.7143\", \"train/hateful_memes/binary_f1/avg\": \"0.7250\", \"train/hateful_memes/roc_auc\": \"0.8961\", \"train/hateful_memes/roc_auc/avg\": \"0.8840\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 3, \"num_updates\": 700, \"iterations\": 700, \"max_updates\": 22000, \"lr\": \"0.\", \"ups\": \"0.54\", \"time\": \"03m 06s 510ms\", \"time_since_start\": \"21m 54s 678ms\", \"eta\": \"11h 02m 06s 734ms\"}\n",
      "2020-06-09T08:55:25 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T08:58:29 INFO: {\"progress\": \"800/22000\", \"train/total_loss\": \"0.4071\", \"train/total_loss/avg\": \"0.4424\", \"train/hateful_memes/cross_entropy\": \"0.4071\", \"train/hateful_memes/cross_entropy/avg\": \"0.4424\", \"train/hateful_memes/accuracy\": \"0.7812\", \"train/hateful_memes/accuracy/avg\": \"0.7969\", \"train/hateful_memes/binary_f1\": \"0.7000\", \"train/hateful_memes/binary_f1/avg\": \"0.7058\", \"train/hateful_memes/roc_auc\": \"0.8867\", \"train/hateful_memes/roc_auc/avg\": \"0.8792\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 4, \"num_updates\": 800, \"iterations\": 800, \"max_updates\": 22000, \"lr\": \"0.\", \"ups\": \"0.54\", \"time\": \"03m 06s 606ms\", \"time_since_start\": \"25m 01s 285ms\", \"eta\": \"10h 59m 20s 683ms\"}\n",
      "2020-06-09T08:58:31 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:01:36 INFO: {\"progress\": \"900/22000\", \"train/total_loss\": \"0.4071\", \"train/total_loss/avg\": \"0.4279\", \"train/hateful_memes/cross_entropy\": \"0.4071\", \"train/hateful_memes/cross_entropy/avg\": \"0.4279\", \"train/hateful_memes/accuracy\": \"0.7812\", \"train/hateful_memes/accuracy/avg\": \"0.7986\", \"train/hateful_memes/binary_f1\": \"0.7143\", \"train/hateful_memes/binary_f1/avg\": \"0.7129\", \"train/hateful_memes/roc_auc\": \"0.8961\", \"train/hateful_memes/roc_auc/avg\": \"0.8882\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 4, \"num_updates\": 900, \"iterations\": 900, \"max_updates\": 22000, \"lr\": \"0.\", \"ups\": \"0.54\", \"time\": \"03m 06s 483ms\", \"time_since_start\": \"28m 07s 769ms\", \"eta\": \"10h 55m 48s 023ms\"}\n",
      "2020-06-09T09:01:38 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:04:42 INFO: {\"progress\": \"1000/22000\", \"train/total_loss\": \"0.3966\", \"train/total_loss/avg\": \"0.4142\", \"train/hateful_memes/cross_entropy\": \"0.3966\", \"train/hateful_memes/cross_entropy/avg\": \"0.4142\", \"train/hateful_memes/accuracy\": \"0.7812\", \"train/hateful_memes/accuracy/avg\": \"0.8094\", \"train/hateful_memes/binary_f1\": \"0.7143\", \"train/hateful_memes/binary_f1/avg\": \"0.7285\", \"train/hateful_memes/roc_auc\": \"0.8961\", \"train/hateful_memes/roc_auc/avg\": \"0.8944\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 4, \"num_updates\": 1000, \"iterations\": 1000, \"max_updates\": 22000, \"lr\": \"0.\", \"ups\": \"0.54\", \"time\": \"03m 06s 276ms\", \"time_since_start\": \"31m 14s 045ms\", \"eta\": \"10h 51m 57s 996ms\"}\n",
      "2020-06-09T09:04:42 INFO: Checkpoint time. Saving a checkpoint.\n",
      "2020-06-09T09:04:47 INFO: Evaluation time. Running on full validation set...\n",
      "2020-06-09T09:06:06 INFO: {\"progress\": \"1000/22000\", \"val/total_loss\": \"0.9417\", \"val/hateful_memes/cross_entropy\": \"0.9417\", \"val/hateful_memes/accuracy\": \"0.5680\", \"val/hateful_memes/binary_f1\": \"0.3864\", \"val/hateful_memes/roc_auc\": \"0.6310\", \"num_updates\": 1000, \"epoch\": 4, \"iterations\": 1000, \"max_updates\": 22000, \"val_time\": \"40s 490ms\", \"best_update\": 1000, \"best_iteration\": 1000, \"best_val/hateful_memes/roc_auc\": \"0.630992\"}\n",
      "2020-06-09T09:06:08 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:09:13 INFO: {\"progress\": \"1100/22000\", \"train/total_loss\": \"0.3966\", \"train/total_loss/avg\": \"0.4098\", \"train/hateful_memes/cross_entropy\": \"0.3966\", \"train/hateful_memes/cross_entropy/avg\": \"0.4098\", \"train/hateful_memes/accuracy\": \"0.8125\", \"train/hateful_memes/accuracy/avg\": \"0.8097\", \"train/hateful_memes/binary_f1\": \"0.7143\", \"train/hateful_memes/binary_f1/avg\": \"0.7259\", \"train/hateful_memes/roc_auc\": \"0.9004\", \"train/hateful_memes/roc_auc/avg\": \"0.8983\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 5, \"num_updates\": 1100, \"iterations\": 1100, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.53\", \"time\": \"03m 07s 086ms\", \"time_since_start\": \"35m 45s 072ms\", \"eta\": \"10h 51m 40s 998ms\"}\n",
      "2020-06-09T09:09:15 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:12:19 INFO: {\"progress\": \"1200/22000\", \"train/total_loss\": \"0.3722\", \"train/total_loss/avg\": \"0.4019\", \"train/hateful_memes/cross_entropy\": \"0.3722\", \"train/hateful_memes/cross_entropy/avg\": \"0.4019\", \"train/hateful_memes/accuracy\": \"0.8125\", \"train/hateful_memes/accuracy/avg\": \"0.8125\", \"train/hateful_memes/binary_f1\": \"0.7143\", \"train/hateful_memes/binary_f1/avg\": \"0.7289\", \"train/hateful_memes/roc_auc\": \"0.9004\", \"train/hateful_memes/roc_auc/avg\": \"0.9023\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 5, \"num_updates\": 1200, \"iterations\": 1200, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 229ms\", \"time_since_start\": \"38m 51s 301ms\", \"eta\": \"10h 45m 35s 663ms\"}\n",
      "2020-06-09T09:12:21 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:15:26 INFO: {\"progress\": \"1300/22000\", \"train/total_loss\": \"0.3722\", \"train/total_loss/avg\": \"0.3989\", \"train/hateful_memes/cross_entropy\": \"0.3722\", \"train/hateful_memes/cross_entropy/avg\": \"0.3989\", \"train/hateful_memes/accuracy\": \"0.8125\", \"train/hateful_memes/accuracy/avg\": \"0.8197\", \"train/hateful_memes/binary_f1\": \"0.7200\", \"train/hateful_memes/binary_f1/avg\": \"0.7388\", \"train/hateful_memes/roc_auc\": \"0.9091\", \"train/hateful_memes/roc_auc/avg\": \"0.9029\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 5, \"num_updates\": 1300, \"iterations\": 1300, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 655ms\", \"time_since_start\": \"41m 57s 957ms\", \"eta\": \"10h 43m 57s 750ms\"}\n",
      "2020-06-09T09:15:28 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:18:33 INFO: {\"progress\": \"1400/22000\", \"train/total_loss\": \"0.3663\", \"train/total_loss/avg\": \"0.3868\", \"train/hateful_memes/cross_entropy\": \"0.3663\", \"train/hateful_memes/cross_entropy/avg\": \"0.3868\", \"train/hateful_memes/accuracy\": \"0.8125\", \"train/hateful_memes/accuracy/avg\": \"0.8259\", \"train/hateful_memes/binary_f1\": \"0.7200\", \"train/hateful_memes/binary_f1/avg\": \"0.7489\", \"train/hateful_memes/roc_auc\": \"0.9091\", \"train/hateful_memes/roc_auc/avg\": \"0.9086\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 6, \"num_updates\": 1400, \"iterations\": 1400, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 685ms\", \"time_since_start\": \"45m 04s 642ms\", \"eta\": \"10h 40m 57s 119ms\"}\n",
      "2020-06-09T09:18:35 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:21:39 INFO: {\"progress\": \"1500/22000\", \"train/total_loss\": \"0.3663\", \"train/total_loss/avg\": \"0.3826\", \"train/hateful_memes/cross_entropy\": \"0.3663\", \"train/hateful_memes/cross_entropy/avg\": \"0.3826\", \"train/hateful_memes/accuracy\": \"0.8125\", \"train/hateful_memes/accuracy/avg\": \"0.8292\", \"train/hateful_memes/binary_f1\": \"0.7407\", \"train/hateful_memes/binary_f1/avg\": \"0.7561\", \"train/hateful_memes/roc_auc\": \"0.9258\", \"train/hateful_memes/roc_auc/avg\": \"0.9114\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 6, \"num_updates\": 1500, \"iterations\": 1500, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 566ms\", \"time_since_start\": \"48m 11s 209ms\", \"eta\": \"10h 37m 26s 210ms\"}\n",
      "2020-06-09T09:21:41 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:24:46 INFO: {\"progress\": \"1600/22000\", \"train/total_loss\": \"0.3627\", \"train/total_loss/avg\": \"0.3663\", \"train/hateful_memes/cross_entropy\": \"0.3627\", \"train/hateful_memes/cross_entropy/avg\": \"0.3663\", \"train/hateful_memes/accuracy\": \"0.8125\", \"train/hateful_memes/accuracy/avg\": \"0.8379\", \"train/hateful_memes/binary_f1\": \"0.7407\", \"train/hateful_memes/binary_f1/avg\": \"0.7688\", \"train/hateful_memes/roc_auc\": \"0.9258\", \"train/hateful_memes/roc_auc/avg\": \"0.9169\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 7, \"num_updates\": 1600, \"iterations\": 1600, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 618ms\", \"time_since_start\": \"51m 17s 828ms\", \"eta\": \"10h 34m 30s 250ms\"}\n",
      "2020-06-09T09:24:48 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:27:53 INFO: {\"progress\": \"1700/22000\", \"train/total_loss\": \"0.3627\", \"train/total_loss/avg\": \"0.3513\", \"train/hateful_memes/cross_entropy\": \"0.3627\", \"train/hateful_memes/cross_entropy/avg\": \"0.3513\", \"train/hateful_memes/accuracy\": \"0.8125\", \"train/hateful_memes/accuracy/avg\": \"0.8456\", \"train/hateful_memes/binary_f1\": \"0.7619\", \"train/hateful_memes/binary_f1/avg\": \"0.7803\", \"train/hateful_memes/roc_auc\": \"0.9375\", \"train/hateful_memes/roc_auc/avg\": \"0.9218\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 7, \"num_updates\": 1700, \"iterations\": 1700, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 635ms\", \"time_since_start\": \"54m 24s 464ms\", \"eta\": \"10h 31m 27s 080ms\"}\n",
      "2020-06-09T09:27:54 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:30:59 INFO: {\"progress\": \"1800/22000\", \"train/total_loss\": \"0.3235\", \"train/total_loss/avg\": \"0.3382\", \"train/hateful_memes/cross_entropy\": \"0.3235\", \"train/hateful_memes/cross_entropy/avg\": \"0.3382\", \"train/hateful_memes/accuracy\": \"0.8125\", \"train/hateful_memes/accuracy/avg\": \"0.8524\", \"train/hateful_memes/binary_f1\": \"0.7619\", \"train/hateful_memes/binary_f1/avg\": \"0.7904\", \"train/hateful_memes/roc_auc\": \"0.9375\", \"train/hateful_memes/roc_auc/avg\": \"0.9261\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 7, \"num_updates\": 1800, \"iterations\": 1800, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 443ms\", \"time_since_start\": \"57m 30s 907ms\", \"eta\": \"10h 27m 41s 514ms\"}\n",
      "2020-06-09T09:31:01 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:34:06 INFO: {\"progress\": \"1900/22000\", \"train/total_loss\": \"0.3235\", \"train/total_loss/avg\": \"0.3232\", \"train/hateful_memes/cross_entropy\": \"0.3235\", \"train/hateful_memes/cross_entropy/avg\": \"0.3232\", \"train/hateful_memes/accuracy\": \"0.8438\", \"train/hateful_memes/accuracy/avg\": \"0.8602\", \"train/hateful_memes/binary_f1\": \"0.7692\", \"train/hateful_memes/binary_f1/avg\": \"0.8014\", \"train/hateful_memes/roc_auc\": \"0.9469\", \"train/hateful_memes/roc_auc/avg\": \"0.9300\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 8, \"num_updates\": 1900, \"iterations\": 1900, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 785ms\", \"time_since_start\": \"01h 37s 692ms\", \"eta\": \"10h 25m 43s 819ms\"}\n",
      "2020-06-09T09:34:08 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:37:12 INFO: {\"progress\": \"2000/22000\", \"train/total_loss\": \"0.3150\", \"train/total_loss/avg\": \"0.3121\", \"train/hateful_memes/cross_entropy\": \"0.3150\", \"train/hateful_memes/cross_entropy/avg\": \"0.3121\", \"train/hateful_memes/accuracy\": \"0.8438\", \"train/hateful_memes/accuracy/avg\": \"0.8641\", \"train/hateful_memes/binary_f1\": \"0.7692\", \"train/hateful_memes/binary_f1/avg\": \"0.8078\", \"train/hateful_memes/roc_auc\": \"0.9469\", \"train/hateful_memes/roc_auc/avg\": \"0.9333\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 8, \"num_updates\": 2000, \"iterations\": 2000, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 635ms\", \"time_since_start\": \"01h 03m 44s 328ms\", \"eta\": \"10h 22m 07s 179ms\"}\n",
      "2020-06-09T09:37:12 INFO: Checkpoint time. Saving a checkpoint.\n",
      "2020-06-09T09:37:32 INFO: Evaluation time. Running on full validation set...\n",
      "2020-06-09T09:38:20 INFO: {\"progress\": \"2000/22000\", \"val/total_loss\": \"1.4182\", \"val/hateful_memes/cross_entropy\": \"1.4182\", \"val/hateful_memes/accuracy\": \"0.5540\", \"val/hateful_memes/binary_f1\": \"0.3754\", \"val/hateful_memes/roc_auc\": \"0.6013\", \"num_updates\": 2000, \"epoch\": 8, \"iterations\": 2000, \"max_updates\": 22000, \"val_time\": \"11s 004ms\", \"best_update\": 1000, \"best_iteration\": 1000, \"best_val/hateful_memes/roc_auc\": \"0.630992\"}\n",
      "2020-06-09T09:38:22 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:41:27 INFO: {\"progress\": \"2100/22000\", \"train/total_loss\": \"0.3114\", \"train/total_loss/avg\": \"0.3016\", \"train/hateful_memes/cross_entropy\": \"0.3114\", \"train/hateful_memes/cross_entropy/avg\": \"0.3016\", \"train/hateful_memes/accuracy\": \"0.8750\", \"train/hateful_memes/accuracy/avg\": \"0.8690\", \"train/hateful_memes/binary_f1\": \"0.8571\", \"train/hateful_memes/binary_f1/avg\": \"0.8141\", \"train/hateful_memes/roc_auc\": \"0.9492\", \"train/hateful_memes/roc_auc/avg\": \"0.9365\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 8, \"num_updates\": 2100, \"iterations\": 2100, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 727ms\", \"time_since_start\": \"01h 07m 59s 100ms\", \"eta\": \"10h 19m 18s 843ms\"}\n",
      "2020-06-09T09:41:29 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:44:34 INFO: {\"progress\": \"2200/22000\", \"train/total_loss\": \"0.2912\", \"train/total_loss/avg\": \"0.2901\", \"train/hateful_memes/cross_entropy\": \"0.2912\", \"train/hateful_memes/cross_entropy/avg\": \"0.2901\", \"train/hateful_memes/accuracy\": \"0.9062\", \"train/hateful_memes/accuracy/avg\": \"0.8750\", \"train/hateful_memes/binary_f1\": \"0.8571\", \"train/hateful_memes/binary_f1/avg\": \"0.8226\", \"train/hateful_memes/roc_auc\": \"0.9500\", \"train/hateful_memes/roc_auc/avg\": \"0.9394\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 9, \"num_updates\": 2200, \"iterations\": 2200, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 771ms\", \"time_since_start\": \"01h 11m 05s 871ms\", \"eta\": \"10h 16m 20s 806ms\"}\n",
      "2020-06-09T09:44:36 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:47:41 INFO: {\"progress\": \"2300/22000\", \"train/total_loss\": \"0.2579\", \"train/total_loss/avg\": \"0.2788\", \"train/hateful_memes/cross_entropy\": \"0.2579\", \"train/hateful_memes/cross_entropy/avg\": \"0.2788\", \"train/hateful_memes/accuracy\": \"0.9062\", \"train/hateful_memes/accuracy/avg\": \"0.8804\", \"train/hateful_memes/binary_f1\": \"0.8696\", \"train/hateful_memes/binary_f1/avg\": \"0.8303\", \"train/hateful_memes/roc_auc\": \"0.9603\", \"train/hateful_memes/roc_auc/avg\": \"0.9420\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 9, \"num_updates\": 2300, \"iterations\": 2300, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 624ms\", \"time_since_start\": \"01h 14m 12s 496ms\", \"eta\": \"10h 12m 45s 000ms\"}\n",
      "2020-06-09T09:47:43 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:50:47 INFO: {\"progress\": \"2400/22000\", \"train/total_loss\": \"0.2292\", \"train/total_loss/avg\": \"0.2687\", \"train/hateful_memes/cross_entropy\": \"0.2292\", \"train/hateful_memes/cross_entropy/avg\": \"0.2687\", \"train/hateful_memes/accuracy\": \"0.9062\", \"train/hateful_memes/accuracy/avg\": \"0.8854\", \"train/hateful_memes/binary_f1\": \"0.8800\", \"train/hateful_memes/binary_f1/avg\": \"0.8374\", \"train/hateful_memes/roc_auc\": \"0.9838\", \"train/hateful_memes/roc_auc/avg\": \"0.9444\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 10, \"num_updates\": 2400, \"iterations\": 2400, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 684ms\", \"time_since_start\": \"01h 17m 19s 180ms\", \"eta\": \"10h 09m 50s 133ms\"}\n",
      "2020-06-09T09:50:49 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:53:54 INFO: {\"progress\": \"2500/22000\", \"train/total_loss\": \"0.1219\", \"train/total_loss/avg\": \"0.2586\", \"train/hateful_memes/cross_entropy\": \"0.1219\", \"train/hateful_memes/cross_entropy/avg\": \"0.2586\", \"train/hateful_memes/accuracy\": \"0.9375\", \"train/hateful_memes/accuracy/avg\": \"0.8900\", \"train/hateful_memes/binary_f1\": \"0.9286\", \"train/hateful_memes/binary_f1/avg\": \"0.8439\", \"train/hateful_memes/roc_auc\": \"0.9960\", \"train/hateful_memes/roc_auc/avg\": \"0.9467\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 10, \"num_updates\": 2500, \"iterations\": 2500, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 567ms\", \"time_since_start\": \"01h 20m 25s 748ms\", \"eta\": \"10h 06m 20s 685ms\"}\n",
      "2020-06-09T09:53:56 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T09:57:00 INFO: {\"progress\": \"2600/22000\", \"train/total_loss\": \"0.1147\", \"train/total_loss/avg\": \"0.2498\", \"train/hateful_memes/cross_entropy\": \"0.1147\", \"train/hateful_memes/cross_entropy/avg\": \"0.2498\", \"train/hateful_memes/accuracy\": \"0.9375\", \"train/hateful_memes/accuracy/avg\": \"0.8942\", \"train/hateful_memes/binary_f1\": \"0.9286\", \"train/hateful_memes/binary_f1/avg\": \"0.8499\", \"train/hateful_memes/roc_auc\": \"0.9960\", \"train/hateful_memes/roc_auc/avg\": \"0.9487\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 10, \"num_updates\": 2600, \"iterations\": 2600, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 465ms\", \"time_since_start\": \"01h 23m 32s 214ms\", \"eta\": \"10h 02m 54s 336ms\"}\n",
      "2020-06-09T09:57:02 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:00:07 INFO: {\"progress\": \"2700/22000\", \"train/total_loss\": \"0.1117\", \"train/total_loss/avg\": \"0.2427\", \"train/hateful_memes/cross_entropy\": \"0.1117\", \"train/hateful_memes/cross_entropy/avg\": \"0.2427\", \"train/hateful_memes/accuracy\": \"0.9688\", \"train/hateful_memes/accuracy/avg\": \"0.8970\", \"train/hateful_memes/binary_f1\": \"0.9412\", \"train/hateful_memes/binary_f1/avg\": \"0.8541\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9506\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 11, \"num_updates\": 2700, \"iterations\": 2700, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 584ms\", \"time_since_start\": \"01h 26m 38s 798ms\", \"eta\": \"10h 10s 841ms\"}\n",
      "2020-06-09T10:00:09 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:03:13 INFO: {\"progress\": \"2800/22000\", \"train/total_loss\": \"0.1011\", \"train/total_loss/avg\": \"0.2349\", \"train/hateful_memes/cross_entropy\": \"0.1011\", \"train/hateful_memes/cross_entropy/avg\": \"0.2349\", \"train/hateful_memes/accuracy\": \"0.9688\", \"train/hateful_memes/accuracy/avg\": \"0.9007\", \"train/hateful_memes/binary_f1\": \"0.9600\", \"train/hateful_memes/binary_f1/avg\": \"0.8593\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9524\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 11, \"num_updates\": 2800, \"iterations\": 2800, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 477ms\", \"time_since_start\": \"01h 29m 45s 276ms\", \"eta\": \"09h 56m 43s 744ms\"}\n",
      "2020-06-09T10:03:15 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:06:20 INFO: {\"progress\": \"2900/22000\", \"train/total_loss\": \"0.0910\", \"train/total_loss/avg\": \"0.2278\", \"train/hateful_memes/cross_entropy\": \"0.0910\", \"train/hateful_memes/cross_entropy/avg\": \"0.2278\", \"train/hateful_memes/accuracy\": \"0.9688\", \"train/hateful_memes/accuracy/avg\": \"0.9041\", \"train/hateful_memes/binary_f1\": \"0.9630\", \"train/hateful_memes/binary_f1/avg\": \"0.8641\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9540\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 11, \"num_updates\": 2900, \"iterations\": 2900, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 541ms\", \"time_since_start\": \"01h 32m 51s 818ms\", \"eta\": \"09h 53m 49s 511ms\"}\n",
      "2020-06-09T10:06:22 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:09:27 INFO: {\"progress\": \"3000/22000\", \"train/total_loss\": \"0.0578\", \"train/total_loss/avg\": \"0.2220\", \"train/hateful_memes/cross_entropy\": \"0.0578\", \"train/hateful_memes/cross_entropy/avg\": \"0.2220\", \"train/hateful_memes/accuracy\": \"0.9688\", \"train/hateful_memes/accuracy/avg\": \"0.9062\", \"train/hateful_memes/binary_f1\": \"0.9630\", \"train/hateful_memes/binary_f1/avg\": \"0.8667\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9555\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 12, \"num_updates\": 3000, \"iterations\": 3000, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 854ms\", \"time_since_start\": \"01h 35m 58s 672ms\", \"eta\": \"09h 51m 42s 308ms\"}\n",
      "2020-06-09T10:09:27 INFO: Checkpoint time. Saving a checkpoint.\n",
      "2020-06-09T10:09:46 INFO: Evaluation time. Running on full validation set...\n",
      "2020-06-09T10:10:49 INFO: {\"progress\": \"3000/22000\", \"val/total_loss\": \"1.4681\", \"val/hateful_memes/cross_entropy\": \"1.4681\", \"val/hateful_memes/accuracy\": \"0.5700\", \"val/hateful_memes/binary_f1\": \"0.4044\", \"val/hateful_memes/roc_auc\": \"0.6356\", \"num_updates\": 3000, \"epoch\": 12, \"iterations\": 3000, \"max_updates\": 22000, \"val_time\": \"11s 038ms\", \"best_update\": 3000, \"best_iteration\": 3000, \"best_val/hateful_memes/roc_auc\": \"0.635632\"}\n",
      "2020-06-09T10:10:52 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:13:57 INFO: {\"progress\": \"3100/22000\", \"train/total_loss\": \"0.0560\", \"train/total_loss/avg\": \"0.2151\", \"train/hateful_memes/cross_entropy\": \"0.0560\", \"train/hateful_memes/cross_entropy/avg\": \"0.2151\", \"train/hateful_memes/accuracy\": \"0.9688\", \"train/hateful_memes/accuracy/avg\": \"0.9093\", \"train/hateful_memes/binary_f1\": \"0.9630\", \"train/hateful_memes/binary_f1/avg\": \"0.8710\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9570\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 12, \"num_updates\": 3100, \"iterations\": 3100, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.53\", \"time\": \"03m 07s 017ms\", \"time_since_start\": \"01h 40m 28s 524ms\", \"eta\": \"09h 49m 06s 265ms\"}\n",
      "2020-06-09T10:13:59 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:17:03 INFO: {\"progress\": \"3200/22000\", \"train/total_loss\": \"0.0538\", \"train/total_loss/avg\": \"0.2092\", \"train/hateful_memes/cross_entropy\": \"0.0538\", \"train/hateful_memes/cross_entropy/avg\": \"0.2092\", \"train/hateful_memes/accuracy\": \"0.9688\", \"train/hateful_memes/accuracy/avg\": \"0.9121\", \"train/hateful_memes/binary_f1\": \"0.9630\", \"train/hateful_memes/binary_f1/avg\": \"0.8750\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9583\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 13, \"num_updates\": 3200, \"iterations\": 3200, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 403ms\", \"time_since_start\": \"01h 43m 34s 928ms\", \"eta\": \"09h 44m 03s 915ms\"}\n",
      "2020-06-09T10:17:05 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:20:10 INFO: {\"progress\": \"3300/22000\", \"train/total_loss\": \"0.0491\", \"train/total_loss/avg\": \"0.2043\", \"train/hateful_memes/cross_entropy\": \"0.0491\", \"train/hateful_memes/cross_entropy/avg\": \"0.2043\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9148\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.8788\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9596\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 13, \"num_updates\": 3300, \"iterations\": 3300, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 674ms\", \"time_since_start\": \"01h 46m 41s 602ms\", \"eta\": \"09h 41m 48s 164ms\"}\n",
      "2020-06-09T10:20:12 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:23:16 INFO: {\"progress\": \"3400/22000\", \"train/total_loss\": \"0.0475\", \"train/total_loss/avg\": \"0.1993\", \"train/hateful_memes/cross_entropy\": \"0.0475\", \"train/hateful_memes/cross_entropy/avg\": \"0.1993\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9173\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.8824\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9608\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 13, \"num_updates\": 3400, \"iterations\": 3400, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 569ms\", \"time_since_start\": \"01h 49m 48s 172ms\", \"eta\": \"09h 38m 22s 016ms\"}\n",
      "2020-06-09T10:23:18 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:26:23 INFO: {\"progress\": \"3500/22000\", \"train/total_loss\": \"0.0371\", \"train/total_loss/avg\": \"0.1943\", \"train/hateful_memes/cross_entropy\": \"0.0371\", \"train/hateful_memes/cross_entropy/avg\": \"0.1943\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9196\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.8858\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9619\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 14, \"num_updates\": 3500, \"iterations\": 3500, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 436ms\", \"time_since_start\": \"01h 52m 54s 609ms\", \"eta\": \"09h 34m 50s 743ms\"}\n",
      "2020-06-09T10:26:25 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:29:29 INFO: {\"progress\": \"3600/22000\", \"train/total_loss\": \"0.0363\", \"train/total_loss/avg\": \"0.1892\", \"train/hateful_memes/cross_entropy\": \"0.0363\", \"train/hateful_memes/cross_entropy/avg\": \"0.1892\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9219\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.8889\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9630\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 14, \"num_updates\": 3600, \"iterations\": 3600, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 427ms\", \"time_since_start\": \"01h 56m 01s 036ms\", \"eta\": \"09h 31m 42s 579ms\"}\n",
      "2020-06-09T10:29:31 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:32:35 INFO: {\"progress\": \"3700/22000\", \"train/total_loss\": \"0.0300\", \"train/total_loss/avg\": \"0.1846\", \"train/hateful_memes/cross_entropy\": \"0.0300\", \"train/hateful_memes/cross_entropy/avg\": \"0.1846\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9240\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.8919\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9640\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 14, \"num_updates\": 3700, \"iterations\": 3700, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 285ms\", \"time_since_start\": \"01h 59m 07s 321ms\", \"eta\": \"09h 28m 10s 176ms\"}\n",
      "2020-06-09T10:32:37 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:35:42 INFO: {\"progress\": \"3800/22000\", \"train/total_loss\": \"0.0292\", \"train/total_loss/avg\": \"0.1801\", \"train/hateful_memes/cross_entropy\": \"0.0292\", \"train/hateful_memes/cross_entropy/avg\": \"0.1801\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9260\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.8948\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9649\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 15, \"num_updates\": 3800, \"iterations\": 3800, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 825ms\", \"time_since_start\": \"02h 02m 14s 147ms\", \"eta\": \"09h 26m 42s 222ms\"}\n",
      "2020-06-09T10:35:44 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:38:49 INFO: {\"progress\": \"3900/22000\", \"train/total_loss\": \"0.0292\", \"train/total_loss/avg\": \"0.1788\", \"train/hateful_memes/cross_entropy\": \"0.0292\", \"train/hateful_memes/cross_entropy/avg\": \"0.1788\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9255\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.8916\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9658\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 15, \"num_updates\": 3900, \"iterations\": 3900, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 348ms\", \"time_since_start\": \"02h 05m 20s 495ms\", \"eta\": \"09h 22m 09s 102ms\"}\n",
      "2020-06-09T10:38:51 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:41:55 INFO: {\"progress\": \"4000/22000\", \"train/total_loss\": \"0.0287\", \"train/total_loss/avg\": \"0.1745\", \"train/hateful_memes/cross_entropy\": \"0.0287\", \"train/hateful_memes/cross_entropy/avg\": \"0.1745\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9273\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.8943\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9667\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 16, \"num_updates\": 4000, \"iterations\": 4000, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 577ms\", \"time_since_start\": \"02h 08m 27s 073ms\", \"eta\": \"09h 19m 43s 952ms\"}\n",
      "2020-06-09T10:41:55 INFO: Checkpoint time. Saving a checkpoint.\n",
      "2020-06-09T10:42:15 INFO: Evaluation time. Running on full validation set...\n",
      "2020-06-09T10:43:00 INFO: {\"progress\": \"4000/22000\", \"val/total_loss\": \"1.9630\", \"val/hateful_memes/cross_entropy\": \"1.9630\", \"val/hateful_memes/accuracy\": \"0.5800\", \"val/hateful_memes/binary_f1\": \"0.3824\", \"val/hateful_memes/roc_auc\": \"0.6311\", \"num_updates\": 4000, \"epoch\": 16, \"iterations\": 4000, \"max_updates\": 22000, \"val_time\": \"10s 718ms\", \"best_update\": 3000, \"best_iteration\": 3000, \"best_val/hateful_memes/roc_auc\": \"0.635632\"}\n",
      "2020-06-09T10:43:02 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:46:08 INFO: {\"progress\": \"4100/22000\", \"train/total_loss\": \"0.0239\", \"train/total_loss/avg\": \"0.1703\", \"train/hateful_memes/cross_entropy\": \"0.0239\", \"train/hateful_memes/cross_entropy/avg\": \"0.1703\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9291\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.8968\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9675\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 16, \"num_updates\": 4100, \"iterations\": 4100, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 976ms\", \"time_since_start\": \"02h 12m 39s 430ms\", \"eta\": \"09h 17m 48s 877ms\"}\n",
      "2020-06-09T10:46:09 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:49:14 INFO: {\"progress\": \"4200/22000\", \"train/total_loss\": \"0.0235\", \"train/total_loss/avg\": \"0.1667\", \"train/hateful_memes/cross_entropy\": \"0.0235\", \"train/hateful_memes/cross_entropy/avg\": \"0.1667\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9308\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.8993\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9682\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 16, \"num_updates\": 4200, \"iterations\": 4200, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 150ms\", \"time_since_start\": \"02h 15m 45s 580ms\", \"eta\": \"09h 12m 14s 772ms\"}\n",
      "2020-06-09T10:49:16 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:52:20 INFO: {\"progress\": \"4300/22000\", \"train/total_loss\": \"0.0235\", \"train/total_loss/avg\": \"0.1638\", \"train/hateful_memes/cross_entropy\": \"0.0235\", \"train/hateful_memes/cross_entropy/avg\": \"0.1638\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9317\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.9009\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9690\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 17, \"num_updates\": 4300, \"iterations\": 4300, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 758ms\", \"time_since_start\": \"02h 18m 52s 339ms\", \"eta\": \"09h 10m 56s 210ms\"}\n",
      "2020-06-09T10:52:22 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:55:27 INFO: {\"progress\": \"4400/22000\", \"train/total_loss\": \"0.0228\", \"train/total_loss/avg\": \"0.1603\", \"train/hateful_memes/cross_entropy\": \"0.0228\", \"train/hateful_memes/cross_entropy/avg\": \"0.1603\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9332\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.9031\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9697\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 17, \"num_updates\": 4400, \"iterations\": 4400, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 406ms\", \"time_since_start\": \"02h 21m 58s 745ms\", \"eta\": \"09h 06m 47s 580ms\"}\n",
      "2020-06-09T10:55:29 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T10:58:33 INFO: {\"progress\": \"4500/22000\", \"train/total_loss\": \"0.0235\", \"train/total_loss/avg\": \"0.1578\", \"train/hateful_memes/cross_entropy\": \"0.0235\", \"train/hateful_memes/cross_entropy/avg\": \"0.1578\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9340\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.9040\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9704\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 17, \"num_updates\": 4500, \"iterations\": 4500, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 382ms\", \"time_since_start\": \"02h 25m 05s 128ms\", \"eta\": \"09h 03m 37s 027ms\"}\n",
      "2020-06-09T10:58:35 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "2020-06-09T11:01:40 INFO: {\"progress\": \"4600/22000\", \"train/total_loss\": \"0.0235\", \"train/total_loss/avg\": \"0.1553\", \"train/hateful_memes/cross_entropy\": \"0.0235\", \"train/hateful_memes/cross_entropy/avg\": \"0.1553\", \"train/hateful_memes/accuracy\": \"1.0000\", \"train/hateful_memes/accuracy/avg\": \"0.9348\", \"train/hateful_memes/binary_f1\": \"1.0000\", \"train/hateful_memes/binary_f1/avg\": \"0.9053\", \"train/hateful_memes/roc_auc\": \"1.0000\", \"train/hateful_memes/roc_auc/avg\": \"0.9710\", \"max mem\": 12643.0, \"experiment\": \"run\", \"epoch\": 18, \"num_updates\": 4600, \"iterations\": 4600, \"max_updates\": 22000, \"lr\": \"0.00001\", \"ups\": \"0.54\", \"time\": \"03m 06s 600ms\", \"time_since_start\": \"02h 28m 11s 729ms\", \"eta\": \"09h 01m 08s 492ms\"}\n",
      "2020-06-09T11:01:42 WARNING: /opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/concat_bert\" mmf_run config=configs/concat_bert/defaults_custom.yaml model=concat_bert dataset=hateful_memes run_type=train checkpoint.resume_zoo=concat_bert.hateful_memes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/late_fusion\" mmf_run config=configs/late_fusion/defaults_custom.yaml model=late_fusion dataset=hateful_memes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !MMF_SAVE_DIR=\"/home/jupyter/meme_hateful_detection/save/concat_bert\" mmf_run config=configs/concat_bert/defaults_custom.yaml model=concat_bert dataset=hateful_memes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions val\n",
    "for index, row in df_model_config.iterrows():\n",
    "    model_baseline = row['baseline']\n",
    "    model_key = row['model_key']\n",
    "    model_pretrained_key = row['pretrained_key']\n",
    "    baseline_config = row['baseline_config']\n",
    "    custom_config = row['custom_config']\n",
    "    str_pred_eval = f'mmf_predict config={custom_config} model={model_key} dataset=hateful_memes run_type=train --evalai_inference=1 --resume_file={PATH_SAVE}/model_key.pth'\n",
    "#     !{str_pred_eval}\n",
    "    print(str_pred_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mmf_predict config=configs/unimodal/image_custom.yaml model=unimodal_image dataset=hateful_memes run_type=train evalai_inference=1 resume_file=/home/jupyter/meme_hateful_detection/save/unimodal_image_grid.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/unimodal/image.yaml\n",
      "Overriding option model to unimodal_image\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to unimodal_image.hateful_memes.images\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 13305222\n",
      "Logging to: /home/jupyter/meme_hateful_detection/logs/train_2020-06-09T08:18:13.log\n",
      "2020-06-09T08:18:13 INFO: =====  Training Parameters    =====\n",
      "2020-06-09T08:18:13 INFO: {\n",
      "    \"batch_size\": 32,\n",
      "    \"checkpoint_interval\": 1000,\n",
      "    \"clip_gradients\": false,\n",
      "    \"clip_norm_mode\": \"all\",\n",
      "    \"dataset_size_proportional_sampling\": true,\n",
      "    \"device\": \"cuda\",\n",
      "    \"early_stop\": {\n",
      "        \"criteria\": \"hateful_memes/roc_auc\",\n",
      "        \"enabled\": false,\n",
      "        \"minimize\": false,\n",
      "        \"patience\": 4000\n",
      "    },\n",
      "    \"evaluate_metrics\": true,\n",
      "    \"evaluation_interval\": 1000,\n",
      "    \"experiment_name\": \"run\",\n",
      "    \"fast_read\": false,\n",
      "    \"find_unused_parameters\": false,\n",
      "    \"local_rank\": null,\n",
      "    \"log_detailed_config\": true,\n",
      "    \"log_format\": \"json\",\n",
      "    \"log_interval\": 100,\n",
      "    \"logger_level\": \"info\",\n",
      "    \"lr_ratio\": 0.1,\n",
      "    \"lr_scheduler\": true,\n",
      "    \"lr_steps\": [],\n",
      "    \"max_epochs\": null,\n",
      "    \"max_updates\": 22000,\n",
      "    \"num_workers\": 4,\n",
      "    \"pin_memory\": false,\n",
      "    \"seed\": 13305222,\n",
      "    \"should_not_log\": false,\n",
      "    \"tensorboard\": false,\n",
      "    \"trainer\": \"base_trainer\",\n",
      "    \"use_warmup\": false,\n",
      "    \"verbose_dump\": false,\n",
      "    \"warmup_factor\": 0.2,\n",
      "    \"warmup_iterations\": 1000\n",
      "}\n",
      "2020-06-09T08:18:13 INFO: ======  Dataset Attributes  ======\n",
      "2020-06-09T08:18:13 INFO: ======== hateful_memes =======\n",
      "2020-06-09T08:18:13 INFO: {\n",
      "    \"annotations\": {\n",
      "        \"test\": [\n",
      "            \"hateful_memes/defaults/annotations/test.jsonl\"\n",
      "        ],\n",
      "        \"train\": [\n",
      "            \"hateful_memes/defaults/annotations/train.jsonl\"\n",
      "        ],\n",
      "        \"val\": [\n",
      "            \"hateful_memes/defaults/annotations/dev.jsonl\"\n",
      "        ]\n",
      "    },\n",
      "    \"data_dir\": \"/home/jupyter/meme_hateful_detection/data/raw/datasets\",\n",
      "    \"depth_first\": false,\n",
      "    \"fast_read\": false,\n",
      "    \"features\": {\n",
      "        \"test\": [\n",
      "            \"hateful_memes/defaults/features/detectron.lmdb\"\n",
      "        ],\n",
      "        \"train\": [\n",
      "            \"hateful_memes/defaults/features/detectron.lmdb\"\n",
      "        ],\n",
      "        \"val\": [\n",
      "            \"hateful_memes/defaults/features/detectron.lmdb\"\n",
      "        ]\n",
      "    },\n",
      "    \"images\": {\n",
      "        \"test\": [\n",
      "            \"hateful_memes/defaults/images/\"\n",
      "        ],\n",
      "        \"train\": [\n",
      "            \"hateful_memes/defaults/images/\"\n",
      "        ],\n",
      "        \"val\": [\n",
      "            \"hateful_memes/defaults/images/\"\n",
      "        ]\n",
      "    },\n",
      "    \"max_features\": 100,\n",
      "    \"processors\": {\n",
      "        \"bbox_processor\": {\n",
      "            \"params\": {\n",
      "                \"max_length\": 50\n",
      "            },\n",
      "            \"type\": \"bbox\"\n",
      "        },\n",
      "        \"image_processor\": {\n",
      "            \"params\": {\n",
      "                \"transforms\": [\n",
      "                    {\n",
      "                        \"params\": {\n",
      "                            \"size\": [\n",
      "                                256,\n",
      "                                256\n",
      "                            ]\n",
      "                        },\n",
      "                        \"type\": \"Resize\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"params\": {\n",
      "                            \"size\": [\n",
      "                                224,\n",
      "                                224\n",
      "                            ]\n",
      "                        },\n",
      "                        \"type\": \"CenterCrop\"\n",
      "                    },\n",
      "                    \"ToTensor\",\n",
      "                    \"GrayScaleTo3Channels\",\n",
      "                    {\n",
      "                        \"params\": {\n",
      "                            \"mean\": [\n",
      "                                0.46777044,\n",
      "                                0.44531429,\n",
      "                                0.40661017\n",
      "                            ],\n",
      "                            \"std\": [\n",
      "                                0.12221994,\n",
      "                                0.12145835,\n",
      "                                0.14380469\n",
      "                            ]\n",
      "                        },\n",
      "                        \"type\": \"Normalize\"\n",
      "                    }\n",
      "                ]\n",
      "            },\n",
      "            \"type\": \"torchvision_transforms\"\n",
      "        },\n",
      "        \"text_processor\": {\n",
      "            \"params\": {\n",
      "                \"max_length\": 14,\n",
      "                \"preprocessor\": {\n",
      "                    \"params\": {},\n",
      "                    \"type\": \"simple_sentence\"\n",
      "                },\n",
      "                \"vocab\": {\n",
      "                    \"embedding_name\": \"glove.6B.300d\",\n",
      "                    \"type\": \"intersected\",\n",
      "                    \"vocab_file\": \"hateful_memes/defaults/extras/vocabs/vocabulary_100k.txt\"\n",
      "                }\n",
      "            },\n",
      "            \"type\": \"vocab\"\n",
      "        }\n",
      "    },\n",
      "    \"return_features_info\": false,\n",
      "    \"use_features\": false,\n",
      "    \"use_images\": true\n",
      "}\n",
      "2020-06-09T08:18:13 INFO: ======  Optimizer Attributes  ======\n",
      "2020-06-09T08:18:13 INFO: {\n",
      "    \"params\": {\n",
      "        \"eps\": 1e-08,\n",
      "        \"lr\": 1e-05\n",
      "    },\n",
      "    \"type\": \"adam_w\"\n",
      "}\n",
      "2020-06-09T08:18:13 INFO: ======  Model (unimodal_image) Attributes  ======\n",
      "2020-06-09T08:18:13 INFO: {\n",
      "    \"classifier\": {\n",
      "        \"params\": {\n",
      "            \"hidden_dim\": 768,\n",
      "            \"in_dim\": 2048,\n",
      "            \"num_layers\": 2,\n",
      "            \"out_dim\": 2\n",
      "        },\n",
      "        \"type\": \"mlp\"\n",
      "    },\n",
      "    \"direct_features_input\": false,\n",
      "    \"finetune_lr_multiplier\": 1,\n",
      "    \"freeze_base\": false,\n",
      "    \"losses\": [\n",
      "        {\n",
      "            \"type\": \"cross_entropy\"\n",
      "        }\n",
      "    ],\n",
      "    \"modal_encoder\": {\n",
      "        \"params\": {\n",
      "            \"num_output_features\": 1,\n",
      "            \"pool_type\": \"avg\",\n",
      "            \"pretrained\": true\n",
      "        },\n",
      "        \"type\": \"resnet152\"\n",
      "    },\n",
      "    \"modal_hidden_size\": 2048,\n",
      "    \"num_labels\": 2\n",
      "}\n",
      "2020-06-09T08:18:13 INFO: Loading datasets\n",
      "2020-06-09T08:18:22 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-06-09T08:18:26 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-06-09T08:18:26 INFO: Loading checkpoint\n",
      "2020-06-09T08:18:29 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-06-09T08:18:29 INFO: Checkpoint loaded\n",
      "2020-06-09T08:18:29 INFO: Starting test inference predictions\n",
      "2020-06-09T08:18:29 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 32/32 [00:15<00:00,  2.09it/s]\n",
      "2020-06-09T08:18:45 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/save/reports/hateful_memes_run_test_2020-06-09T08:18:45.csv\n",
      "2020-06-09T08:18:45 INFO: Finished predicting\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/unimodal/with_features.yaml\n",
      "Overriding option model to unimodal_image\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to unimodal_image.hateful_memes.features\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 51896021\n",
      "Logging to: /home/jupyter/meme_hateful_detection/logs/train_2020-06-09T08:18:51.log\n",
      "2020-06-09T08:18:51 INFO: =====  Training Parameters    =====\n",
      "2020-06-09T08:18:51 INFO: {\n",
      "    \"batch_size\": 32,\n",
      "    \"checkpoint_interval\": 1000,\n",
      "    \"clip_gradients\": false,\n",
      "    \"clip_norm_mode\": \"all\",\n",
      "    \"dataset_size_proportional_sampling\": true,\n",
      "    \"device\": \"cuda\",\n",
      "    \"early_stop\": {\n",
      "        \"criteria\": \"hateful_memes/roc_auc\",\n",
      "        \"enabled\": false,\n",
      "        \"minimize\": false,\n",
      "        \"patience\": 4000\n",
      "    },\n",
      "    \"evaluate_metrics\": true,\n",
      "    \"evaluation_interval\": 1000,\n",
      "    \"experiment_name\": \"run\",\n",
      "    \"fast_read\": false,\n",
      "    \"find_unused_parameters\": false,\n",
      "    \"local_rank\": null,\n",
      "    \"log_detailed_config\": true,\n",
      "    \"log_format\": \"json\",\n",
      "    \"log_interval\": 100,\n",
      "    \"logger_level\": \"info\",\n",
      "    \"lr_ratio\": 0.1,\n",
      "    \"lr_scheduler\": true,\n",
      "    \"lr_steps\": [],\n",
      "    \"max_epochs\": null,\n",
      "    \"max_updates\": 22000,\n",
      "    \"num_workers\": 4,\n",
      "    \"pin_memory\": false,\n",
      "    \"seed\": 51896021,\n",
      "    \"should_not_log\": false,\n",
      "    \"tensorboard\": false,\n",
      "    \"trainer\": \"base_trainer\",\n",
      "    \"use_warmup\": false,\n",
      "    \"verbose_dump\": false,\n",
      "    \"warmup_factor\": 0.2,\n",
      "    \"warmup_iterations\": 1000\n",
      "}\n",
      "2020-06-09T08:18:51 INFO: ======  Dataset Attributes  ======\n",
      "2020-06-09T08:18:51 INFO: ======== hateful_memes =======\n",
      "2020-06-09T08:18:51 INFO: {\n",
      "    \"annotations\": {\n",
      "        \"test\": [\n",
      "            \"hateful_memes/defaults/annotations/test.jsonl\"\n",
      "        ],\n",
      "        \"train\": [\n",
      "            \"hateful_memes/defaults/annotations/train.jsonl\"\n",
      "        ],\n",
      "        \"val\": [\n",
      "            \"hateful_memes/defaults/annotations/dev.jsonl\"\n",
      "        ]\n",
      "    },\n",
      "    \"data_dir\": \"/home/jupyter/meme_hateful_detection/data/raw/datasets\",\n",
      "    \"depth_first\": false,\n",
      "    \"fast_read\": false,\n",
      "    \"features\": {\n",
      "        \"test\": [\n",
      "            \"hateful_memes/defaults/features/detectron.lmdb\"\n",
      "        ],\n",
      "        \"train\": [\n",
      "            \"hateful_memes/defaults/features/detectron.lmdb\"\n",
      "        ],\n",
      "        \"val\": [\n",
      "            \"hateful_memes/defaults/features/detectron.lmdb\"\n",
      "        ]\n",
      "    },\n",
      "    \"images\": {\n",
      "        \"test\": [\n",
      "            \"hateful_memes/defaults/images/\"\n",
      "        ],\n",
      "        \"train\": [\n",
      "            \"hateful_memes/defaults/images/\"\n",
      "        ],\n",
      "        \"val\": [\n",
      "            \"hateful_memes/defaults/images/\"\n",
      "        ]\n",
      "    },\n",
      "    \"max_features\": 100,\n",
      "    \"processors\": {\n",
      "        \"bbox_processor\": {\n",
      "            \"params\": {\n",
      "                \"max_length\": 50\n",
      "            },\n",
      "            \"type\": \"bbox\"\n",
      "        },\n",
      "        \"image_processor\": {\n",
      "            \"params\": {\n",
      "                \"transforms\": [\n",
      "                    {\n",
      "                        \"params\": {\n",
      "                            \"size\": [\n",
      "                                256,\n",
      "                                256\n",
      "                            ]\n",
      "                        },\n",
      "                        \"type\": \"Resize\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"params\": {\n",
      "                            \"size\": [\n",
      "                                224,\n",
      "                                224\n",
      "                            ]\n",
      "                        },\n",
      "                        \"type\": \"CenterCrop\"\n",
      "                    },\n",
      "                    \"ToTensor\",\n",
      "                    \"GrayScaleTo3Channels\",\n",
      "                    {\n",
      "                        \"params\": {\n",
      "                            \"mean\": [\n",
      "                                0.46777044,\n",
      "                                0.44531429,\n",
      "                                0.40661017\n",
      "                            ],\n",
      "                            \"std\": [\n",
      "                                0.12221994,\n",
      "                                0.12145835,\n",
      "                                0.14380469\n",
      "                            ]\n",
      "                        },\n",
      "                        \"type\": \"Normalize\"\n",
      "                    }\n",
      "                ]\n",
      "            },\n",
      "            \"type\": \"torchvision_transforms\"\n",
      "        },\n",
      "        \"text_processor\": {\n",
      "            \"params\": {\n",
      "                \"max_length\": 14,\n",
      "                \"preprocessor\": {\n",
      "                    \"params\": {},\n",
      "                    \"type\": \"simple_sentence\"\n",
      "                },\n",
      "                \"vocab\": {\n",
      "                    \"embedding_name\": \"glove.6B.300d\",\n",
      "                    \"type\": \"intersected\",\n",
      "                    \"vocab_file\": \"hateful_memes/defaults/extras/vocabs/vocabulary_100k.txt\"\n",
      "                }\n",
      "            },\n",
      "            \"type\": \"vocab\"\n",
      "        }\n",
      "    },\n",
      "    \"return_features_info\": true,\n",
      "    \"use_features\": true,\n",
      "    \"use_images\": false\n",
      "}\n",
      "2020-06-09T08:18:51 INFO: ======  Optimizer Attributes  ======\n",
      "2020-06-09T08:18:51 INFO: {\n",
      "    \"params\": {\n",
      "        \"eps\": 1e-08,\n",
      "        \"lr\": 5e-05\n",
      "    },\n",
      "    \"type\": \"adam_w\"\n",
      "}\n",
      "2020-06-09T08:18:51 INFO: ======  Model (unimodal_image) Attributes  ======\n",
      "2020-06-09T08:18:51 INFO: {\n",
      "    \"classifier\": {\n",
      "        \"params\": {\n",
      "            \"hidden_dim\": 768,\n",
      "            \"in_dim\": 2048,\n",
      "            \"num_layers\": 2,\n",
      "            \"out_dim\": 2\n",
      "        },\n",
      "        \"type\": \"mlp\"\n",
      "    },\n",
      "    \"direct_features_input\": true,\n",
      "    \"finetune_lr_multiplier\": 1,\n",
      "    \"freeze_base\": false,\n",
      "    \"losses\": [\n",
      "        {\n",
      "            \"type\": \"cross_entropy\"\n",
      "        }\n",
      "    ],\n",
      "    \"modal_encoder\": {\n",
      "        \"params\": {\n",
      "            \"bias_file\": \"models/detectron.defaults/fc7_b.pkl\",\n",
      "            \"in_dim\": 2048,\n",
      "            \"model_data_dir\": \"/home/jupyter/meme_hateful_detection/data/raw\",\n",
      "            \"num_output_features\": 1,\n",
      "            \"pool_type\": \"avg\",\n",
      "            \"pretrained\": true,\n",
      "            \"weights_file\": \"models/detectron.defaults/fc7_w.pkl\"\n",
      "        },\n",
      "        \"type\": \"finetune_faster_rcnn_fpn_fc7\"\n",
      "    },\n",
      "    \"modal_hidden_size\": 2048,\n",
      "    \"model_data_dir\": \"/home/jupyter/meme_hateful_detection/data/raw\",\n",
      "    \"num_labels\": 2\n",
      "}\n",
      "2020-06-09T08:18:51 INFO: Loading datasets\n",
      "2020-06-09T08:18:54 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-06-09T08:18:58 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-06-09T08:18:58 INFO: Loading checkpoint\n",
      "2020-06-09T08:18:59 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-06-09T08:18:59 INFO: Checkpoint loaded\n",
      "2020-06-09T08:18:59 INFO: Starting test inference predictions\n",
      "2020-06-09T08:18:59 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 32/32 [00:55<00:00,  1.74s/it]\n",
      "2020-06-09T08:19:55 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/save/reports/hateful_memes_run_test_2020-06-09T08:19:55.csv\n",
      "2020-06-09T08:19:55 INFO: Finished predicting\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/unimodal/bert.yaml\n",
      "Overriding option model to unimodal_text\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to unimodal_text.hateful_memes.bert\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 214570\n",
      "Logging to: /home/jupyter/meme_hateful_detection/logs/train_2020-06-09T08:20:00.log\n",
      "2020-06-09T08:20:00 INFO: Loading datasets\n",
      "2020-06-09T08:20:07 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-06-09T08:20:11 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-06-09T08:20:11 INFO: Loading checkpoint\n",
      "2020-06-09T08:20:16 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-06-09T08:20:16 INFO: Checkpoint loaded\n",
      "2020-06-09T08:20:16 INFO: Starting test inference predictions\n",
      "2020-06-09T08:20:16 INFO: Predicting for hateful_memes\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:10<00:00,  1.25s/it]\n",
      "2020-06-09T08:20:26 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/save/reports/hateful_memes_run_test_2020-06-09T08:20:26.csv\n",
      "2020-06-09T08:20:26 INFO: Finished predicting\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/late_fusion/defaults.yaml\n",
      "Overriding option model to late_fusion\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to late_fusion.hateful_memes\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 31281266\n",
      "Logging to: /home/jupyter/meme_hateful_detection/logs/train_2020-06-09T08:20:31.log\n",
      "2020-06-09T08:20:31 INFO: Loading datasets\n",
      "2020-06-09T08:20:36 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-06-09T08:20:40 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-06-09T08:20:40 INFO: Loading checkpoint\n",
      "2020-06-09T08:20:47 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-06-09T08:20:47 INFO: Checkpoint loaded\n",
      "2020-06-09T08:20:47 INFO: Starting test inference predictions\n",
      "2020-06-09T08:20:47 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 16/16 [00:17<00:00,  1.09s/it]\n",
      "2020-06-09T08:21:05 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/save/reports/hateful_memes_run_test_2020-06-09T08:21:05.csv\n",
      "2020-06-09T08:21:05 INFO: Finished predicting\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/concat_bert/defaults.yaml\n",
      "Overriding option model to concat_bert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to concat_bert.hateful_memes\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 9991757\n",
      "Logging to: /home/jupyter/meme_hateful_detection/logs/train_2020-06-09T08:21:09.log\n",
      "2020-06-09T08:21:09 INFO: Loading datasets\n",
      "2020-06-09T08:21:15 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-06-09T08:21:19 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-06-09T08:21:19 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/concat_bert/concat_bert.finetuned.hateful_memes.tar.gz to /home/jupyter/meme_hateful_detection/data/raw/models/concat_bert.hateful_memes/concat_bert.finetuned.hateful_memes.tar.gz ]\n",
      "Downloading concat_bert.finetuned.hateful_memes.tar.gz: 100%|█| 633M/633M [00:21\n",
      "[ Starting checksum for concat_bert.finetuned.hateful_memes.tar.gz]\n",
      "[ Checksum successful for concat_bert.finetuned.hateful_memes.tar.gz]\n",
      "Unpacking concat_bert.finetuned.hateful_memes.tar.gz\n",
      "2020-06-09T08:21:49 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-06-09T08:21:49 INFO: Checkpoint loaded\n",
      "2020-06-09T08:21:49 INFO: Starting test inference predictions\n",
      "2020-06-09T08:21:49 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 16/16 [00:16<00:00,  1.06s/it]\n",
      "2020-06-09T08:22:06 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/save/reports/hateful_memes_run_test_2020-06-09T08:22:06.csv\n",
      "2020-06-09T08:22:06 INFO: Finished predicting\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/mmbt/defaults.yaml\n",
      "Overriding option model to mmbt\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to mmbt.hateful_memes.images\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 11791356\n",
      "Logging to: /home/jupyter/meme_hateful_detection/logs/train_2020-06-09T08:22:11.log\n",
      "2020-06-09T08:22:11 INFO: Loading datasets\n",
      "2020-06-09T08:22:17 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-06-09T08:22:21 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-06-09T08:22:21 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/mmbt/mmbt.finetuned.hateful_memes_images.tar.gz to /home/jupyter/meme_hateful_detection/data/raw/models/mmbt.hateful_memes.images/mmbt.finetuned.hateful_memes_images.tar.gz ]\n",
      "Downloading mmbt.finetuned.hateful_memes_images.tar.gz: 100%|█| 630M/630M [00:14\n",
      "[ Starting checksum for mmbt.finetuned.hateful_memes_images.tar.gz]\n",
      "[ Checksum successful for mmbt.finetuned.hateful_memes_images.tar.gz]\n",
      "Unpacking mmbt.finetuned.hateful_memes_images.tar.gz\n",
      "2020-06-09T08:22:45 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-06-09T08:22:45 INFO: Checkpoint loaded\n",
      "2020-06-09T08:22:45 INFO: Starting test inference predictions\n",
      "2020-06-09T08:22:45 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 32/32 [00:16<00:00,  1.89it/s]\n",
      "2020-06-09T08:23:02 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/save/reports/hateful_memes_run_test_2020-06-09T08:23:02.csv\n",
      "2020-06-09T08:23:02 INFO: Finished predicting\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/mmbt/with_features.yaml\n",
      "Overriding option model to mmbt\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to mmbt.hateful_memes.features\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 7278876\n",
      "Logging to: /home/jupyter/meme_hateful_detection/logs/train_2020-06-09T08:23:07.log\n",
      "2020-06-09T08:23:07 INFO: Loading datasets\n",
      "2020-06-09T08:23:11 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-06-09T08:23:15 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-06-09T08:23:15 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/mmbt/mmbt.finetuned.hateful_memes_features.tar.gz to /home/jupyter/meme_hateful_detection/data/raw/models/mmbt.hateful_memes.features/mmbt.finetuned.hateful_memes_features.tar.gz ]\n",
      "Downloading mmbt.finetuned.hateful_memes_features.tar.gz: 100%|█| 429M/429M [00:\n",
      "[ Starting checksum for mmbt.finetuned.hateful_memes_features.tar.gz]\n",
      "[ Checksum successful for mmbt.finetuned.hateful_memes_features.tar.gz]\n",
      "Unpacking mmbt.finetuned.hateful_memes_features.tar.gz\n",
      "2020-06-09T08:23:30 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-06-09T08:23:30 INFO: Checkpoint loaded\n",
      "2020-06-09T08:23:30 INFO: Starting test inference predictions\n",
      "2020-06-09T08:23:30 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 32/32 [00:17<00:00,  1.87it/s]\n",
      "2020-06-09T08:23:47 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/save/reports/hateful_memes_run_test_2020-06-09T08:23:47.csv\n",
      "2020-06-09T08:23:47 INFO: Finished predicting\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/vilbert/defaults.yaml\n",
      "Overriding option model to vilbert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.direct\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 53555761\n",
      "Logging to: /home/jupyter/meme_hateful_detection/logs/train_2020-06-09T08:23:53.log\n",
      "2020-06-09T08:23:53 INFO: Loading datasets\n",
      "2020-06-09T08:23:58 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-06-09T08:24:02 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-06-09T08:24:02 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.finetuned.hateful_memes_direct.tar.gz to /home/jupyter/meme_hateful_detection/data/raw/models/vilbert.finetuned.hateful_memes.direct/vilbert.finetuned.hateful_memes_direct.tar.gz ]\n",
      "Downloading vilbert.finetuned.hateful_memes_direct.tar.gz: 100%|█| 918M/918M [00\n",
      "[ Starting checksum for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
      "[ Checksum successful for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
      "Unpacking vilbert.finetuned.hateful_memes_direct.tar.gz\n",
      "2020-06-09T08:24:32 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-06-09T08:24:32 INFO: Checkpoint loaded\n",
      "2020-06-09T08:24:32 INFO: Starting test inference predictions\n",
      "2020-06-09T08:24:32 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 32/32 [00:19<00:00,  1.64it/s]\n",
      "2020-06-09T08:24:51 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/save/reports/hateful_memes_run_test_2020-06-09T08:24:51.csv\n",
      "2020-06-09T08:24:51 INFO: Finished predicting\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/visual_bert/direct.yaml\n",
      "Overriding option model to visual_bert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.direct\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 56755470\n",
      "Logging to: /home/jupyter/meme_hateful_detection/logs/train_2020-06-09T08:24:56.log\n",
      "2020-06-09T08:24:56 INFO: Loading datasets\n",
      "2020-06-09T08:25:00 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-06-09T08:25:04 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-06-09T08:25:04 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.finetuned.hateful_memes_direct.tar.gz to /home/jupyter/meme_hateful_detection/data/raw/models/visual_bert.finetuned.hateful_memes.direct/visual_bert.finetuned.hateful_memes_direct.tar.gz ]\n",
      "Downloading visual_bert.finetuned.hateful_memes_direct.tar.gz: 100%|█| 415M/415M  \n",
      "[ Starting checksum for visual_bert.finetuned.hateful_memes_direct.tar.gz]\n",
      "[ Checksum successful for visual_bert.finetuned.hateful_memes_direct.tar.gz]\n",
      "Unpacking visual_bert.finetuned.hateful_memes_direct.tar.gz\n",
      "2020-06-09T08:25:19 WARNING: /home/jupyter/mmf/mmf/utils/checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "2020-06-09T08:25:19 INFO: Checkpoint loaded\n",
      "2020-06-09T08:25:19 INFO: Starting test inference predictions\n",
      "2020-06-09T08:25:19 INFO: Predicting for hateful_memes\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:19<00:00,  2.44s/it]\n",
      "2020-06-09T08:25:38 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/save/reports/hateful_memes_run_test_2020-06-09T08:25:38.csv\n",
      "2020-06-09T08:25:38 INFO: Finished predicting\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/vilbert/from_cc.yaml\n",
      "Overriding option model to vilbert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.from_cc_original\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 45158100\n",
      "Logging to: /home/jupyter/meme_hateful_detection/logs/train_2020-06-09T08:25:45.log\n",
      "2020-06-09T08:25:45 INFO: Loading datasets\n",
      "2020-06-09T08:25:49 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-06-09T08:25:53 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-06-09T08:25:54 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.finetuned.hateful_memes_from_cc.tar.gz to /home/jupyter/meme_hateful_detection/data/raw/models/vilbert.finetuned.hateful_memes.from_cc_original/vilbert.finetuned.hateful_memes_from_cc.tar.gz ]\n",
      "Downloading vilbert.finetuned.hateful_memes_from_cc.tar.gz: 100%|█| 918M/918M [0\n",
      "[ Starting checksum for vilbert.finetuned.hateful_memes_from_cc.tar.gz]\n",
      "[ Checksum successful for vilbert.finetuned.hateful_memes_from_cc.tar.gz]\n",
      "Unpacking vilbert.finetuned.hateful_memes_from_cc.tar.gz\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.v_embeddings.image_embeddings.weight from model.bert.v_embeddings.image_embeddings.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.v_embeddings.image_embeddings.bias from model.bert.v_embeddings.image_embeddings.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.v_embeddings.image_location_embeddings.weight from model.bert.v_embeddings.image_location_embeddings.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.v_embeddings.image_location_embeddings.bias from model.bert.v_embeddings.image_location_embeddings.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.v_embeddings.LayerNorm.weight from model.bert.v_embeddings.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.v_embeddings.LayerNorm.bias from model.bert.v_embeddings.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.attention.self.query.weight from model.bert.encoder.v_layer.0.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.attention.self.query.bias from model.bert.encoder.v_layer.0.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.attention.self.key.weight from model.bert.encoder.v_layer.0.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.attention.self.key.bias from model.bert.encoder.v_layer.0.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.attention.self.value.weight from model.bert.encoder.v_layer.0.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.attention.self.value.bias from model.bert.encoder.v_layer.0.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.attention.output.dense.weight from model.bert.encoder.v_layer.0.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.attention.output.dense.bias from model.bert.encoder.v_layer.0.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.intermediate.dense.weight from model.bert.encoder.v_layer.0.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.intermediate.dense.bias from model.bert.encoder.v_layer.0.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.output.dense.weight from model.bert.encoder.v_layer.0.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.output.dense.bias from model.bert.encoder.v_layer.0.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.output.LayerNorm.weight from model.bert.encoder.v_layer.0.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.0.output.LayerNorm.bias from model.bert.encoder.v_layer.0.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.attention.self.query.weight from model.bert.encoder.v_layer.1.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.attention.self.query.bias from model.bert.encoder.v_layer.1.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.attention.self.key.weight from model.bert.encoder.v_layer.1.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.attention.self.key.bias from model.bert.encoder.v_layer.1.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.attention.self.value.weight from model.bert.encoder.v_layer.1.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.attention.self.value.bias from model.bert.encoder.v_layer.1.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.attention.output.dense.weight from model.bert.encoder.v_layer.1.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.attention.output.dense.bias from model.bert.encoder.v_layer.1.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.intermediate.dense.weight from model.bert.encoder.v_layer.1.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.intermediate.dense.bias from model.bert.encoder.v_layer.1.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.output.dense.weight from model.bert.encoder.v_layer.1.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.output.dense.bias from model.bert.encoder.v_layer.1.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.output.LayerNorm.weight from model.bert.encoder.v_layer.1.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.1.output.LayerNorm.bias from model.bert.encoder.v_layer.1.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.attention.self.query.weight from model.bert.encoder.v_layer.2.attention.self.query.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.attention.self.query.bias from model.bert.encoder.v_layer.2.attention.self.query.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.attention.self.key.weight from model.bert.encoder.v_layer.2.attention.self.key.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.attention.self.key.bias from model.bert.encoder.v_layer.2.attention.self.key.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.attention.self.value.weight from model.bert.encoder.v_layer.2.attention.self.value.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.attention.self.value.bias from model.bert.encoder.v_layer.2.attention.self.value.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.attention.output.dense.weight from model.bert.encoder.v_layer.2.attention.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.attention.output.dense.bias from model.bert.encoder.v_layer.2.attention.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.intermediate.dense.weight from model.bert.encoder.v_layer.2.intermediate.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.intermediate.dense.bias from model.bert.encoder.v_layer.2.intermediate.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.output.dense.weight from model.bert.encoder.v_layer.2.output.dense.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.output.dense.bias from model.bert.encoder.v_layer.2.output.dense.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.output.LayerNorm.weight from model.bert.encoder.v_layer.2.output.LayerNorm.weight\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.2.output.LayerNorm.bias from model.bert.encoder.v_layer.2.output.LayerNorm.bias\n",
      "2020-06-09T08:26:21 INFO: Copying model.bert.encoder.v_layer.3.attention.self.query.weight from model.bert.encoder.v_layer.3.attention.self.query.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.attention.self.query.bias from model.bert.encoder.v_layer.3.attention.self.query.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.attention.self.key.weight from model.bert.encoder.v_layer.3.attention.self.key.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.attention.self.key.bias from model.bert.encoder.v_layer.3.attention.self.key.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.attention.self.value.weight from model.bert.encoder.v_layer.3.attention.self.value.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.attention.self.value.bias from model.bert.encoder.v_layer.3.attention.self.value.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.attention.output.dense.weight from model.bert.encoder.v_layer.3.attention.output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.attention.output.dense.bias from model.bert.encoder.v_layer.3.attention.output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.intermediate.dense.weight from model.bert.encoder.v_layer.3.intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.intermediate.dense.bias from model.bert.encoder.v_layer.3.intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.output.dense.weight from model.bert.encoder.v_layer.3.output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.output.dense.bias from model.bert.encoder.v_layer.3.output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.output.LayerNorm.weight from model.bert.encoder.v_layer.3.output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.3.output.LayerNorm.bias from model.bert.encoder.v_layer.3.output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.attention.self.query.weight from model.bert.encoder.v_layer.4.attention.self.query.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.attention.self.query.bias from model.bert.encoder.v_layer.4.attention.self.query.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.attention.self.key.weight from model.bert.encoder.v_layer.4.attention.self.key.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.attention.self.key.bias from model.bert.encoder.v_layer.4.attention.self.key.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.attention.self.value.weight from model.bert.encoder.v_layer.4.attention.self.value.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.attention.self.value.bias from model.bert.encoder.v_layer.4.attention.self.value.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.attention.output.dense.weight from model.bert.encoder.v_layer.4.attention.output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.attention.output.dense.bias from model.bert.encoder.v_layer.4.attention.output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.intermediate.dense.weight from model.bert.encoder.v_layer.4.intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.intermediate.dense.bias from model.bert.encoder.v_layer.4.intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.output.dense.weight from model.bert.encoder.v_layer.4.output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.output.dense.bias from model.bert.encoder.v_layer.4.output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.output.LayerNorm.weight from model.bert.encoder.v_layer.4.output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.4.output.LayerNorm.bias from model.bert.encoder.v_layer.4.output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.attention.self.query.weight from model.bert.encoder.v_layer.5.attention.self.query.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.attention.self.query.bias from model.bert.encoder.v_layer.5.attention.self.query.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.attention.self.key.weight from model.bert.encoder.v_layer.5.attention.self.key.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.attention.self.key.bias from model.bert.encoder.v_layer.5.attention.self.key.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.attention.self.value.weight from model.bert.encoder.v_layer.5.attention.self.value.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.attention.self.value.bias from model.bert.encoder.v_layer.5.attention.self.value.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.attention.output.dense.weight from model.bert.encoder.v_layer.5.attention.output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.attention.output.dense.bias from model.bert.encoder.v_layer.5.attention.output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.intermediate.dense.weight from model.bert.encoder.v_layer.5.intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.intermediate.dense.bias from model.bert.encoder.v_layer.5.intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.output.dense.weight from model.bert.encoder.v_layer.5.output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.output.dense.bias from model.bert.encoder.v_layer.5.output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.output.LayerNorm.weight from model.bert.encoder.v_layer.5.output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.v_layer.5.output.LayerNorm.bias from model.bert.encoder.v_layer.5.output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biattention.query1.weight from model.bert.encoder.c_layer.0.biattention.query1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biattention.query1.bias from model.bert.encoder.c_layer.0.biattention.query1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biattention.key1.weight from model.bert.encoder.c_layer.0.biattention.key1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biattention.key1.bias from model.bert.encoder.c_layer.0.biattention.key1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biattention.value1.weight from model.bert.encoder.c_layer.0.biattention.value1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biattention.value1.bias from model.bert.encoder.c_layer.0.biattention.value1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biattention.query2.weight from model.bert.encoder.c_layer.0.biattention.query2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biattention.query2.bias from model.bert.encoder.c_layer.0.biattention.query2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biattention.key2.weight from model.bert.encoder.c_layer.0.biattention.key2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biattention.key2.bias from model.bert.encoder.c_layer.0.biattention.key2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biattention.value2.weight from model.bert.encoder.c_layer.0.biattention.value2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biattention.value2.bias from model.bert.encoder.c_layer.0.biattention.value2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense1.weight from model.bert.encoder.c_layer.0.biOutput.dense1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense1.bias from model.bert.encoder.c_layer.0.biOutput.dense1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense1.weight from model.bert.encoder.c_layer.0.biOutput.q_dense1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense1.bias from model.bert.encoder.c_layer.0.biOutput.q_dense1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense2.weight from model.bert.encoder.c_layer.0.biOutput.dense2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biOutput.dense2.bias from model.bert.encoder.c_layer.0.biOutput.dense2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense2.weight from model.bert.encoder.c_layer.0.biOutput.q_dense2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.biOutput.q_dense2.bias from model.bert.encoder.c_layer.0.biOutput.q_dense2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.v_intermediate.dense.weight from model.bert.encoder.c_layer.0.v_intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.v_intermediate.dense.bias from model.bert.encoder.c_layer.0.v_intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.v_output.dense.weight from model.bert.encoder.c_layer.0.v_output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.v_output.dense.bias from model.bert.encoder.c_layer.0.v_output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.v_output.LayerNorm.weight from model.bert.encoder.c_layer.0.v_output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.v_output.LayerNorm.bias from model.bert.encoder.c_layer.0.v_output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.t_intermediate.dense.weight from model.bert.encoder.c_layer.0.t_intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.t_intermediate.dense.bias from model.bert.encoder.c_layer.0.t_intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.t_output.dense.weight from model.bert.encoder.c_layer.0.t_output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.t_output.dense.bias from model.bert.encoder.c_layer.0.t_output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.t_output.LayerNorm.weight from model.bert.encoder.c_layer.0.t_output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.0.t_output.LayerNorm.bias from model.bert.encoder.c_layer.0.t_output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biattention.query1.weight from model.bert.encoder.c_layer.1.biattention.query1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biattention.query1.bias from model.bert.encoder.c_layer.1.biattention.query1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biattention.key1.weight from model.bert.encoder.c_layer.1.biattention.key1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biattention.key1.bias from model.bert.encoder.c_layer.1.biattention.key1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biattention.value1.weight from model.bert.encoder.c_layer.1.biattention.value1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biattention.value1.bias from model.bert.encoder.c_layer.1.biattention.value1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biattention.query2.weight from model.bert.encoder.c_layer.1.biattention.query2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biattention.query2.bias from model.bert.encoder.c_layer.1.biattention.query2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biattention.key2.weight from model.bert.encoder.c_layer.1.biattention.key2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biattention.key2.bias from model.bert.encoder.c_layer.1.biattention.key2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biattention.value2.weight from model.bert.encoder.c_layer.1.biattention.value2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biattention.value2.bias from model.bert.encoder.c_layer.1.biattention.value2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense1.weight from model.bert.encoder.c_layer.1.biOutput.dense1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense1.bias from model.bert.encoder.c_layer.1.biOutput.dense1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense1.weight from model.bert.encoder.c_layer.1.biOutput.q_dense1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense1.bias from model.bert.encoder.c_layer.1.biOutput.q_dense1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense2.weight from model.bert.encoder.c_layer.1.biOutput.dense2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biOutput.dense2.bias from model.bert.encoder.c_layer.1.biOutput.dense2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense2.weight from model.bert.encoder.c_layer.1.biOutput.q_dense2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.biOutput.q_dense2.bias from model.bert.encoder.c_layer.1.biOutput.q_dense2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.v_intermediate.dense.weight from model.bert.encoder.c_layer.1.v_intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.v_intermediate.dense.bias from model.bert.encoder.c_layer.1.v_intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.v_output.dense.weight from model.bert.encoder.c_layer.1.v_output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.v_output.dense.bias from model.bert.encoder.c_layer.1.v_output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.v_output.LayerNorm.weight from model.bert.encoder.c_layer.1.v_output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.v_output.LayerNorm.bias from model.bert.encoder.c_layer.1.v_output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.t_intermediate.dense.weight from model.bert.encoder.c_layer.1.t_intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.t_intermediate.dense.bias from model.bert.encoder.c_layer.1.t_intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.t_output.dense.weight from model.bert.encoder.c_layer.1.t_output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.t_output.dense.bias from model.bert.encoder.c_layer.1.t_output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.t_output.LayerNorm.weight from model.bert.encoder.c_layer.1.t_output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.1.t_output.LayerNorm.bias from model.bert.encoder.c_layer.1.t_output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biattention.query1.weight from model.bert.encoder.c_layer.2.biattention.query1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biattention.query1.bias from model.bert.encoder.c_layer.2.biattention.query1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biattention.key1.weight from model.bert.encoder.c_layer.2.biattention.key1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biattention.key1.bias from model.bert.encoder.c_layer.2.biattention.key1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biattention.value1.weight from model.bert.encoder.c_layer.2.biattention.value1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biattention.value1.bias from model.bert.encoder.c_layer.2.biattention.value1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biattention.query2.weight from model.bert.encoder.c_layer.2.biattention.query2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biattention.query2.bias from model.bert.encoder.c_layer.2.biattention.query2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biattention.key2.weight from model.bert.encoder.c_layer.2.biattention.key2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biattention.key2.bias from model.bert.encoder.c_layer.2.biattention.key2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biattention.value2.weight from model.bert.encoder.c_layer.2.biattention.value2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biattention.value2.bias from model.bert.encoder.c_layer.2.biattention.value2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense1.weight from model.bert.encoder.c_layer.2.biOutput.dense1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense1.bias from model.bert.encoder.c_layer.2.biOutput.dense1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense1.weight from model.bert.encoder.c_layer.2.biOutput.q_dense1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense1.bias from model.bert.encoder.c_layer.2.biOutput.q_dense1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense2.weight from model.bert.encoder.c_layer.2.biOutput.dense2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biOutput.dense2.bias from model.bert.encoder.c_layer.2.biOutput.dense2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense2.weight from model.bert.encoder.c_layer.2.biOutput.q_dense2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.biOutput.q_dense2.bias from model.bert.encoder.c_layer.2.biOutput.q_dense2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.v_intermediate.dense.weight from model.bert.encoder.c_layer.2.v_intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.v_intermediate.dense.bias from model.bert.encoder.c_layer.2.v_intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.v_output.dense.weight from model.bert.encoder.c_layer.2.v_output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.v_output.dense.bias from model.bert.encoder.c_layer.2.v_output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.v_output.LayerNorm.weight from model.bert.encoder.c_layer.2.v_output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.v_output.LayerNorm.bias from model.bert.encoder.c_layer.2.v_output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.t_intermediate.dense.weight from model.bert.encoder.c_layer.2.t_intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.t_intermediate.dense.bias from model.bert.encoder.c_layer.2.t_intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.t_output.dense.weight from model.bert.encoder.c_layer.2.t_output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.t_output.dense.bias from model.bert.encoder.c_layer.2.t_output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.t_output.LayerNorm.weight from model.bert.encoder.c_layer.2.t_output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.2.t_output.LayerNorm.bias from model.bert.encoder.c_layer.2.t_output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biattention.query1.weight from model.bert.encoder.c_layer.3.biattention.query1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biattention.query1.bias from model.bert.encoder.c_layer.3.biattention.query1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biattention.key1.weight from model.bert.encoder.c_layer.3.biattention.key1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biattention.key1.bias from model.bert.encoder.c_layer.3.biattention.key1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biattention.value1.weight from model.bert.encoder.c_layer.3.biattention.value1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biattention.value1.bias from model.bert.encoder.c_layer.3.biattention.value1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biattention.query2.weight from model.bert.encoder.c_layer.3.biattention.query2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biattention.query2.bias from model.bert.encoder.c_layer.3.biattention.query2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biattention.key2.weight from model.bert.encoder.c_layer.3.biattention.key2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biattention.key2.bias from model.bert.encoder.c_layer.3.biattention.key2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biattention.value2.weight from model.bert.encoder.c_layer.3.biattention.value2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biattention.value2.bias from model.bert.encoder.c_layer.3.biattention.value2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense1.weight from model.bert.encoder.c_layer.3.biOutput.dense1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense1.bias from model.bert.encoder.c_layer.3.biOutput.dense1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense1.weight from model.bert.encoder.c_layer.3.biOutput.q_dense1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense1.bias from model.bert.encoder.c_layer.3.biOutput.q_dense1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense2.weight from model.bert.encoder.c_layer.3.biOutput.dense2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biOutput.dense2.bias from model.bert.encoder.c_layer.3.biOutput.dense2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense2.weight from model.bert.encoder.c_layer.3.biOutput.q_dense2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.biOutput.q_dense2.bias from model.bert.encoder.c_layer.3.biOutput.q_dense2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.v_intermediate.dense.weight from model.bert.encoder.c_layer.3.v_intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.v_intermediate.dense.bias from model.bert.encoder.c_layer.3.v_intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.v_output.dense.weight from model.bert.encoder.c_layer.3.v_output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.v_output.dense.bias from model.bert.encoder.c_layer.3.v_output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.v_output.LayerNorm.weight from model.bert.encoder.c_layer.3.v_output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.v_output.LayerNorm.bias from model.bert.encoder.c_layer.3.v_output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.t_intermediate.dense.weight from model.bert.encoder.c_layer.3.t_intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.t_intermediate.dense.bias from model.bert.encoder.c_layer.3.t_intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.t_output.dense.weight from model.bert.encoder.c_layer.3.t_output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.t_output.dense.bias from model.bert.encoder.c_layer.3.t_output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.t_output.LayerNorm.weight from model.bert.encoder.c_layer.3.t_output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.3.t_output.LayerNorm.bias from model.bert.encoder.c_layer.3.t_output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biattention.query1.weight from model.bert.encoder.c_layer.4.biattention.query1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biattention.query1.bias from model.bert.encoder.c_layer.4.biattention.query1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biattention.key1.weight from model.bert.encoder.c_layer.4.biattention.key1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biattention.key1.bias from model.bert.encoder.c_layer.4.biattention.key1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biattention.value1.weight from model.bert.encoder.c_layer.4.biattention.value1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biattention.value1.bias from model.bert.encoder.c_layer.4.biattention.value1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biattention.query2.weight from model.bert.encoder.c_layer.4.biattention.query2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biattention.query2.bias from model.bert.encoder.c_layer.4.biattention.query2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biattention.key2.weight from model.bert.encoder.c_layer.4.biattention.key2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biattention.key2.bias from model.bert.encoder.c_layer.4.biattention.key2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biattention.value2.weight from model.bert.encoder.c_layer.4.biattention.value2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biattention.value2.bias from model.bert.encoder.c_layer.4.biattention.value2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense1.weight from model.bert.encoder.c_layer.4.biOutput.dense1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense1.bias from model.bert.encoder.c_layer.4.biOutput.dense1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense1.weight from model.bert.encoder.c_layer.4.biOutput.q_dense1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense1.bias from model.bert.encoder.c_layer.4.biOutput.q_dense1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense2.weight from model.bert.encoder.c_layer.4.biOutput.dense2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biOutput.dense2.bias from model.bert.encoder.c_layer.4.biOutput.dense2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense2.weight from model.bert.encoder.c_layer.4.biOutput.q_dense2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.biOutput.q_dense2.bias from model.bert.encoder.c_layer.4.biOutput.q_dense2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.v_intermediate.dense.weight from model.bert.encoder.c_layer.4.v_intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.v_intermediate.dense.bias from model.bert.encoder.c_layer.4.v_intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.v_output.dense.weight from model.bert.encoder.c_layer.4.v_output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.v_output.dense.bias from model.bert.encoder.c_layer.4.v_output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.v_output.LayerNorm.weight from model.bert.encoder.c_layer.4.v_output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.v_output.LayerNorm.bias from model.bert.encoder.c_layer.4.v_output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.t_intermediate.dense.weight from model.bert.encoder.c_layer.4.t_intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.t_intermediate.dense.bias from model.bert.encoder.c_layer.4.t_intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.t_output.dense.weight from model.bert.encoder.c_layer.4.t_output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.t_output.dense.bias from model.bert.encoder.c_layer.4.t_output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.t_output.LayerNorm.weight from model.bert.encoder.c_layer.4.t_output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.4.t_output.LayerNorm.bias from model.bert.encoder.c_layer.4.t_output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biattention.query1.weight from model.bert.encoder.c_layer.5.biattention.query1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biattention.query1.bias from model.bert.encoder.c_layer.5.biattention.query1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biattention.key1.weight from model.bert.encoder.c_layer.5.biattention.key1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biattention.key1.bias from model.bert.encoder.c_layer.5.biattention.key1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biattention.value1.weight from model.bert.encoder.c_layer.5.biattention.value1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biattention.value1.bias from model.bert.encoder.c_layer.5.biattention.value1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biattention.query2.weight from model.bert.encoder.c_layer.5.biattention.query2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biattention.query2.bias from model.bert.encoder.c_layer.5.biattention.query2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biattention.key2.weight from model.bert.encoder.c_layer.5.biattention.key2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biattention.key2.bias from model.bert.encoder.c_layer.5.biattention.key2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biattention.value2.weight from model.bert.encoder.c_layer.5.biattention.value2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biattention.value2.bias from model.bert.encoder.c_layer.5.biattention.value2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense1.weight from model.bert.encoder.c_layer.5.biOutput.dense1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense1.bias from model.bert.encoder.c_layer.5.biOutput.dense1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense1.weight from model.bert.encoder.c_layer.5.biOutput.q_dense1.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense1.bias from model.bert.encoder.c_layer.5.biOutput.q_dense1.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense2.weight from model.bert.encoder.c_layer.5.biOutput.dense2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biOutput.dense2.bias from model.bert.encoder.c_layer.5.biOutput.dense2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense2.weight from model.bert.encoder.c_layer.5.biOutput.q_dense2.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.biOutput.q_dense2.bias from model.bert.encoder.c_layer.5.biOutput.q_dense2.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.v_intermediate.dense.weight from model.bert.encoder.c_layer.5.v_intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.v_intermediate.dense.bias from model.bert.encoder.c_layer.5.v_intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.v_output.dense.weight from model.bert.encoder.c_layer.5.v_output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.v_output.dense.bias from model.bert.encoder.c_layer.5.v_output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.v_output.LayerNorm.weight from model.bert.encoder.c_layer.5.v_output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.v_output.LayerNorm.bias from model.bert.encoder.c_layer.5.v_output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.t_intermediate.dense.weight from model.bert.encoder.c_layer.5.t_intermediate.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.t_intermediate.dense.bias from model.bert.encoder.c_layer.5.t_intermediate.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.t_output.dense.weight from model.bert.encoder.c_layer.5.t_output.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.t_output.dense.bias from model.bert.encoder.c_layer.5.t_output.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.t_output.LayerNorm.weight from model.bert.encoder.c_layer.5.t_output.LayerNorm.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.encoder.c_layer.5.t_output.LayerNorm.bias from model.bert.encoder.c_layer.5.t_output.LayerNorm.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.t_pooler.dense.weight from model.bert.t_pooler.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.t_pooler.dense.bias from model.bert.t_pooler.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.v_pooler.dense.weight from model.bert.v_pooler.dense.weight\n",
      "2020-06-09T08:26:22 INFO: Copying model.bert.v_pooler.dense.bias from model.bert.v_pooler.dense.bias\n",
      "2020-06-09T08:26:22 INFO: Pretrained model loaded\n",
      "2020-06-09T08:26:22 INFO: Starting test inference predictions\n",
      "2020-06-09T08:26:22 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 32/32 [00:19<00:00,  1.67it/s]\n",
      "2020-06-09T08:26:41 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/save/reports/hateful_memes_run_test_2020-06-09T08:26:41.csv\n",
      "2020-06-09T08:26:41 INFO: Finished predicting\n",
      "Importing user_dir from /home/jupyter/meme_hateful_detection\n",
      "Overriding option config to configs/visual_bert/from_coco.yaml\n",
      "Overriding option model to visual_bert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to test\n",
      "Overriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.from_coco\n",
      "Overriding option evaluation.predict to true\n",
      "Using seed 46373263\n",
      "Logging to: /home/jupyter/meme_hateful_detection/logs/train_2020-06-09T08:26:46.log\n",
      "2020-06-09T08:26:46 INFO: Loading datasets\n",
      "2020-06-09T08:26:50 INFO: CUDA Device 0 is: Tesla T4\n",
      "2020-06-09T08:26:54 INFO: Torch version is: 1.5.0+cu101\n",
      "2020-06-09T08:26:54 INFO: Loading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.finetuned.hateful_memes_from_coco.tar.gz to /home/jupyter/meme_hateful_detection/data/raw/models/visual_bert.finetuned.hateful_memes.from_coco/visual_bert.finetuned.hateful_memes_from_coco.tar.gz ]\n",
      "Downloading visual_bert.finetuned.hateful_memes_from_coco.tar.gz: 100%|█| 414M/4     \n",
      "[ Starting checksum for visual_bert.finetuned.hateful_memes_from_coco.tar.gz]\n",
      "[ Checksum successful for visual_bert.finetuned.hateful_memes_from_coco.tar.gz]\n",
      "Unpacking visual_bert.finetuned.hateful_memes_from_coco.tar.gz\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
      "2020-06-09T08:27:08 INFO: Copying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
      "2020-06-09T08:27:08 INFO: Pretrained model loaded\n",
      "2020-06-09T08:27:08 INFO: Starting test inference predictions\n",
      "2020-06-09T08:27:08 INFO: Predicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 16/16 [00:16<00:00,  1.04s/it]\n",
      "2020-06-09T08:27:25 INFO: Wrote evalai predictions for hateful_memes to /home/jupyter/meme_hateful_detection/save/reports/hateful_memes_run_test_2020-06-09T08:27:25.csv\n",
      "2020-06-09T08:27:25 INFO: Finished predicting\n"
     ]
    }
   ],
   "source": [
    "# Predictions test\n",
    "for index, row in df_model_config.iterrows():\n",
    "    model_baseline = row['baseline']\n",
    "    model_key = row['model_key']\n",
    "    model_pretrained_key = row['pretrained_key']\n",
    "    baseline_config = row['baseline_config']\n",
    "    custom_config = row['custom_config']\n",
    "    save_dir = row['save_dir']\n",
    "#     str_exec = f'mmf_predict config={baseline_config} model={model_key} dataset=hateful_memes run_type=test checkpoint.resume_zoo={model_pretrained_key}'\n",
    "    # Predictions test\n",
    "    str_pred_test = f'MMF_SAVE_DIR=\"{PATH_SAVE}/{save_dir}\" mmf_predict config={custom_config} model={model_key} dataset=hateful_memes run_type=test'\n",
    "    print(str_pred_test)\n",
    "#     !{str_pred_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained model in your code\n",
    "model_baseline = 'Image-Grid'\n",
    "model_key = 'unimodal_image'\n",
    "model_pretrained_key = 'unimodal_image.hateful_memes.images'\n",
    "baseline_config = 'configs/unimodal/image.yaml'\n",
    "custom_config = row['custom_config']\n",
    "# Training\n",
    "str_run_train = f'mmf_run config={custom_config} model={model_key} dataset=hateful_memes'\n",
    "# Evaluation\n",
    "str_run_eval  = f'mmf_run config={custom_config} model={model_key} dataset=hateful_memes run_type=val'\n",
    "# Predictions val\n",
    "str_pred_eval = f'mmf_predict config={custom_config} model={model_key} dataset=hateful_memes run_type=train'\n",
    "# Predictions test\n",
    "str_pred_test = f'mmf_predict config={custom_config} model={model_key} dataset=hateful_memes run_type=test'\n",
    "# Evaluating Train dataset\n",
    "str_pret_eval = f'mmf_run config={custom_config} model={model_key} dataset=hateful_memes run_type=train checkpoint.resume_zoo={model_pretrained_key}'\n",
    "\n",
    "# Evaluating Pretrained Models\n",
    "str_pret_eval = f'mmf_run config={custom_config} model={model_key} dataset=hateful_memes run_type=val checkpoint.resume_zoo={model_pretrained_key}'\n",
    "# Predictions Pretrained Models\n",
    "str_pret_test = f'mmf_predict config={custom_config} model={model_key} dataset=hateful_memes run_type=test checkpoint.resume_zoo={model_pretrained_key}'"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T11:17:34.283823Z",
     "start_time": "2020-05-19T11:17:33.169031Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "apZzGk6K0Nu3",
    "outputId": "aec69580-b96f-4b4c-e515-47bac6001d61"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import tensorflow\n",
    "# import keras\n",
    "# import h5py\n",
    "# import progressbar\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "\n",
    "# Machine Learning packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from pickle import load\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "PATH_CURRENT = '/home/jupyter/meme_hateful_detection'\n",
    "PATH_TRAIN_MEMES = f'{PATH_CURRENT}/data/raw/facebook_memes'\n",
    "PATH_MEMES_DATASET = f'{PATH_CURRENT}/data/raw/datasets/hateful_memes/defaults/annotations'\n",
    "PATH_INTERIM = f'{PATH_CURRENT}/data/interim'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Li-Iqeol6sX1"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 3000000    # max no. of words for tokenizer\n",
    "MAX_SEQUENCE_LENGTH = 200 # max length of each entry (sentence), including padding\n",
    "VALIDATION_SPLIT = 0.2   # data for validation (not used in training)\n",
    "EMBEDDING_DIM = 100      # embedding dimensions for word vectors (word2vec/GloVe)\n",
    "GLOVE_DIR = \"glove/glove.6B.\"+str(EMBEDDING_DIM)+\"d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rk8F-QOT6giJ"
   },
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) \n",
    "nlp.max_length = MAX_NB_WORDS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OR5zErSx0Nu9"
   },
   "source": [
    "#  1. Exploratory Data Analysis\n",
    "\n",
    "## Context\n",
    "As said above, we've worked on a dataset of job descriptions and their meta information in which a small proportion of these descriptions were fake or scam, which can be identified by the column \"fraudulent\".\n",
    "\n",
    "## Data files\n",
    "train.jsonl - the training set\n",
    "dev.jsonl - the development set\n",
    "test.jsonl - the test set\n",
    "\n",
    "### Columns\n",
    "* id:  Meme id\n",
    "* img: Meme image file\n",
    "* text: A string representing the text in the meme image\n",
    "* label: Probability that the meme is hateful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T11:17:34.382574Z",
     "start_time": "2020-05-19T11:17:34.320625Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "bG3reWKC0Nu-",
    "outputId": "c0720eda-4987-45f6-8cc4-366cdceac4af"
   },
   "outputs": [],
   "source": [
    "df_memes_dev = pd.read_json(f'{PATH_MEMES_DATASET}/dev.jsonl', lines=True, orient='records')\n",
    "df_memes_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "0X-23G5rM5WY",
    "outputId": "9ec5ae00-82c4-4ad6-b70a-3c30f0b049a7"
   },
   "outputs": [],
   "source": [
    "df_memes_train = pd.read_json(f'{PATH_MEMES_DATASET}/train.jsonl', lines=True, orient='records')\n",
    "df_memes_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "CH44KtTeNLBp",
    "outputId": "c29ed78f-fe7f-4a17-9b3a-a9e70c6f770c"
   },
   "outputs": [],
   "source": [
    "df_memes_test = pd.read_json(f'{PATH_MEMES_DATASET}/test.jsonl', lines=True, orient='records')\n",
    "df_memes_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZwT84JJ40NvC"
   },
   "outputs": [],
   "source": [
    "df_memes_normal  = df_memes_train[df_memes_train['label']==0]\n",
    "df_memes_hateful = df_memes_train[df_memes_train['label']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kcmZA5PD0NvF"
   },
   "source": [
    "# 2. Word Exploratory Data Analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T12:23:53.385723Z",
     "start_time": "2020-05-19T12:23:53.356799Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bPX1N6We0NvG",
    "outputId": "c5470a21-c205-4b7e-f165-5ce3910870fb"
   },
   "outputs": [],
   "source": [
    "df_memes_test['text'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memes_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T13:09:13.514160Z",
     "start_time": "2020-05-19T13:09:13.508140Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "hOpF7c9N0NvJ"
   },
   "outputs": [],
   "source": [
    "def series_to_str(series_column):\n",
    "    '''This function converts a series to text, concatenating its values'''\n",
    "    str_text = ' '.join(series_column)\n",
    "    return(clean_text(str_text))\n",
    "    \n",
    "def clean_text(str_text_raw):\n",
    "    '''This function clean a given text'''\n",
    "    str_text = str_text_raw.lower()\n",
    "    return(str_text)\n",
    "  \n",
    "def clean_image_path(str_image):\n",
    "    str_image_clean = str_image.replace('img/','')\n",
    "    return(str_image_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14INi04o__RX"
   },
   "outputs": [],
   "source": [
    "def string_to_token(string, str_pickle = None):\n",
    "    '''\n",
    "    This function takes a sentence and returns the list of tokens and all their information\n",
    "    * Text: The original text of the lexeme.\n",
    "    * Lemme: Lexeme.\n",
    "    * Orth: The hash value of the lexeme.\n",
    "    * is alpha: Does the lexeme consist of alphabetic characters?\n",
    "    * is digit: Does the lexeme consist of digits?\n",
    "    * is_title: Is the token in titlecase? \n",
    "    * is_punct: Is the token punctuation?\n",
    "    * is_space: Does the token consist of whitespace characters?\n",
    "    * is_stop: Is the token part of a “stop list”?\n",
    "    * is_digit: Does the token consist of digits?\n",
    "    * lang: Language of the token\n",
    "    * tag: Fine-grained part-of-speech. The complete list is in: \n",
    "    https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html, also using: spacy.explain(\"RB\")\n",
    "    * pos: Coarse-grained part-of-speech.\n",
    "    * has_vector: A boolean value indicating whether a word vector is associated with the token.\n",
    "    * vector_norm: The L2 norm of the token’s vector representation.\n",
    "    * is_ovv: '''\n",
    "    doc = nlp(string)\n",
    "    l_token = [[token.text, token.lemma_, token.orth, token.is_alpha, token.is_digit, token.is_title, token.lang_, \n",
    "        token.tag_, token.pos_, token.has_vector, token.vector_norm, token.is_oov]\n",
    "        for token in doc if not token.is_punct | token.is_space | token.is_stop | token.is_digit | token.like_url \n",
    "               | token.like_num | token.like_email & token.is_oov]\n",
    "    df_token = pd.DataFrame(l_token, columns=['text', 'lemme', 'orth', 'is_alpha', 'is_digit', 'is_title', 'language',\n",
    "                                          'tag', 'part_of_speech', 'has_vector', 'vector_norm', 'is_oov'])\n",
    "    #Convert plural text to singular\n",
    "    df_token['text_to_singular'] = np.where(df_token['tag'].isin(['NNPS', 'NNS']), df_token['lemme'], df_token['text'])\n",
    "    if(str_pickle!=None):\n",
    "        df_token.to_pickle(f'../data/pickles/{str_pickle}.pkl')\n",
    "    del l_token\n",
    "    return(df_token)\n",
    "\n",
    "def list_to_bow(l_words):\n",
    "    '''\n",
    "    This function takes a list of words and create the bag of words ordered by desc order\n",
    "    '''\n",
    "    cv = CountVectorizer(l_words)\n",
    "    # show resulting vocabulary; the numbers are not counts, they are the position in the sparse vector.\n",
    "    count_vector=cv.fit_transform(l_words)\n",
    "    word_freq = Counter(l_words)\n",
    "    print(f'Bag of words size: {count_vector.shape}\\nUnique words size: {len(word_freq)}')\n",
    "    dict_word_freq = dict(word_freq.most_common())\n",
    "    return(dict_word_freq)\n",
    "\n",
    "def create_wordcloud(dict_words, b_plot=False):\n",
    "    wordcloud = WordCloud(width = 1000, height = 500, normalize_plurals=True).generate_from_frequencies(dict_words)\n",
    "    if(b_plot==True):\n",
    "      plt.figure(figsize=(20,8))\n",
    "      plt.imshow(wordcloud)\n",
    "      plt.axis('off')\n",
    "      plt.title(\"Most lemma words\", fontsize=25)\n",
    "      plt.show\n",
    "    return(wordcloud)\n",
    "\n",
    "def apply_cleaning(string):\n",
    "    '''\n",
    "    This function takes a sentence and returns a clean text\n",
    "    '''\n",
    "    doc = nlp(clean_text(string))\n",
    "    l_token = [token.text for token in doc if not token.is_punct | token.is_space | token.is_stop | \n",
    "               token.is_digit | token.like_url | token.like_num | token.like_email & token.is_oov]\n",
    "    return ' '.join(l_token)\n",
    "\n",
    "def apply_lemma(string):\n",
    "    '''\n",
    "    This function takes a sentence and returns a clean text\n",
    "    '''\n",
    "    doc = nlp(clean_text(string))\n",
    "    l_token = [token.lemma_ for token in doc if not token.is_punct | token.is_space | token.is_stop | \n",
    "               token.is_digit | token.like_url | token.like_num | token.like_email & token.is_oov]\n",
    "    return ' '.join(l_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "vVDdoDS97CWo",
    "outputId": "036885b6-9859-4f66-aee1-ab5cfdf9b801"
   },
   "outputs": [],
   "source": [
    "str_text_total_clean  = series_to_str(df_memes_train.text)\n",
    "str_text_total_normal = series_to_str(df_memes_normal.text)\n",
    "str_text_total_hateful= series_to_str(df_memes_hateful.text)\n",
    "print(f'Total bow with lenght: {len(str_text_total_clean)}')\n",
    "print(f'Total normal memes with lenght: {len(str_text_total_normal)}')\n",
    "print(f'Total hateful memes with lenght: {len(str_text_total_hateful)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "colab_type": "code",
    "id": "RFGD4s5s7CXz",
    "outputId": "a9300de2-8ded-41fc-ed73-7be80afc06a7"
   },
   "outputs": [],
   "source": [
    "df_token = string_to_token(str_text_total_clean)\n",
    "dict_word_freq_lemme = list_to_bow(list(df_token['lemme']))\n",
    "wordcloud = create_wordcloud(dict_word_freq_lemme)\n",
    "wordcloud = WordCloud(width = 1000, height = 500, normalize_plurals=True).generate_from_frequencies(dict_word_freq_lemme)\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title(\"Most lemma words\", fontsize=25)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "colab_type": "code",
    "id": "PIPd_kk9QbG7",
    "outputId": "c8f74609-e92b-4009-e950-6d5438eecdcd"
   },
   "outputs": [],
   "source": [
    "df_token = string_to_token(str_text_total_normal)\n",
    "dict_word_freq_lemme = list_to_bow(list(df_token['lemme']))\n",
    "wordcloud = WordCloud(width = 1000, height = 500, normalize_plurals=True).generate_from_frequencies(dict_word_freq_lemme)\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title(\"Most lemma words\", fontsize=25)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T13:16:00.372769Z",
     "start_time": "2020-05-19T13:16:00.366780Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "colab_type": "code",
    "id": "KoTANybC0NvQ",
    "outputId": "c920a062-1fd7-48d3-f150-91b5c3de7ae4"
   },
   "outputs": [],
   "source": [
    "df_token = string_to_token(str_text_total_hateful)\n",
    "dict_word_freq_lemme = list_to_bow(list(df_token['lemme']))\n",
    "wordcloud = WordCloud(width = 1000, height = 500, normalize_plurals=True).generate_from_frequencies(dict_word_freq_lemme)\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title(\"Most lemma words\", fontsize=25)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "WSWmrlbRQbFU",
    "outputId": "a63ca4bc-bc6b-4aa6-e42a-f051bb482759"
   },
   "outputs": [],
   "source": [
    "df_memes_dev['img']   = df_memes_dev['img'].apply(lambda x: clean_image_path(x))\n",
    "df_memes_train['img'] = df_memes_train['img'].apply(lambda x: clean_image_path(x))\n",
    "df_memes_test['img']  = df_memes_test['img'].apply(lambda x: clean_image_path(x))\n",
    "df_memes_dev['text_clean']   = df_memes_dev['text'].apply(lambda x: apply_cleaning(x))\n",
    "df_memes_train['text_clean'] = df_memes_train['text'].apply(lambda x: apply_cleaning(x))\n",
    "df_memes_test['text_clean']  = df_memes_test['text'].apply(lambda x: apply_cleaning(x))\n",
    "df_memes_dev['text_lemma']   = df_memes_dev['text'].apply(lambda x: apply_lemma(x))\n",
    "df_memes_train['text_lemma'] = df_memes_train['text'].apply(lambda x: apply_lemma(x))\n",
    "df_memes_test['text_lemma']  = df_memes_test['text'].apply(lambda x: apply_lemma(x))\n",
    "df_memes_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AL8tm5okJghV"
   },
   "outputs": [],
   "source": [
    "df_memes_dev.to_pickle(f'{PATH_INTERIM}/df_memes_dev_clean.pkl') #export pickle\n",
    "df_memes_train.to_pickle(f'{PATH_INTERIM}/df_memes_train_clean.pkl')\n",
    "df_memes_test.to_pickle(f'{PATH_INTERIM}/df_memes_test_clean.pkl')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1.Intro_and_EDA.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
